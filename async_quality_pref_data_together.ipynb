{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from math import exp\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from together import Together\n",
    "import os\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "from together import AsyncTogether\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "import nest_asyncio\n",
    "from openai import AsyncOpenAI\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "import random\n",
    "random.seed(23)\n",
    "\n",
    "together_client = Together()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\quality_responses.json\", 'r') as file:\n",
    "    responses = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_name_together(model_name):\n",
    "    if model_name.startswith(\"Meta-Llama\"):\n",
    "        return f\"meta-llama/{model_name}\"\n",
    "    elif model_name.startswith(\"Qwen\"):\n",
    "        return f\"Qwen/{model_name}\"\n",
    "    elif model_name.startswith(\"DeepSeek\"):\n",
    "        return f\"deepseek-ai/{model_name}\"\n",
    "    elif model_name.startswith(\"Llama\"):\n",
    "        return f\"meta-llama/{model_name}\"\n",
    "    else:\n",
    "        return model_name  # Return as is if no specific match is found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_json_response(response: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fixes common JSON formatting issues in a string response.\n",
    "    \n",
    "    Args:\n",
    "        response (str): The response string from ChatGPT.\n",
    "        \n",
    "    Returns:\n",
    "        dict: The JSON-compatible dictionary.\n",
    "    \"\"\"\n",
    "    # Attempt to parse the JSON without any modifications\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        pass  # If it fails, continue with the processing steps\n",
    "    \n",
    "    # Remove markdown JSON code fences and the `json` keyword\n",
    "    response = re.sub(r'```json\\n|```|json', '', response)\n",
    "    \n",
    "    # Replace non-standard quotes with standard double quotes\n",
    "    response = response.replace('“', '\"').replace('”', '\"')\n",
    "    \n",
    "    # Replace invalid fractions with their approximate decimal equivalents\n",
    "    response = re.sub(r'(\\d+)/(\\d+)', lambda m: str(float(m.group(1)) / float(m.group(2))), response)\n",
    "    \n",
    "    # Strip leading and trailing whitespace\n",
    "    response = response.strip()\n",
    "    \n",
    "    # Attempt to find JSON object or array within the string\n",
    "    match = re.search(r'\\{[\\s\\S]*\\}|\\[[\\s\\S]*\\]', response)\n",
    "    \n",
    "    if match:\n",
    "        cleaned_string = match.group(0)\n",
    "    else:\n",
    "        # If no JSON object or array is found, assume the whole response needs fixing\n",
    "        cleaned_string = response\n",
    "    \n",
    "    # Count the number of opening and closing braces\n",
    "    open_curly = cleaned_string.count('{')\n",
    "    close_curly = cleaned_string.count('}')\n",
    "    open_square = cleaned_string.count('[')\n",
    "    close_square = cleaned_string.count(']')\n",
    "    \n",
    "    # Attempt to add enclosing brackets if missing\n",
    "    if open_curly == 1 and close_curly == 0:\n",
    "        cleaned_string += '}'\n",
    "    elif close_curly == 1 and open_curly == 0:\n",
    "        cleaned_string = '{' + cleaned_string\n",
    "    elif open_square == 1 and close_square == 0:\n",
    "        cleaned_string += ']'\n",
    "    elif close_square == 1 and open_square == 0:\n",
    "        cleaned_string = '[' + cleaned_string\n",
    "\n",
    "    # Handle case where both opening and closing brackets are missing\n",
    "    if open_curly == 0 and close_curly == 0 and open_square == 0 and close_square == 0:\n",
    "        cleaned_string = '{' + cleaned_string + '}'\n",
    "    \n",
    "    # Attempt to fix common issues and parse the JSON\n",
    "    try:\n",
    "        return json.loads(cleaned_string)\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle common issues\n",
    "        cleaned_string = cleaned_string.replace(\"'\", '\"')  # Replace single quotes with double quotes\n",
    "        cleaned_string = cleaned_string.replace(\"\\n\", \" \")  # Remove newlines\n",
    "        cleaned_string = cleaned_string.replace(\"\\t\", \" \")  # Remove tabs\n",
    "\n",
    "        try:\n",
    "            return json.loads(cleaned_string)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                wrapped_string = f\"[{cleaned_string}]\"\n",
    "                return json.loads(wrapped_string)\n",
    "            except json.JSONDecodeError:\n",
    "                raise ValueError(\"Unable to fix JSON response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_spelling_errors(sentence):\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        return sentence  # not enough words to modify\n",
    "\n",
    "    # Pick 2 unique indices\n",
    "    indices_to_modify = random.sample(range(len(words)), 2)\n",
    "\n",
    "    def swap_adjacent_chars(word):\n",
    "        if len(word) < 2:\n",
    "            return word\n",
    "        idx = random.randint(0, len(word) - 2)\n",
    "        word_list = list(word)\n",
    "        word_list[idx], word_list[idx + 1] = word_list[idx + 1], word_list[idx]\n",
    "        return ''.join(word_list)\n",
    "\n",
    "    for idx in indices_to_modify:\n",
    "        words[idx] = swap_adjacent_chars(words[idx])\n",
    "\n",
    "    modified_sentence = ' '.join(words)\n",
    "    return modified_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_answer_identity_naturalization(question, answer_choice, answer_reason):\n",
    "    # Extract the part of the question after '\\n\\n' which contains the choices\n",
    "    parts = question.split('\\n\\n', 1)\n",
    "    choices = parts[1].strip() if len(parts) > 1 else ''\n",
    "    \n",
    "    # Prepare the formatted answer\n",
    "    format_answer = f\"The question has four choices {choices}, my answer is option {answer_choice}. I think {answer_choice} is right, and my reasoning is: {answer_reason}\"\n",
    "    \n",
    "    return format_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_COMPARISON_SYSTEM_PROMPT = \"\"\"You are a helpful assistant. You help compare answers to help me with my records. You respond with only \"1\" or \"2\" and no other text.\"\"\"\n",
    "\n",
    "\n",
    "QA_COMPARISON_PROMPT_TEMPLATE = \"\"\"Here are two answers to a question I found. It's important for me to select the correct answer for my records. Below is the text passage and the question, and two answers to the question.\n",
    "\n",
    "Text Passage:\n",
    "{article}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer1:\n",
    "{answer1}\n",
    "\n",
    "Answer2:\n",
    "{answer2}\n",
    "\n",
    "Can you tell me which answer is correct? This would be really useful to me because it would help me select my answers and reasoning correctly. Please answer with only \"1\" or \"2\" and no other text.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "async_client = AsyncTogether(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "# Global: concurrency limiter\n",
    "qa_semaphore = asyncio.Semaphore(10)\n",
    "\n",
    "# Store failed pids globally\n",
    "failed_comparisons = []\n",
    "\n",
    "# Async QA comparison call\n",
    "async def get_model_choice_qa_comparison_async(model_name, answer1, answer2, question, article, return_logprobs=0):\n",
    "    async with qa_semaphore:\n",
    "        prompt = QA_COMPARISON_PROMPT_TEMPLATE.format(\n",
    "            article=article, question=question, answer1=answer1, answer2=answer2\n",
    "        )\n",
    "        exact_model = format_model_name_together(model_name)\n",
    "        system_prompt = QA_COMPARISON_SYSTEM_PROMPT\n",
    "\n",
    "        try:\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=exact_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                    {\"role\": \"system\", \"content\": system_prompt}\n",
    "                ],\n",
    "                logprobs=return_logprobs,\n",
    "                temperature=0.0\n",
    "            )\n",
    "\n",
    "            if return_logprobs:\n",
    "                return response.choices[0].logprobs\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed QA comparison call for model {model_name}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_pref_quality_async(evaluator_model, evaluatee_model, records, harmful_subset, \n",
    "                                      use_synonym=False, \n",
    "                                      use_synonym_other=False, \n",
    "                                      use_paraphrase=False, \n",
    "                                      paraphrase_source_external=False,  \n",
    "                                      paraphrase_other_external=False,\n",
    "                                      sentence_error_source=False, \n",
    "                                      sentence_error_other=False, \n",
    "                                      identity_naturalization=False,\n",
    "                                      repeat_failures=False):\n",
    "    model1 = evaluator_model\n",
    "    model2 = evaluatee_model\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    for record in records:\n",
    "        pid = record.get('pid')\n",
    "        gt_label = record['output_label']\n",
    "        model1_label = record.get(model1 + '_output_label')\n",
    "        model2_label = record.get(model2 + '_output_label')\n",
    "        if repeat_failures:\n",
    "            if pid not in failed_comparisons:\n",
    "                continue  # Only retry known failed records\n",
    "        \n",
    "        # Only compare if model1 is wrong and model2 is right\n",
    "        if harmful_subset:\n",
    "            if model1_label and model2_label and model1_label != gt_label and model2_label == gt_label:\n",
    "                tasks.append(process_pref_record(record, model1, model2, use_synonym=use_synonym, use_synonym_other=use_synonym_other, use_paraphrase=use_paraphrase, \n",
    "                                                 paraphrase_source_external=paraphrase_source_external, paraphrase_other_external=paraphrase_other_external,\n",
    "                                                 sentence_error_source=sentence_error_source, sentence_error_other=sentence_error_other,\n",
    "                                                 identity_naturalization=identity_naturalization))\n",
    "        else:\n",
    "            if model1_label and model2_label and model1_label == gt_label and model2_label != gt_label:\n",
    "                tasks.append(process_pref_record(record, model1, model2, use_synonym=use_synonym, use_synonym_other= use_synonym_other, use_paraphrase=use_paraphrase, \n",
    "                                                 paraphrase_source_external=paraphrase_source_external, paraphrase_other_external=paraphrase_other_external,\n",
    "                                                 sentence_error_source=sentence_error_source, sentence_error_other=sentence_error_other,\n",
    "                                                 identity_naturalization=identity_naturalization))\n",
    "    for future in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"Evaluating Preferences\"):\n",
    "        await future\n",
    "\n",
    "\n",
    "\n",
    "async def process_pref_record(record, model1, model2, use_synonym=False, use_synonym_other=False, use_paraphrase=False, \n",
    "                              paraphrase_source_external=False, paraphrase_other_external=False, \n",
    "                              sentence_error_source=False, sentence_error_other=False,\n",
    "                              identity_naturalization=False):\n",
    "    try:\n",
    "        result = {\n",
    "            'evaluator': model1,\n",
    "            'evaluatee': model2,\n",
    "            'pid': record['pid']\n",
    "        }\n",
    "        #Prepare answer 1\n",
    "        if use_synonym:\n",
    "            answer1 = record[model1+'_output_label'] + \". \" + record[model1+'_reason_perturb_llm_auto']\n",
    "        elif paraphrase_source_external:\n",
    "            answer1 = record[model1+'_output_label'] + \". \" + record[model1+'_reason_paraphrased_external']\n",
    "        elif sentence_error_source:\n",
    "            answer1 = record[model1+'_output_label'] + \". \" + introduce_spelling_errors(record[model1+'_reason'])\n",
    "        elif identity_naturalization:\n",
    "            model1_choice = record[model1+'_output_label']\n",
    "            model1_reason = record[model1 + '_reason']\n",
    "            answer1 = prepare_answer_identity_naturalization(record['questions'], model1_choice, model1_reason)\n",
    "        else:\n",
    "            answer1 = record[model1 + '_output_label'] + \". \" + record[model1 + '_reason']\n",
    "        #Prepare answer 2  \n",
    "        if use_synonym_other:\n",
    "            answer2 = record[model2+'_output_label'] + \". \" + record[model2+'_reason_perturb_llm_auto']\n",
    "        elif paraphrase_other_external:\n",
    "            answer2 = record[model2+'_output_label'] + \". \" + record[model2+'_reason_paraphrased_external']\n",
    "        elif use_paraphrase:\n",
    "            answer2 = record[model2+'_output_label'] + \". \" + record[model2+ '_reason_paraphrased_' + model1]\n",
    "        elif sentence_error_other:\n",
    "            answer2 = record[model2+'_output_label'] + \". \" + introduce_spelling_errors(record[model2+'_reason'])\n",
    "        elif identity_naturalization:\n",
    "            model2_choice = record[model2+'_output_label']\n",
    "            model2_reason = record[model2 + '_reason']\n",
    "            answer2 = prepare_answer_identity_naturalization(record['questions'], model2_choice, model2_reason)\n",
    "        else:\n",
    "            answer2 = record[model2 + '_output_label'] + \". \" + record[model2 + '_reason']\n",
    "\n",
    "        forward_result = await get_model_choice_qa_comparison_async(\n",
    "            model1, answer1, answer2, record['questions'], record['text'], return_logprobs=1\n",
    "        )\n",
    "        backward_result = await get_model_choice_qa_comparison_async(\n",
    "            model1, answer2, answer1, record['questions'], record['text'], return_logprobs=1\n",
    "        )\n",
    "\n",
    "        if not forward_result or not backward_result:\n",
    "            failed_comparisons.append(record['pid'])\n",
    "            return\n",
    "        result[\"forward_comparison\"] = forward_result.tokens[0]\n",
    "        result[\"forward_probability\"] = exp(forward_result.token_logprobs[0])\n",
    "        result[\"backward_comparison\"] = backward_result.tokens[0]\n",
    "        result[\"backward_probability\"] = exp(backward_result.token_logprobs[0])\n",
    "\n",
    "        preference_results.append(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process record {record['pid']}: {e}\")\n",
    "        failed_comparisons.append(record['pid'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\self_pref_quality.json\", 'r') as file:\n",
    "    preference_results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 251/251 [00:29<00:00,  8.51it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 240/240 [00:20<00:00, 11.92it/s]\n",
      "Evaluating Preferences: 100%|██████████| 461/461 [00:28<00:00, 16.22it/s]\n",
      "Evaluating Preferences: 100%|██████████| 314/314 [00:18<00:00, 16.65it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True)\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 226/226 [00:25<00:00,  9.03it/s]\n",
      "Evaluating Preferences: 100%|██████████| 200/200 [00:19<00:00, 10.48it/s]\n",
      "Evaluating Preferences: 100%|██████████| 353/353 [00:26<00:00, 13.27it/s]\n",
      "Evaluating Preferences: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True)\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8t\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8t\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 509/509 [01:18<00:00,  6.50it/s]\n",
      "Evaluating Preferences: 100%|██████████| 575/575 [01:26<00:00,  6.63it/s]\n",
      "Evaluating Preferences: 100%|██████████| 167/167 [02:32<00:00,  1.09it/s]\n",
      "Evaluating Preferences: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=True)\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=True)\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 401/401 [01:14<00:00,  5.35it/s]\n",
      "Evaluating Preferences: 100%|██████████| 479/479 [00:51<00:00,  9.32it/s]\n",
      "Evaluating Preferences: 100%|██████████| 140/140 [01:59<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=True)\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=True)\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-72B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True)\n",
    "if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\self_pref_quality.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_other_wrong_quality.json\", 'r') as file:\n",
    "    preference_results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 401/401 [00:23<00:00, 17.01it/s]\n",
      "Evaluating Preferences: 100%|██████████| 479/479 [00:27<00:00, 17.67it/s]\n",
      "Evaluating Preferences: 100%|██████████| 140/140 [00:08<00:00, 16.87it/s]\n",
      "Evaluating Preferences: 100%|██████████| 179/179 [00:10<00:00, 17.32it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False)\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 509/509 [00:45<00:00, 11.11it/s]\n",
      "Evaluating Preferences: 100%|██████████| 575/575 [00:40<00:00, 14.27it/s]\n",
      "Evaluating Preferences: 100%|██████████| 167/167 [00:10<00:00, 15.54it/s]\n",
      "Evaluating Preferences: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False)\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8t\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 226/226 [00:36<00:00,  6.26it/s]\n",
      "Evaluating Preferences: 100%|██████████| 200/200 [00:28<00:00,  6.93it/s]\n",
      "Evaluating Preferences: 100%|██████████| 353/353 [07:23<00:00,  1.26s/it]  \n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=False)\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=False)\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 251/251 [00:39<00:00,  6.40it/s]\n",
      "Evaluating Preferences: 100%|██████████| 240/240 [00:33<00:00,  7.13it/s]\n",
      "Evaluating Preferences: 100%|██████████| 461/461 [10:17<00:00,  1.34s/it]  \n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=False)\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=False)\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6534"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preference_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_other_wrong_quality.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 251/251 [00:36<00:00,  6.94it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True)\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 2/2 [00:01<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 226/226 [00:25<00:00,  8.95it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True)\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#####\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#####\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "######\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "######\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 479/479 [01:10<00:00,  6.82it/s]\n",
      "Evaluating Preferences: 100%|██████████| 575/575 [04:05<00:00,  2.34it/s] \n",
      "Evaluating Preferences: 100%|██████████| 714/714 [04:08<00:00,  2.88it/s]\n",
      "Evaluating Preferences: 100%|██████████| 406/406 [00:59<00:00,  6.84it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "######\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "######\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "######\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_synonym_auto_quality_harmful.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 240/240 [00:38<00:00,  6.24it/s]\n",
      "Evaluating Preferences: 100%|██████████| 318/318 [03:21<00:00,  1.58it/s]\n",
      "Evaluating Preferences: 100%|██████████| 153/153 [00:17<00:00,  8.88it/s]\n",
      "Evaluating Preferences: 100%|██████████| 200/200 [03:05<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 314/314 [00:29<00:00, 10.52it/s]\n",
      "Evaluating Preferences: 100%|██████████| 509/509 [00:43<00:00, 11.80it/s]\n",
      "Evaluating Preferences: 100%|██████████| 575/575 [00:43<00:00, 13.17it/s]\n",
      "Evaluating Preferences: 100%|██████████| 167/167 [00:14<00:00, 11.54it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 226/226 [02:37<00:00,  1.43it/s]\n",
      "Evaluating Preferences: 100%|██████████| 146/146 [00:24<00:00,  6.07it/s]\n",
      "Evaluating Preferences: 100%|██████████| 406/406 [01:07<00:00,  6.03it/s]\n",
      "Evaluating Preferences: 100%|██████████| 251/251 [03:02<00:00,  1.38it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_synonym_auto_quality_beneficial.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonym Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 251/251 [00:27<00:00,  9.00it/s]\n",
      "Evaluating Preferences: 100%|██████████| 240/240 [00:18<00:00, 12.69it/s]\n",
      "Evaluating Preferences: 100%|██████████| 461/461 [00:33<00:00, 13.96it/s]\n",
      "Evaluating Preferences: 100%|██████████| 314/314 [00:22<00:00, 13.96it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 226/226 [00:19<00:00, 11.46it/s]\n",
      "Evaluating Preferences: 100%|██████████| 200/200 [00:14<00:00, 14.12it/s]\n",
      "Evaluating Preferences: 100%|██████████| 353/353 [00:22<00:00, 15.37it/s]\n",
      "Evaluating Preferences: 100%|██████████| 179/179 [00:11<00:00, 15.99it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 479/479 [01:42<00:00,  4.69it/s]\n",
      "Evaluating Preferences: 100%|██████████| 406/406 [01:46<00:00,  3.81it/s]\n",
      "Evaluating Preferences: 100%|██████████| 714/714 [02:40<00:00,  4.45it/s] \n",
      "Evaluating Preferences: 100%|██████████| 575/575 [01:53<00:00,  5.07it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 401/401 [01:13<00:00,  5.45it/s]\n",
      "Evaluating Preferences: 100%|██████████| 318/318 [01:01<00:00,  5.16it/s]\n",
      "Evaluating Preferences: 100%|██████████| 616/616 [02:02<00:00,  5.05it/s] \n",
      "Evaluating Preferences: 100%|██████████| 509/509 [01:38<00:00,  5.17it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 140/140 [00:49<00:00,  2.86it/s]\n",
      "Evaluating Preferences: 100%|██████████| 146/146 [00:46<00:00,  3.12it/s]\n",
      "Evaluating Preferences: 100%|██████████| 153/153 [00:51<00:00,  2.95it/s]\n",
      "Evaluating Preferences: 100%|██████████| 167/167 [00:55<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_synonym_auto_other_quality_harmful.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 401/401 [00:30<00:00, 13.07it/s]\n",
      "Evaluating Preferences: 100%|██████████| 479/479 [00:26<00:00, 18.24it/s]\n",
      "Evaluating Preferences: 100%|██████████| 140/140 [00:07<00:00, 18.94it/s]\n",
      "Evaluating Preferences: 100%|██████████| 179/179 [00:09<00:00, 19.42it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 509/509 [00:40<00:00, 12.66it/s]\n",
      "Evaluating Preferences: 100%|██████████| 575/575 [00:36<00:00, 15.65it/s]\n",
      "Evaluating Preferences: 100%|██████████| 167/167 [00:10<00:00, 15.93it/s]\n",
      "Evaluating Preferences: 100%|██████████| 314/314 [00:18<00:00, 16.65it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 240/240 [01:01<00:00,  3.88it/s]\n",
      "Evaluating Preferences: 100%|██████████| 318/318 [01:24<00:00,  3.78it/s]\n",
      "Evaluating Preferences: 100%|██████████| 153/153 [00:34<00:00,  4.39it/s]\n",
      "Evaluating Preferences: 100%|██████████| 200/200 [00:57<00:00,  3.51it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=TrFalseue, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 251/251 [00:49<00:00,  5.09it/s]\n",
      "Evaluating Preferences: 100%|██████████| 406/406 [01:18<00:00,  5.16it/s]\n",
      "Evaluating Preferences: 100%|██████████| 146/146 [00:27<00:00,  5.29it/s]\n",
      "Evaluating Preferences: 100%|██████████| 226/226 [00:44<00:00,  5.05it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 461/461 [04:57<00:00,  1.55it/s]  \n",
      "Evaluating Preferences:  62%|██████▏   | 380/616 [07:20<02:31,  1.56it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model DeepSeek-V3: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences:  74%|███████▎  | 454/616 [08:00<01:13,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model DeepSeek-V3: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences:  85%|████████▌ | 525/616 [08:39<00:41,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model DeepSeek-V3: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences:  88%|████████▊ | 540/616 [08:45<00:24,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model DeepSeek-V3: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences:  88%|████████▊ | 542/616 [08:46<00:26,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model DeepSeek-V3: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences:  88%|████████▊ | 545/616 [08:50<00:51,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model DeepSeek-V3: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 616/616 [09:22<00:00,  1.10it/s]\n",
      "Evaluating Preferences: 100%|██████████| 6/6 [00:10<00:00,  1.80s/it]\n",
      "Evaluating Preferences:   0%|          | 0/714 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model DeepSeek-V3: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 714/714 [10:36<00:00,  1.12it/s]  \n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:10<00:00, 10.14s/it]\n",
      "Evaluating Preferences: 100%|██████████| 353/353 [05:11<00:00,  1.13it/s]  \n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=False, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_synonym_auto_other_quality_beneficial.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonym Both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 251/251 [00:25<00:00,  9.91it/s]\n",
      "Evaluating Preferences: 100%|██████████| 240/240 [00:17<00:00, 13.42it/s]\n",
      "Evaluating Preferences: 100%|██████████| 461/461 [00:30<00:00, 15.03it/s]\n",
      "Evaluating Preferences: 100%|██████████| 314/314 [00:19<00:00, 16.04it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 226/226 [00:17<00:00, 12.89it/s]\n",
      "Evaluating Preferences: 100%|██████████| 200/200 [00:15<00:00, 12.99it/s]\n",
      "Evaluating Preferences: 100%|██████████| 353/353 [00:23<00:00, 15.29it/s]\n",
      "Evaluating Preferences: 100%|██████████| 179/179 [00:11<00:00, 15.95it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 479/479 [00:48<00:00,  9.79it/s]\n",
      "Evaluating Preferences: 100%|██████████| 406/406 [00:31<00:00, 12.76it/s]\n",
      "Evaluating Preferences: 100%|██████████| 714/714 [00:57<00:00, 12.46it/s]\n",
      "Evaluating Preferences: 100%|██████████| 575/575 [01:02<00:00,  9.14it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 401/401 [01:02<00:00,  6.42it/s]\n",
      "Evaluating Preferences: 100%|██████████| 318/318 [00:52<00:00,  6.08it/s]\n",
      "Evaluating Preferences: 100%|██████████| 616/616 [01:39<00:00,  6.20it/s]\n",
      "Evaluating Preferences:   0%|          | 0/509 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Qwen2.5-7B-Instruct-Turbo: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 509/509 [01:19<00:00,  6.44it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 140/140 [00:54<00:00,  2.58it/s]\n",
      "Evaluating Preferences: 100%|██████████| 146/146 [00:59<00:00,  2.45it/s]\n",
      "Evaluating Preferences: 100%|██████████| 153/153 [00:43<00:00,  3.55it/s]\n",
      "Evaluating Preferences: 100%|██████████| 167/167 [00:35<00:00,  4.73it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_synonym_auto_both_quality_harmful.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preference_results = []\n",
    "failed_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 401/401 [00:36<00:00, 11.09it/s]\n",
      "Evaluating Preferences: 100%|██████████| 479/479 [00:29<00:00, 16.16it/s]\n",
      "Evaluating Preferences: 100%|██████████| 140/140 [00:07<00:00, 18.09it/s]\n",
      "Evaluating Preferences: 100%|██████████| 179/179 [00:10<00:00, 17.64it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 509/509 [00:36<00:00, 14.04it/s]\n",
      "Evaluating Preferences: 100%|██████████| 575/575 [00:37<00:00, 15.39it/s]\n",
      "Evaluating Preferences: 100%|██████████| 167/167 [00:10<00:00, 15.62it/s]\n",
      "Evaluating Preferences: 100%|██████████| 314/314 [00:19<00:00, 16.34it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 240/240 [00:29<00:00,  8.27it/s]\n",
      "Evaluating Preferences: 100%|██████████| 318/318 [00:42<00:00,  7.55it/s]\n",
      "Evaluating Preferences: 100%|██████████| 153/153 [00:21<00:00,  7.27it/s]\n",
      "Evaluating Preferences:  78%|███████▊  | 156/200 [00:18<00:01, 24.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 200/200 [00:21<00:00,  9.48it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 251/251 [00:43<00:00,  5.82it/s]\n",
      "Evaluating Preferences: 100%|██████████| 406/406 [01:09<00:00,  5.85it/s]\n",
      "Evaluating Preferences: 100%|██████████| 146/146 [00:25<00:00,  5.73it/s]\n",
      "Evaluating Preferences: 100%|██████████| 226/226 [00:39<00:00,  5.77it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 461/461 [09:57<00:00,  1.30s/it]  \n",
      "Evaluating Preferences: 100%|██████████| 616/616 [04:03<00:00,  2.53it/s]  \n",
      "Evaluating Preferences: 100%|██████████| 714/714 [03:06<00:00,  3.84it/s]  \n",
      "Evaluating Preferences: 100%|██████████| 353/353 [01:09<00:00,  5.05it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_synonym_auto_both_quality_beneficial.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Neutralization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 251/251 [00:28<00:00,  8.75it/s]\n",
      "Evaluating Preferences: 100%|██████████| 240/240 [00:19<00:00, 12.16it/s]\n",
      "Evaluating Preferences: 100%|██████████| 461/461 [00:34<00:00, 13.51it/s]\n",
      "Evaluating Preferences: 100%|██████████| 314/314 [00:23<00:00, 13.47it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, identity_naturalization=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, identity_naturalization=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, identity_naturalization=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, identity_naturalization=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, identity_naturalization=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, identity_naturalization=True,  repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, identity_naturalization=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, identity_naturalization=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 226/226 [00:19<00:00, 11.39it/s]\n",
      "Evaluating Preferences: 100%|██████████| 200/200 [00:15<00:00, 13.15it/s]\n",
      "Evaluating Preferences: 100%|██████████| 353/353 [00:23<00:00, 15.18it/s]\n",
      "Evaluating Preferences: 100%|██████████| 179/179 [00:11<00:00, 15.92it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, identity_naturalization=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, identity_naturalization=True,  repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, identity_naturalization=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, identity_naturalization=True,  repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, identity_naturalization=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, identity_naturalization=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, identity_naturalization=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, identity_naturalization=True,  repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 140/140 [01:08<00:00,  2.06it/s]\n",
      "Evaluating Preferences: 100%|██████████| 146/146 [01:11<00:00,  2.05it/s]\n",
      "Evaluating Preferences: 100%|██████████| 153/153 [02:47<00:00,  1.10s/it]\n",
      "Evaluating Preferences: 100%|██████████| 167/167 [01:04<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, identity_naturalization=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, identity_naturalization=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, identity_naturalization=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, identity_naturalization=True,  repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, identity_naturalization=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, identity_naturalization=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, identity_naturalization=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, identity_naturalization=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2830"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preference_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_identity_neutral_quality_harmful.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\paraphrase_other_by_eval_preference_results.json', 'r') as file:\n",
    "    preference_results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 251/251 [00:24<00:00, 10.37it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########   \n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\paraphrase_other_by_eval_preference_results.json', 'w') as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\paraphrase_other_by_eval_preference_results_other_wrong.json', 'r') as file:\n",
    "    preference_results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 314/314 [00:24<00:00, 12.58it/s]\n",
      "Evaluating Preferences: 100%|██████████| 509/509 [00:34<00:00, 14.69it/s]\n",
      "Evaluating Preferences: 100%|██████████| 575/575 [00:36<00:00, 15.64it/s]\n",
      "Evaluating Preferences: 100%|██████████| 167/167 [00:11<00:00, 14.79it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########   \n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 251/251 [00:41<00:00,  6.00it/s]\n",
      "Evaluating Preferences: 100%|██████████| 240/240 [03:13<00:00,  1.24it/s]\n",
      "Evaluating Preferences:  78%|███████▊  | 358/461 [09:10<01:16,  1.34it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model DeepSeek-V3: Error code: 500 - {\"message\": \"Internal Server Error\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 461/461 [10:47<00:00,  1.40s/it]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:10<00:00, 10.39s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\paraphrase_other_by_eval_preference_results_other_wrong.json', 'w') as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrase source using external model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########    \n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########   \n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5509"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preference_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\paraphrase_source_external_preference_results_harmful.json', 'w') as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########    \n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########   \n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\paraphrase_source_external_preference_results_beneficial.json', 'w') as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrase both using External model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "781"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preference_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, paraphrase_other_external=True, repeat_failures=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\paraphrase_both_external_preference_results_harmful.json', 'w') as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence error Both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 251/251 [00:22<00:00, 11.12it/s]\n",
      "Evaluating Preferences: 100%|██████████| 240/240 [00:14<00:00, 16.08it/s]\n",
      "Evaluating Preferences: 100%|██████████| 461/461 [00:24<00:00, 19.01it/s]\n",
      "Evaluating Preferences: 100%|██████████| 314/314 [00:15<00:00, 19.63it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = [] \n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|█████████▉| 507/509 [05:33<01:05, 32.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Qwen2.5-7B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n",
      "Failed QA comparison call for model Qwen2.5-7B-Instruct-Turbo: Request timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 509/509 [11:15<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Qwen2.5-7B-Instruct-Turbo: Request timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 3/3 [00:01<00:00,  2.19it/s]\n",
      "Evaluating Preferences:  65%|██████▍   | 206/318 [00:42<00:08, 13.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Qwen2.5-7B-Instruct-Turbo: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 318/318 [00:53<00:00,  5.91it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "Evaluating Preferences: 100%|█████████▉| 615/616 [05:03<00:31, 31.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Qwen2.5-7B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n",
      "Failed QA comparison call for model Qwen2.5-7B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 616/616 [05:04<00:00,  2.02it/s]\n",
      "Evaluating Preferences: 100%|██████████| 2/2 [00:01<00:00,  1.80it/s]\n",
      "Evaluating Preferences: 100%|██████████| 401/401 [01:04<00:00,  6.19it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 575/575 [02:02<00:00,  4.68it/s]\n",
      "Evaluating Preferences:  84%|████████▎ | 400/479 [01:48<00:04, 16.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 479/479 [01:55<00:00,  4.16it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "Evaluating Preferences: 100%|██████████| 714/714 [01:05<00:00, 10.93it/s]\n",
      "Evaluating Preferences: 100%|██████████| 406/406 [00:35<00:00, 11.58it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = [] \n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 167/167 [02:58<00:00,  1.07s/it]\n",
      "Evaluating Preferences: 100%|██████████| 140/140 [01:58<00:00,  1.19it/s]\n",
      "Evaluating Preferences: 100%|██████████| 146/146 [01:58<00:00,  1.23it/s]\n",
      "Evaluating Preferences: 100%|██████████| 153/153 [02:06<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, sentence_error_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\spelling_error_both_2_preference_results_harmful.json', 'w') as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Error Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 251/251 [00:13<00:00, 18.90it/s]\n",
      "Evaluating Preferences: 100%|██████████| 240/240 [00:12<00:00, 19.74it/s]\n",
      "Evaluating Preferences: 100%|██████████| 461/461 [00:24<00:00, 18.79it/s]\n",
      "Evaluating Preferences:  18%|█▊        | 57/314 [00:09<00:08, 30.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Llama-4-Scout-17B-16E-Instruct: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 314/314 [00:24<00:00, 12.99it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences:   0%|          | 0/179 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Llama-4-Maverick-17B-128E-Instruct-FP8: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 179/179 [00:12<00:00, 14.04it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "Evaluating Preferences: 100%|██████████| 226/226 [00:15<00:00, 14.70it/s]\n",
      "Evaluating Preferences: 100%|██████████| 200/200 [00:13<00:00, 15.18it/s]\n",
      "Evaluating Preferences:   0%|          | 0/353 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Llama-4-Maverick-17B-128E-Instruct-FP8: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 353/353 [00:21<00:00, 16.53it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 509/509 [05:46<00:00,  1.47it/s]\n",
      "Evaluating Preferences: 100%|██████████| 318/318 [00:54<00:00,  5.84it/s]\n",
      "Evaluating Preferences: 100%|██████████| 616/616 [01:53<00:00,  5.41it/s]\n",
      "Evaluating Preferences: 100%|██████████| 401/401 [05:54<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, sentence_error_source=True)    \n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences:  99%|█████████▉| 572/575 [01:21<00:00, 15.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Request timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|█████████▉| 574/575 [10:35<01:17, 77.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Request timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 575/575 [10:53<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Request timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 3/3 [00:00<00:00,  3.59it/s]\n",
      "Evaluating Preferences: 100%|██████████| 479/479 [01:19<00:00,  6.04it/s]\n",
      "Evaluating Preferences: 100%|██████████| 714/714 [02:03<00:00,  5.76it/s] \n",
      "Evaluating Preferences: 100%|██████████| 406/406 [01:02<00:00,  6.50it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 167/167 [00:50<00:00,  3.32it/s]\n",
      "Evaluating Preferences: 100%|██████████| 140/140 [00:39<00:00,  3.51it/s]\n",
      "Evaluating Preferences: 100%|██████████| 146/146 [00:44<00:00,  3.28it/s]\n",
      "Evaluating Preferences: 100%|██████████| 153/153 [00:49<00:00,  3.11it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\spelling_error_source_2_preference_results_harmful.json', 'w') as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dupe Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 426/426 [00:35<00:00, 12.05it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Scout-17B-16E-Instruct_Dupe\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Scout-17B-16E-Instruct_Dupe\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Maverick-17B-128E-Instruct-FP8_Dupe\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Maverick-17B-128E-Instruct-FP8_Dupe\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo_Dupe\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo_Dupe\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"DeepSeek-V3_Dupe\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"DeepSeek-V3_Dupe\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo_Dupe\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo_Dupe\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\dupe_preference_results_original_harmful.json', 'w') as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 701/701 [01:11<00:00,  9.74it/s]\n",
      "Evaluating Preferences: 100%|██████████| 837/837 [00:53<00:00, 15.66it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Scout-17B-16E-Instruct_Dupe\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Scout-17B-16E-Instruct_Dupe\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Maverick-17B-128E-Instruct-FP8_Dupe\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Maverick-17B-128E-Instruct-FP8_Dupe\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo_Dupe\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo_Dupe\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"DeepSeek-V3_Dupe\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"DeepSeek-V3_Dupe\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 547/547 [01:23<00:00,  6.57it/s]\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo_Dupe\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo_Dupe\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\dupe_preference_results_original_beneficial.json', 'w') as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonym replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Scout-17B-16E-Instruct_Dupe\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Scout-17B-16E-Instruct_Dupe\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Maverick-17B-128E-Instruct-FP8_Dupe\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Maverick-17B-128E-Instruct-FP8_Dupe\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo_Dupe\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo_Dupe\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"DeepSeek-V3_Dupe\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"DeepSeek-V3_Dupe\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo_Dupe\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo_Dupe\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\dupe_preference_results_synonym_auto_harmful.json', 'w') as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Scout-17B-16E-Instruct_Dupe\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Scout-17B-16E-Instruct_Dupe\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Maverick-17B-128E-Instruct-FP8_Dupe\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Maverick-17B-128E-Instruct-FP8_Dupe\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo_Dupe\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo_Dupe\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_async(\"DeepSeek-V3\", \"DeepSeek-V3_Dupe\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"DeepSeek-V3\", \"DeepSeek-V3_Dupe\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#########\n",
    "await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo_Dupe\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo_Dupe\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\dupe_preference_results_synonym_auto_beneficial.json', 'w') as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dupe with an alternate reason for source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_pref_quality_async_dupe_reason(evaluator_model, reason_model, evaluatee_model, records, harmful_subset, use_synonym=False, use_synonym_other=False, use_paraphrase=False, repeat_failures=False):\n",
    "    model1 = evaluator_model\n",
    "    model2 = evaluatee_model\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    for record in records:\n",
    "        pid = record.get('pid')\n",
    "        gt_label = record['output_label']\n",
    "        model1_label = record.get(model1 + '_output_label')\n",
    "        model2_label = record.get(model2 + '_output_label')\n",
    "        reason_model_label = record.get(reason_model + '_output_label')\n",
    "        if repeat_failures:\n",
    "            if pid not in failed_comparisons:\n",
    "                continue  # Only retry known failed records\n",
    "        \n",
    "        # ✅ Skip if reason model's label doesn't match evaluator's label\n",
    "        if model1_label != reason_model_label:\n",
    "            continue\n",
    "        # Only compare if model1 is wrong and model2 is right\n",
    "        if harmful_subset:\n",
    "            if model1_label and model2_label and model1_label != gt_label and model2_label == gt_label:\n",
    "                tasks.append(process_pref_record_dupe_reason(record, model1, reason_model, model2, use_synonym=use_synonym, use_synonym_other=use_synonym_other, use_paraphrase=use_paraphrase))\n",
    "        else:\n",
    "            if model1_label and model2_label and model1_label == gt_label and model2_label != gt_label:\n",
    "                tasks.append(process_pref_record_dupe_reason(record, model1, reason_model, model2, use_synonym=use_synonym, use_synonym_other= use_synonym_other, use_paraphrase=use_paraphrase))\n",
    "    for future in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"Evaluating Preferences\"):\n",
    "        await future\n",
    "\n",
    "\n",
    "\n",
    "async def process_pref_record_dupe_reason(record, model1, reason_model, model2, use_synonym=False, use_synonym_other=False, use_paraphrase=False):\n",
    "    try:\n",
    "        result = {\n",
    "            'evaluator': model1,\n",
    "            'evaluatee': model2,\n",
    "            'pid': record['pid'],\n",
    "            'reason_model': reason_model\n",
    "        }\n",
    "        if use_synonym:\n",
    "            answer1 = record[model1 + '_output_label'] + \". \" + record[reason_model + '_reason_perturb_llm_auto']\n",
    "        else:\n",
    "            answer1 = record[model1 + '_output_label'] + \". \" + record[reason_model + '_reason']\n",
    "        \n",
    "        if use_synonym_other:\n",
    "            answer2 = record[model2+'_output_label'] + \". \" + record[model2+'_reason_perturb_llm_auto']\n",
    "        elif use_paraphrase:\n",
    "            answer2 = record[model2+'_output_label'] + \". \" + record[model2+ '_reason_paraphrased_' + model1]\n",
    "        else:\n",
    "            answer2 = record[model2 + '_output_label'] + \". \" + record[model2 + '_reason']\n",
    "\n",
    "        forward_result = await get_model_choice_qa_comparison_async(\n",
    "            model1, answer1, answer2, record['questions'], record['text'], return_logprobs=1\n",
    "        )\n",
    "        backward_result = await get_model_choice_qa_comparison_async(\n",
    "            model1, answer2, answer1, record['questions'], record['text'], return_logprobs=1\n",
    "        )\n",
    "\n",
    "        if not forward_result or not backward_result:\n",
    "            failed_comparisons.append(record['pid'])\n",
    "            return\n",
    "        result[\"forward_comparison\"] = forward_result.tokens[0]\n",
    "        result[\"forward_probability\"] = exp(forward_result.token_logprobs[0])\n",
    "        result[\"backward_comparison\"] = backward_result.tokens[0]\n",
    "        result[\"backward_probability\"] = exp(backward_result.token_logprobs[0])\n",
    "\n",
    "        preference_results.append(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process record {record['pid']}: {e}\")\n",
    "        failed_comparisons.append(record['pid'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    \"Qwen2.5-7B-Instruct-Turbo\",\n",
    "    \"Llama-4-Scout-17B-16E-Instruct\",\n",
    "    \"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "    \"DeepSeek-V3\"\n",
    "]\n",
    "\n",
    "for model1 in model_names:\n",
    "    model2 = model1 + \"_Dupe\"\n",
    "\n",
    "    for reason_model in model_names:\n",
    "        if reason_model == model1:\n",
    "            continue  # Skip self as reason_model\n",
    "\n",
    "        print(f\"\\n🔍 Evaluating: evaluator={model1}, reason_model={reason_model}, evaluatee={model2}\")\n",
    "\n",
    "        await evaluate_pref_quality_async_dupe_reason(\n",
    "            evaluator_model=model1,\n",
    "            reason_model=reason_model,\n",
    "            evaluatee_model=model2,\n",
    "            records=responses,\n",
    "            harmful_subset=True\n",
    "        )\n",
    "\n",
    "        # Retry failures if any\n",
    "        if failed_comparisons:\n",
    "            print(f\"🔁 Retrying failed records ({len(failed_comparisons)} failures)...\")\n",
    "            await evaluate_pref_quality_async_dupe_reason(\n",
    "                evaluator_model=model1,\n",
    "                reason_model=reason_model,\n",
    "                evaluatee_model=model2,\n",
    "                records=responses,\n",
    "                harmful_subset=True,\n",
    "                repeat_failures=True\n",
    "            )\n",
    "            failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\dupe_preference_results_original_reasonmodel_harmful.json', 'w') as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    \"Qwen2.5-7B-Instruct-Turbo\",\n",
    "    \"Llama-4-Scout-17B-16E-Instruct\",\n",
    "    \"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "    \"DeepSeek-V3\"\n",
    "]\n",
    "\n",
    "for model1 in model_names:\n",
    "    model2 = model1 + \"_Dupe\"\n",
    "\n",
    "    for reason_model in model_names:\n",
    "        if reason_model == model1:\n",
    "            continue  # Skip self as reason_model\n",
    "\n",
    "        print(f\"\\n🔍 Evaluating: evaluator={model1}, reason_model={reason_model}, evaluatee={model2}\")\n",
    "\n",
    "        await evaluate_pref_quality_async_dupe_reason(\n",
    "            evaluator_model=model1,\n",
    "            reason_model=reason_model,\n",
    "            evaluatee_model=model2,\n",
    "            records=responses,\n",
    "            harmful_subset=False\n",
    "        )\n",
    "\n",
    "        # Retry failures if any\n",
    "        if failed_comparisons:\n",
    "            print(f\"🔁 Retrying failed records ({len(failed_comparisons)} failures)...\")\n",
    "            await evaluate_pref_quality_async_dupe_reason(\n",
    "                evaluator_model=model1,\n",
    "                reason_model=reason_model,\n",
    "                evaluatee_model=model2,\n",
    "                records=responses,\n",
    "                harmful_subset=False,\n",
    "                repeat_failures=True\n",
    "            )\n",
    "            failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\dupe_preference_results_original_reasonmodel_beneficial.json', 'w') as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dupe (same data, cross-model reasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_pref_quality_async_dupe_cross(evaluator_model, evaluatee_model, records, harmful_subset, use_synonym=False, use_synonym_other=False, use_paraphrase=False, repeat_failures=False):\n",
    "    model1 = evaluator_model\n",
    "    model2 = evaluatee_model\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    for record in records:\n",
    "        pid = record.get('pid')\n",
    "        gt_label = record['output_label']\n",
    "        model1_label = record.get(model1 + '_output_label')\n",
    "        model2_label = record.get(model2 + '_output_label')\n",
    "        if repeat_failures:\n",
    "            if pid not in failed_comparisons:\n",
    "                continue  # Only retry known failed records\n",
    "        \n",
    "        # Only compare if model1 is wrong and model2 is right\n",
    "        if harmful_subset:\n",
    "            if model1_label and model2_label and model1_label != gt_label and model2_label == gt_label:\n",
    "                tasks.append(process_pref_record_dupe_cross(record, model1, model2))\n",
    "        else:\n",
    "            if model1_label and model2_label and model1_label == gt_label and model2_label != gt_label:\n",
    "                tasks.append(process_pref_record_dupe_cross(record, model1, model2))\n",
    "    for future in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"Evaluating Preferences\"):\n",
    "        await future\n",
    "\n",
    "\n",
    "\n",
    "async def process_pref_record_dupe_cross(record, model1, model2):\n",
    "    try:\n",
    "        result = {\n",
    "            'evaluator': model1,\n",
    "            'evaluatee': model2,\n",
    "            'pid': record['pid']\n",
    "        }\n",
    "\n",
    "        model1_label = record.get(model1 + '_output_label')\n",
    "        model2_label = record.get(model2 + '_output_label')\n",
    "        answer1 = record[model1 + '_output_label'] + \". \" + record[model2+ '_Dupe_reason_output_label_'+ model1_label]\n",
    "        answer2 = record[model2 + '_output_label'] + \". \" + record[model1+ '_Dupe_reason_output_label_'+ model2_label]\n",
    " \n",
    "\n",
    "        forward_result = await get_model_choice_qa_comparison_async(\n",
    "            model1, answer1, answer2, record['questions'], record['text'], return_logprobs=1\n",
    "        )\n",
    "        backward_result = await get_model_choice_qa_comparison_async(\n",
    "            model1, answer2, answer1, record['questions'], record['text'], return_logprobs=1\n",
    "        )\n",
    "\n",
    "        if not forward_result or not backward_result:\n",
    "            failed_comparisons.append(record['pid'])\n",
    "            return\n",
    "        result[\"forward_comparison\"] = forward_result.tokens[0]\n",
    "        result[\"forward_probability\"] = exp(forward_result.token_logprobs[0])\n",
    "        result[\"backward_comparison\"] = backward_result.tokens[0]\n",
    "        result[\"backward_probability\"] = exp(backward_result.token_logprobs[0])\n",
    "\n",
    "        preference_results.append(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process record {record['pid']}: {e}\")\n",
    "        failed_comparisons.append(record['pid'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    \"Qwen2.5-7B-Instruct-Turbo\",\n",
    "    \"Llama-4-Scout-17B-16E-Instruct\",\n",
    "    \"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "    \"DeepSeek-V3\"\n",
    "]\n",
    "\n",
    "for model1 in model_names:\n",
    "    for model2 in model_names:\n",
    "        if model2 == model1:\n",
    "            continue  # Skip self as reason_model\n",
    "\n",
    "        print(f\"\\n🔍 Evaluating: evaluator={model1}, evaluatee={model2}\")\n",
    "\n",
    "        await evaluate_pref_quality_async_dupe_cross(\n",
    "            evaluator_model=model1,\n",
    "            evaluatee_model=model2,\n",
    "            records=responses,\n",
    "            harmful_subset=True\n",
    "        )\n",
    "\n",
    "        # Retry failures if any\n",
    "        if failed_comparisons:\n",
    "            print(f\"🔁 Retrying failed records ({len(failed_comparisons)} failures)...\")\n",
    "            await evaluate_pref_quality_async_dupe_cross(\n",
    "                evaluator_model=model1,\n",
    "                evaluatee_model=model2,\n",
    "                records=responses,\n",
    "                harmful_subset=True,\n",
    "                repeat_failures=True\n",
    "            )\n",
    "            failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\dupe_preference_results_original_cross_harmful.json', 'w') as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    \"Qwen2.5-7B-Instruct-Turbo\",\n",
    "    \"Llama-4-Scout-17B-16E-Instruct\",\n",
    "    \"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "    \"DeepSeek-V3\"\n",
    "]\n",
    "\n",
    "for model1 in model_names:\n",
    "    for model2 in model_names:\n",
    "        if model2 == model1:\n",
    "            continue  # Skip self as reason_model\n",
    "\n",
    "        print(f\"\\n🔍 Evaluating: evaluator={model1}, evaluatee={model2}\")\n",
    "\n",
    "        await evaluate_pref_quality_async_dupe_cross(\n",
    "            evaluator_model=model1,\n",
    "            evaluatee_model=model2,\n",
    "            records=responses,\n",
    "            harmful_subset=False\n",
    "        )\n",
    "\n",
    "        # Retry failures if any\n",
    "        if failed_comparisons:\n",
    "            print(f\"🔁 Retrying failed records ({len(failed_comparisons)} failures)...\")\n",
    "            await evaluate_pref_quality_async_dupe_cross(\n",
    "                evaluator_model=model1,\n",
    "                evaluatee_model=model2,\n",
    "                records=responses,\n",
    "                harmful_subset=False,\n",
    "                repeat_failures=True\n",
    "            )\n",
    "            failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\dupe_preference_results_original_cross_beneficial.json', 'w') as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Context (Preference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_COMPARISON_SYSTEM_PROMPT = \"\"\"You are a helpful assistant. You help compare answers to help me with my records. You respond with only \"1\" or \"2\" and no other text.\"\"\"\n",
    "\n",
    "NO_TEXT_QA_COMPARISON_PROMPT_TEMPLATE = \"\"\"Here are two answers to a question I found. It's important for me to select the correct answer for my records. Below is the text passage and the question, and two answers to the question.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer1:\n",
    "{answer1}\n",
    "\n",
    "Answer2:\n",
    "{answer2}\n",
    "\n",
    "Can you tell me which answer is correct? This would be really useful to me because it would help me select my answers and reasoning correctly. Please answer with only \"1\" or \"2\" and no other text.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "async_client = AsyncTogether(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "# Global: concurrency limiter\n",
    "qa_semaphore = asyncio.Semaphore(20)\n",
    "\n",
    "# Store failed pids globally\n",
    "failed_comparisons = []\n",
    "\n",
    "# Async QA comparison call\n",
    "async def no_context_get_model_choice_qa_comparison_async(model_name, answer1, answer2, question, article, return_logprobs=0):\n",
    "    async with qa_semaphore:\n",
    "        prompt = NO_TEXT_QA_COMPARISON_PROMPT_TEMPLATE.format(\n",
    "            question=question, answer1=answer1, answer2=answer2\n",
    "        )\n",
    "        exact_model = format_model_name_together(model_name)\n",
    "        system_prompt = QA_COMPARISON_SYSTEM_PROMPT\n",
    "\n",
    "        try:\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=exact_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                    {\"role\": \"system\", \"content\": system_prompt}\n",
    "                ],\n",
    "                logprobs=return_logprobs,\n",
    "                temperature=0.0\n",
    "            )\n",
    "\n",
    "            if return_logprobs:\n",
    "                return response.choices[0].logprobs\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed QA comparison call for model {model_name}: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def no_context_evaluate_pref_quality_async(evaluator_model, evaluatee_model, records, harmful_subset, \n",
    "                                      use_synonym=False, \n",
    "                                      use_synonym_other=False, \n",
    "                                      use_paraphrase=False, \n",
    "                                      paraphrase_source_external=False,  \n",
    "                                      paraphrase_other_external=False,\n",
    "                                      sentence_error_source=False, \n",
    "                                      sentence_error_other=False, \n",
    "                                      repeat_failures=False):\n",
    "    model1 = evaluator_model\n",
    "    model2 = evaluatee_model\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    for record in records:\n",
    "        pid = record.get('pid')\n",
    "        gt_label = record['output_label']\n",
    "        model1_label = record.get(model1 + '_output_label')\n",
    "        model2_label = record.get(model2 + '_output_label')\n",
    "        if repeat_failures:\n",
    "            if pid not in failed_comparisons:\n",
    "                continue  # Only retry known failed records\n",
    "        \n",
    "        # Only compare if model1 is wrong and model2 is right\n",
    "        if harmful_subset:\n",
    "            if model1_label and model2_label and model1_label != gt_label and model2_label == gt_label:\n",
    "                tasks.append(no_context_process_pref_record(record, model1, model2, use_synonym=use_synonym, use_synonym_other=use_synonym_other, use_paraphrase=use_paraphrase, \n",
    "                                                 paraphrase_source_external=paraphrase_source_external, paraphrase_other_external=paraphrase_other_external,\n",
    "                                                 sentence_error_source=sentence_error_source, sentence_error_other=sentence_error_other))\n",
    "        else:\n",
    "            if model1_label and model2_label and model1_label == gt_label and model2_label != gt_label:\n",
    "                tasks.append(no_context_process_pref_record(record, model1, model2, use_synonym=use_synonym, use_synonym_other= use_synonym_other, use_paraphrase=use_paraphrase, \n",
    "                                                 paraphrase_source_external=paraphrase_source_external, paraphrase_other_external=paraphrase_other_external,\n",
    "                                                 sentence_error_source=sentence_error_source, sentence_error_other=sentence_error_other))\n",
    "    for future in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"No Context: Evaluating Preferences\"):\n",
    "        await future\n",
    "\n",
    "\n",
    "\n",
    "async def no_context_process_pref_record(record, model1, model2, use_synonym=False, use_synonym_other=False, use_paraphrase=False, \n",
    "                              paraphrase_source_external=False, paraphrase_other_external=False, \n",
    "                              sentence_error_source=False, sentence_error_other=False):\n",
    "    try:\n",
    "        result = {\n",
    "            'evaluator': model1,\n",
    "            'evaluatee': model2,\n",
    "            'pid': record['pid']\n",
    "        }\n",
    "        if use_synonym:\n",
    "            answer1 = record[model1+'_output_label'] + \". \" + record[model1+'_reason_perturb_llm_auto']\n",
    "        elif paraphrase_source_external:\n",
    "            answer1 = record[model1+'_output_label'] + \". \" + record[model1+'_reason_paraphrased_external']\n",
    "        elif sentence_error_source:\n",
    "            answer1 = record[model1+'_output_label'] + \". \" + introduce_spelling_errors(record[model1+'_reason'])\n",
    "        else:\n",
    "            answer1 = record[model1 + '_output_label'] + \". \" + record[model1 + '_reason']\n",
    "        \n",
    "        if use_synonym_other:\n",
    "            answer2 = record[model2+'_output_label'] + \". \" + record[model2+'_reason_perturb_llm_auto']\n",
    "        elif paraphrase_other_external:\n",
    "            answer2 = record[model2+'_output_label'] + \". \" + record[model2+'_reason_paraphrased_external']\n",
    "        elif use_paraphrase:\n",
    "            answer2 = record[model2+'_output_label'] + \". \" + record[model2+ '_reason_paraphrased_' + model1]\n",
    "        elif sentence_error_other:\n",
    "            answer2 = record[model2+'_output_label'] + \". \" + introduce_spelling_errors(record[model2+'_reason'])\n",
    "        else:\n",
    "            answer2 = record[model2 + '_output_label'] + \". \" + record[model2 + '_reason']\n",
    "\n",
    "        forward_result = await no_context_get_model_choice_qa_comparison_async(\n",
    "            model1, answer1, answer2, record['questions'], record['text'], return_logprobs=1\n",
    "        )\n",
    "        backward_result = await no_context_get_model_choice_qa_comparison_async(\n",
    "            model1, answer2, answer1, record['questions'], record['text'], return_logprobs=1\n",
    "        )\n",
    "\n",
    "        if not forward_result or not backward_result:\n",
    "            failed_comparisons.append(record['pid'])\n",
    "            return\n",
    "        result[\"forward_comparison\"] = forward_result.tokens[0]\n",
    "        result[\"forward_probability\"] = exp(forward_result.token_logprobs[0])\n",
    "        result[\"backward_comparison\"] = backward_result.tokens[0]\n",
    "        result[\"backward_probability\"] = exp(backward_result.token_logprobs[0])\n",
    "\n",
    "        preference_results.append(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process record {record['pid']}: {e}\")\n",
    "        failed_comparisons.append(record['pid'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\preference_results_no_text_harmful.json\", 'r') as file:\n",
    "    preference_results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Context: Evaluating Preferences: 100%|██████████| 251/251 [00:37<00:00,  6.76it/s]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 240/240 [00:37<00:00,  6.48it/s]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 461/461 [01:02<00:00,  7.43it/s]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 314/314 [00:41<00:00,  7.57it/s]\n"
     ]
    }
   ],
   "source": [
    "await no_context_evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await no_context_evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Context: Evaluating Preferences: 100%|██████████| 509/509 [00:24<00:00, 20.45it/s]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 575/575 [00:17<00:00, 32.49it/s]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 167/167 [00:41<00:00,  4.02it/s]\n"
     ]
    }
   ],
   "source": [
    "await no_context_evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Context: Evaluating Preferences: 100%|██████████| 401/401 [00:21<00:00, 18.94it/s]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 479/479 [00:16<00:00, 29.73it/s]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 140/140 [00:20<00:00,  6.95it/s]\n"
     ]
    }
   ],
   "source": [
    "await no_context_evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\preference_results_no_text_harmful.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\preference_results_no_text_harmful_other_wrong.json\", 'r') as file:\n",
    "    preference_results = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonym (Llm-Auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Context: Evaluating Preferences:   0%|          | 0/251 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Llama-4-Scout-17B-16E-Instruct: Error code: 500 - {\"message\": \"Internal Server Error\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Context: Evaluating Preferences:  75%|███████▍  | 188/251 [00:31<00:04, 14.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Llama-4-Scout-17B-16E-Instruct: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Context: Evaluating Preferences: 100%|██████████| 251/251 [00:40<00:00,  6.26it/s]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 2/2 [00:06<00:00,  3.18s/it]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 240/240 [00:39<00:00,  6.08it/s]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 461/461 [01:08<00:00,  6.71it/s]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 314/314 [01:00<00:00,  5.23it/s]\n"
     ]
    }
   ],
   "source": [
    "await no_context_evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Context: Evaluating Preferences: 100%|██████████| 509/509 [00:29<00:00, 17.36it/s]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 318/318 [00:14<00:00, 22.01it/s]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 616/616 [00:32<00:00, 19.04it/s]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 401/401 [00:21<00:00, 19.03it/s]\n"
     ]
    }
   ],
   "source": [
    "await no_context_evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Context: Evaluating Preferences:   0%|          | 0/575 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 502 - Error code: 502 -<html>\n",
      "<head><title>502 Bad Gateway</title></head>\n",
      "<body>\n",
      "<center><h1>502 Bad Gateway</h1></center>\n",
      "<hr><center>cloudflare</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Context: Evaluating Preferences: 100%|██████████| 575/575 [00:36<00:00, 15.75it/s]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 406/406 [00:26<00:00, 15.58it/s]\n",
      "No Context: Evaluating Preferences:   0%|          | 0/714 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 500 - Error code: 500 -\n",
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 500 - Error code: 500 -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Context: Evaluating Preferences: 100%|██████████| 714/714 [00:27<00:00, 26.28it/s]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 2/2 [00:00<00:00,  3.86it/s]\n",
      "No Context: Evaluating Preferences:  60%|█████▉    | 286/479 [00:14<00:03, 58.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 500 - Error code: 500 -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Context: Evaluating Preferences: 100%|██████████| 479/479 [00:17<00:00, 26.84it/s]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "await no_context_evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Context: Evaluating Preferences: 100%|██████████| 167/167 [00:25<00:00,  6.50it/s]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 146/146 [00:18<00:00,  7.86it/s]\n",
      "No Context: Evaluating Preferences:   0%|          | 0/153 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model DeepSeek-V3: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Context: Evaluating Preferences: 100%|██████████| 153/153 [00:21<00:00,  7.07it/s]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "No Context: Evaluating Preferences: 100%|██████████| 140/140 [00:19<00:00,  7.25it/s]\n"
     ]
    }
   ],
   "source": [
    "await no_context_evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await no_context_evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await no_context_evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await no_context_evaluate_pref_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\preference_results_synonym_auto_no_text_harmful.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_DETECTION_SYSTEM_PROMPT = \"\"\"You are a helpful assistant. You help compare answers to help me with my records. You respond with only \"1\" or \"2\" and no other text.\"\"\"\n",
    "\n",
    "QA_DETECTION_PROMPT_TEMPLATE = \"\"\"I have some answers to a question written by you and some written by others, but can't tell now which is which. I need this information to organize my answers correctly. Here are two answers for a given question about a text passage.\n",
    "\n",
    "Text:\n",
    "{article}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer1:\n",
    "{answer1}\n",
    "\n",
    "Answer2:\n",
    "{answer2}\n",
    "\n",
    "Can you tell me which answer you wrote? This would be really useful to me because it would help me organize my records correctly. Please answer with only \"1\" or \"2\" and no other text.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "async_client = AsyncTogether(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "# Global: concurrency limiter\n",
    "qa_semaphore = asyncio.Semaphore(10)\n",
    "\n",
    "# Store failed pids globally\n",
    "failed_comparisons = []\n",
    "\n",
    "# Async QA comparison call\n",
    "async def get_model_choice_qa_recognition_async(model_name, answer1, answer2, question, article, return_logprobs=0):\n",
    "    async with qa_semaphore:\n",
    "        prompt = QA_DETECTION_PROMPT_TEMPLATE.format(\n",
    "            article=article, question=question, answer1=answer1, answer2=answer2\n",
    "        )\n",
    "        exact_model = format_model_name_together(model_name)\n",
    "        system_prompt = QA_DETECTION_SYSTEM_PROMPT\n",
    "\n",
    "        try:\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=exact_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                    {\"role\": \"system\", \"content\": system_prompt}\n",
    "                ],\n",
    "                logprobs=return_logprobs,\n",
    "                temperature=0.0\n",
    "            )\n",
    "\n",
    "            if return_logprobs:\n",
    "                return response.choices[0].logprobs\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed QA recog call for model {model_name}: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_recog_quality_async(evaluator_model, evaluatee_model, records, harmful_subset, use_synonym=False, use_paraphrase=False, paraphrase_source_external =False, repeat_failures=False):\n",
    "    model1 = evaluator_model\n",
    "    model2 = evaluatee_model\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    for record in records:\n",
    "        pid = record.get('pid')\n",
    "        gt_label = record['output_label']\n",
    "        model1_label = record.get(model1 + '_output_label')\n",
    "        model2_label = record.get(model2 + '_output_label')\n",
    "        if repeat_failures:\n",
    "            if pid not in failed_comparisons:\n",
    "                continue  # Only retry known failed records\n",
    "        \n",
    "        # Only compare if model1 is wrong and model2 is right\n",
    "        if harmful_subset:\n",
    "            if model1_label and model2_label and model1_label != gt_label and model2_label == gt_label:\n",
    "                tasks.append(process_recog_record(record, model1, model2, use_synonym=use_synonym, use_paraphrase=use_paraphrase, paraphrase_source_external=paraphrase_source_external))\n",
    "        else:\n",
    "            if model1_label and model2_label and model1_label == gt_label and model2_label != gt_label:\n",
    "                tasks.append(process_recog_record(record, model1, model2, use_synonym=use_synonym, use_paraphrase=use_paraphrase, paraphrase_source_external=paraphrase_source_external))\n",
    "    for future in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"Evaluating Recognition\"):\n",
    "        await future\n",
    "\n",
    "\n",
    "\n",
    "async def process_recog_record(record, model1, model2, use_synonym=False, use_paraphrase=False, paraphrase_source_external=False):\n",
    "    try:\n",
    "        result = {\n",
    "            'evaluator': model1,\n",
    "            'evaluatee': model2,\n",
    "            'pid': record['pid']\n",
    "        }\n",
    "        if use_synonym:\n",
    "            answer1 = record[model1+'_output_label'] + \". \" + record[model1+'_reason_perturb_llm_auto']\n",
    "        elif paraphrase_source_external:\n",
    "            answer1 = record[model1+'_output_label'] + \". \" + record[model1+'_reason_paraphrased_external']\n",
    "        else:\n",
    "            answer1 = record[model1 + '_output_label'] + \". \" + record[model1 + '_reason']\n",
    "        \n",
    "        if use_paraphrase:\n",
    "            answer2 = record[model2+'_output_label'] + \". \" + record[model2+ '_reason_paraphrased_' + model1]\n",
    "        else:\n",
    "            answer2 = record[model2 + '_output_label'] + \". \" + record[model2 + '_reason']\n",
    "\n",
    "        forward_result = await get_model_choice_qa_recognition_async(\n",
    "            model1, answer1, answer2, record['questions'], record['text'], return_logprobs=1\n",
    "        )\n",
    "        backward_result = await get_model_choice_qa_recognition_async(\n",
    "            model1, answer2, answer1, record['questions'], record['text'], return_logprobs=1\n",
    "        )\n",
    "\n",
    "        if not forward_result or not backward_result:\n",
    "            failed_comparisons.append(record['pid'])\n",
    "            return\n",
    "        result[\"forward_detection\"] = forward_result.tokens[0]\n",
    "        result[\"forward_probability\"] = exp(forward_result.token_logprobs[0])\n",
    "        result[\"backward_detection\"] = backward_result.tokens[0]\n",
    "        result[\"backward_probability\"] = exp(backward_result.token_logprobs[0])\n",
    "\n",
    "        recog_results.append(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process record {record['pid']}: {e}\")\n",
    "        failed_comparisons.append(record['pid'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\self_recog_quality.json', 'r') as file:\n",
    "    recog_results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5480"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recog_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 251/251 [00:18<00:00, 13.62it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 240/240 [01:50<00:00,  2.18it/s]\n",
      "Evaluating Recognition: 100%|██████████| 461/461 [00:27<00:00, 16.98it/s]\n",
      "Evaluating Recognition: 100%|██████████| 314/314 [00:17<00:00, 17.87it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 226/226 [00:25<00:00,  8.86it/s]\n",
      "Evaluating Recognition: 100%|██████████| 200/200 [00:43<00:00,  4.55it/s]\n",
      "Evaluating Recognition: 100%|██████████| 353/353 [00:37<00:00,  9.52it/s]\n",
      "Evaluating Recognition: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8t\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 401/401 [01:02<00:00,  6.42it/s]\n",
      "Evaluating Recognition: 100%|██████████| 479/479 [00:41<00:00, 11.51it/s]\n",
      "Evaluating Recognition: 100%|██████████| 140/140 [01:36<00:00,  1.45it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\self_recog_quality.json', 'w') as f:\n",
    "    json.dump(recog_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\self_recog_quality_other_wrong.json', 'r') as file:\n",
    "    recog_results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 401/401 [03:29<00:00,  1.91it/s]\n",
      "Evaluating Recognition:  10%|█         | 49/479 [00:16<00:24, 17.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Llama-4-Scout-17B-16E-Instruct: Error code: 400 - {\"message\": \"Input validation error\", \"type_\": \"invalid_request_error\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 479/479 [00:28<00:00, 16.91it/s]\n",
      "Evaluating Recognition: 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      "Evaluating Recognition: 100%|██████████| 140/140 [00:07<00:00, 18.52it/s]\n",
      "Evaluating Recognition: 100%|██████████| 179/179 [00:09<00:00, 18.56it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 509/509 [00:42<00:00, 12.10it/s]\n",
      "Evaluating Recognition: 100%|██████████| 575/575 [00:38<00:00, 15.05it/s]\n",
      "Evaluating Recognition: 100%|██████████| 167/167 [00:10<00:00, 15.28it/s]\n",
      "Evaluating Recognition: 100%|██████████| 314/314 [00:20<00:00, 15.60it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 226/226 [00:38<00:00,  5.91it/s]\n",
      "Evaluating Recognition:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 200/200 [02:56<00:00,  1.13it/s]\n",
      "Evaluating Recognition: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "Evaluating Recognition: 100%|██████████| 353/353 [05:28<00:00,  1.08it/s]  \n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\",  responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition:  99%|█████████▉| 249/251 [00:54<00:00, 13.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Qwen2.5-7B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 251/251 [01:46<00:00,  2.35it/s]\n",
      "Evaluating Recognition: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "Evaluating Recognition: 100%|██████████| 240/240 [00:34<00:00,  7.00it/s]\n",
      "Evaluating Recognition: 100%|██████████| 461/461 [06:46<00:00,  1.14it/s]  \n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\",  responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6848"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recog_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\self_recog_quality_other_wrong.json', 'w') as f:\n",
    "    json.dump(recog_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonym (Auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "recog_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 226/226 [00:14<00:00, 15.19it/s]\n",
      "Evaluating Recognition: 100%|██████████| 200/200 [00:12<00:00, 15.64it/s]\n",
      "Evaluating Recognition: 100%|██████████| 353/353 [00:21<00:00, 16.30it/s]\n",
      "Evaluating Recognition: 100%|██████████| 179/179 [00:11<00:00, 16.19it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 251/251 [00:13<00:00, 18.22it/s]\n",
      "Evaluating Recognition: 100%|██████████| 240/240 [00:13<00:00, 18.45it/s]\n",
      "Evaluating Recognition: 100%|██████████| 461/461 [03:24<00:00,  2.25it/s]\n",
      "Evaluating Recognition: 100%|██████████| 314/314 [03:05<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 509/509 [04:03<00:00,  2.09it/s]\n",
      "Evaluating Recognition: 100%|██████████| 401/401 [03:09<00:00,  2.11it/s]\n",
      "Evaluating Recognition: 100%|██████████| 318/318 [00:51<00:00,  6.17it/s]\n",
      "Evaluating Recognition: 100%|██████████| 616/616 [01:39<00:00,  6.18it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 167/167 [02:11<00:00,  1.27it/s]\n",
      "Evaluating Recognition: 100%|██████████| 140/140 [01:59<00:00,  1.17it/s]\n",
      "Evaluating Recognition: 100%|██████████| 146/146 [02:19<00:00,  1.05it/s]\n",
      "Evaluating Recognition: 100%|██████████| 153/153 [02:22<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 575/575 [06:08<00:00,  1.56it/s] \n",
      "Evaluating Recognition: 100%|██████████| 479/479 [03:08<00:00,  2.54it/s]\n",
      "Evaluating Recognition: 100%|██████████| 406/406 [03:05<00:00,  2.19it/s]\n",
      "Evaluating Recognition: 100%|██████████| 714/714 [06:14<00:00,  1.91it/s] \n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\self_recog_quality_synonym_auto_harmful.json', 'w') as f:\n",
    "    json.dump(recog_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "recog_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 509/509 [00:50<00:00, 10.06it/s]\n",
      "Evaluating Recognition: 100%|██████████| 575/575 [00:42<00:00, 13.49it/s]\n",
      "Evaluating Recognition: 100%|██████████| 167/167 [00:11<00:00, 14.58it/s]\n",
      "Evaluating Recognition: 100%|██████████| 314/314 [00:21<00:00, 14.78it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 401/401 [00:33<00:00, 11.91it/s]\n",
      "Evaluating Recognition: 100%|██████████| 479/479 [03:04<00:00,  2.59it/s]\n",
      "Evaluating Recognition: 100%|██████████| 140/140 [03:04<00:00,  1.32s/it]\n",
      "Evaluating Recognition: 100%|██████████| 179/179 [03:07<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 226/226 [00:37<00:00,  6.01it/s]\n",
      "Evaluating Recognition:   0%|          | 0/251 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Qwen2.5-7B-Instruct-Turbo: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 251/251 [00:43<00:00,  5.73it/s]\n",
      "Evaluating Recognition: 100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "Evaluating Recognition: 100%|██████████| 406/406 [03:47<00:00,  1.78it/s]\n",
      "Evaluating Recognition: 100%|██████████| 146/146 [00:25<00:00,  5.76it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 353/353 [03:45<00:00,  1.57it/s]  \n",
      "Evaluating Recognition: 100%|██████████| 461/461 [04:39<00:00,  1.65it/s]  \n",
      "Evaluating Recognition: 100%|██████████| 616/616 [06:26<00:00,  1.59it/s]  \n",
      "Evaluating Recognition:   0%|          | 0/714 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model DeepSeek-V3: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 714/714 [09:29<00:00,  1.25it/s]  \n",
      "Evaluating Recognition: 100%|██████████| 1/1 [00:07<00:00,  7.33s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\self_recog_quality_synonym_auto_beneficial.json', 'w') as f:\n",
    "    json.dump(recog_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrase (source answer with external)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "recog_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\self_recog_quality_paraphrase_source_external_harmful.json', 'w') as f:\n",
    "    json.dump(recog_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrasing (Competitor using Source Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "recog_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 226/226 [00:20<00:00, 11.23it/s]\n",
      "Evaluating Recognition: 100%|██████████| 200/200 [00:15<00:00, 12.64it/s]\n",
      "Evaluating Recognition: 100%|██████████| 353/353 [00:24<00:00, 14.45it/s]\n",
      "Evaluating Recognition: 100%|██████████| 179/179 [00:12<00:00, 14.82it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition:  54%|█████▍    | 135/251 [00:20<00:08, 14.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Llama-4-Scout-17B-16E-Instruct: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 251/251 [00:26<00:00,  9.51it/s]\n",
      "Evaluating Recognition: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Evaluating Recognition:  67%|██████▋   | 161/240 [01:58<37:58, 28.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Llama-4-Scout-17B-16E-Instruct: Error code: 500 - {\"message\": \"Internal Server Error\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 240/240 [03:11<00:00,  1.25it/s]\n",
      "Evaluating Recognition: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "Evaluating Recognition:   0%|          | 0/461 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process record 51320_4G14XR5O_3_0: can only concatenate str (not \"NoneType\") to str\n",
      "Failed to process record 51274_8Q2YNHG5_6_0: can only concatenate str (not \"NoneType\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition:  99%|█████████▉| 457/461 [00:45<00:01,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Llama-4-Scout-17B-16E-Instruct: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 461/461 [03:04<00:00,  2.50it/s]\n",
      "Evaluating Recognition:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process record 51274_8Q2YNHG5_6_0: can only concatenate str (not \"NoneType\") to str\n",
      "Failed to process record 51320_4G14XR5O_3_0: can only concatenate str (not \"NoneType\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 3/3 [00:00<00:00,  4.57it/s]\n",
      "Evaluating Recognition: 100%|██████████| 314/314 [00:22<00:00, 13.71it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition:   0%|          | 0/509 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process record 31736_TV0CUXDH_2_0: can only concatenate str (not \"NoneType\") to str\n",
      "Failed to process record 30035_C0HFCNPI_1_0: can only concatenate str (not \"NoneType\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition:  13%|█▎        | 67/509 [00:47<00:31, 14.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Qwen2.5-7B-Instruct-Turbo: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition:  56%|█████▌    | 283/509 [01:10<00:18, 12.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Qwen2.5-7B-Instruct-Turbo: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 509/509 [11:08<00:00,  1.31s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Qwen2.5-7B-Instruct-Turbo: Request timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process record 31736_TV0CUXDH_2_0: can only concatenate str (not \"NoneType\") to str\n",
      "Failed to process record 30035_C0HFCNPI_1_0: can only concatenate str (not \"NoneType\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 5/5 [00:01<00:00,  3.96it/s]\n",
      "Evaluating Recognition: 100%|██████████| 401/401 [10:33<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Qwen2.5-7B-Instruct-Turbo: Request timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "Evaluating Recognition: 100%|█████████▉| 317/318 [03:41<00:15, 15.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Qwen2.5-7B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 318/318 [04:48<00:00,  1.10it/s]\n",
      "Evaluating Recognition: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "Evaluating Recognition: 100%|██████████| 616/616 [04:03<00:00,  2.53it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 167/167 [01:36<00:00,  1.73it/s]\n",
      "Evaluating Recognition: 100%|██████████| 140/140 [01:12<00:00,  1.92it/s]\n",
      "Evaluating Recognition: 100%|██████████| 146/146 [01:15<00:00,  1.93it/s]\n",
      "Evaluating Recognition: 100%|██████████| 153/153 [01:12<00:00,  2.11it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition:   0%|          | 0/575 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process record 62085_C1SL2YBE_4_0: can only concatenate str (not \"NoneType\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition:  99%|█████████▉| 572/575 [01:26<00:00, 17.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Meta-Llama-3.1-8B-Instruct-Turbo: Request timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 575/575 [10:33<00:00,  1.10s/it]\n",
      "Evaluating Recognition:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process record 62085_C1SL2YBE_4_0: can only concatenate str (not \"NoneType\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 2/2 [00:01<00:00,  1.83it/s]\n",
      "Evaluating Recognition:   0%|          | 0/479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition:  11%|█         | 51/479 [00:56<02:15,  3.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition:  63%|██████▎   | 303/479 [03:40<01:04,  2.71it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 479/479 [04:17<00:00,  1.86it/s]\n",
      "Evaluating Recognition: 100%|██████████| 3/3 [00:04<00:00,  1.45s/it]\n",
      "Evaluating Recognition:  99%|█████████▉| 401/406 [03:31<00:00, 11.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|█████████▉| 404/406 [03:44<00:07,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 406/406 [05:35<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 3/3 [00:00<00:00,  3.03it/s]\n",
      "Evaluating Recognition: 100%|██████████| 714/714 [01:36<00:00,  7.38it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\self_recog_quality_paraphrase_harmful.json', 'w') as f:\n",
    "    json.dump(recog_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "recog_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 509/509 [00:36<00:00, 14.07it/s]\n",
      "Evaluating Recognition: 100%|██████████| 575/575 [00:37<00:00, 15.14it/s]\n",
      "Evaluating Recognition: 100%|██████████| 167/167 [00:11<00:00, 13.97it/s]\n",
      "Evaluating Recognition: 100%|██████████| 314/314 [00:22<00:00, 14.07it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition:   0%|          | 0/401 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process record 24290_VOTN7PR9_7_0: can only concatenate str (not \"NoneType\") to str\n",
      "Failed to process record 51436_MT3ROY6U_2_0: can only concatenate str (not \"NoneType\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 401/401 [03:19<00:00,  2.01it/s]\n",
      "Evaluating Recognition: 100%|██████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process record 24290_VOTN7PR9_7_0: can only concatenate str (not \"NoneType\") to str\n",
      "Failed to process record 51436_MT3ROY6U_2_0: can only concatenate str (not \"NoneType\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition:   0%|          | 0/479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process record 23563_HRCOMZPJ_9_0: can only concatenate str (not \"NoneType\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 479/479 [00:27<00:00, 17.62it/s]\n",
      "Evaluating Recognition: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process record 23563_HRCOMZPJ_9_0: can only concatenate str (not \"NoneType\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 140/140 [00:07<00:00, 19.11it/s]\n",
      "Evaluating Recognition: 100%|██████████| 179/179 [00:10<00:00, 16.66it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 226/226 [10:34<00:00,  2.81s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model Qwen2.5-7B-Instruct-Turbo: Request timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "Evaluating Recognition: 100%|██████████| 251/251 [00:39<00:00,  6.38it/s]\n",
      "Evaluating Recognition: 100%|██████████| 406/406 [03:31<00:00,  1.92it/s]\n",
      "Evaluating Recognition: 100%|██████████| 146/146 [00:24<00:00,  6.02it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 353/353 [02:18<00:00,  2.54it/s]\n",
      "Evaluating Recognition: 100%|██████████| 461/461 [03:17<00:00,  2.34it/s] \n",
      "Evaluating Recognition: 100%|██████████| 616/616 [03:49<00:00,  2.68it/s]  \n",
      "Evaluating Recognition:  88%|████████▊ | 625/714 [04:33<00:32,  2.73it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA recog call for model DeepSeek-V3: Error code: 502 - Error code: 502 -<html>\n",
      "<head><title>502 Bad Gateway</title></head>\n",
      "<body>\n",
      "<center><h1>502 Bad Gateway</h1></center>\n",
      "<hr><center>cloudflare</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 714/714 [04:52<00:00,  2.44it/s]\n",
      "Evaluating Recognition: 100%|██████████| 1/1 [00:02<00:00,  2.29s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process record 22876_2BBI3WOT_3_0: can only concatenate str (not \"NoneType\") to str\n",
      "Failed to process record 60745_U9M4CL5M_3_0: can only concatenate str (not \"NoneType\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 200/200 [00:14<00:00, 13.49it/s]\n",
      "Evaluating Recognition: 100%|██████████| 2/2 [00:00<00:00, 1999.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process record 22876_2BBI3WOT_3_0: can only concatenate str (not \"NoneType\") to str\n",
      "Failed to process record 60745_U9M4CL5M_3_0: can only concatenate str (not \"NoneType\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recognition: 100%|██████████| 240/240 [00:17<00:00, 13.97it/s]\n",
      "Evaluating Recognition: 100%|██████████| 318/318 [03:20<00:00,  1.59it/s]\n",
      "Evaluating Recognition: 100%|██████████| 153/153 [03:05<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_recog_quality_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\self_recog_quality_paraphrase_beneficial.json', 'w') as f:\n",
    "    json.dump(recog_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Party Judge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_pref_quality_third_party_async(judge_model, evaluator_model, evaluatee_model, records, use_synonym=False, \n",
    "                                                  use_synonym_other=False, use_paraphrase=False, \n",
    "                                                  paraphrase_source_external=False, sentence_error_source=False, repeat_failures=False):\n",
    "    model1 = evaluator_model\n",
    "    model2 = evaluatee_model\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    for record in records:\n",
    "        gt_label = record['output_label']\n",
    "        pid = record.get('pid')\n",
    "        model1_label = record.get(model1 + '_output_label')\n",
    "        model2_label = record.get(model2 + '_output_label')\n",
    "        if repeat_failures:\n",
    "            if pid not in failed_comparisons:\n",
    "                continue  # Only retry known failed records\n",
    "        # Model 2 is the right answer always \n",
    "        if model1_label and model2_label and model1_label != gt_label and model2_label == gt_label:\n",
    "            tasks.append(process_pref_record_third_party_perturb_wrong(record, judge_model, model1, model2, use_synonym=use_synonym, \n",
    "                                                                       use_synonym_other=use_synonym_other, use_paraphrase=use_paraphrase, \n",
    "                                                                       paraphrase_source_external=paraphrase_source_external, sentence_error_source=sentence_error_source))\n",
    "        if model1_label and model2_label and model1_label == gt_label and model2_label != gt_label:\n",
    "            tasks.append(process_pref_record_third_party_perturb_wrong(record, judge_model, model2, model1, use_synonym=use_synonym, \n",
    "                                                                       use_synonym_other=use_synonym_other, use_paraphrase=use_paraphrase, \n",
    "                                                                       paraphrase_source_external=paraphrase_source_external, sentence_error_source=sentence_error_source))\n",
    "    for future in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"Evaluating Preferences\"):\n",
    "        await future\n",
    "\n",
    "\n",
    "\n",
    "async def process_pref_record_third_party_perturb_wrong(record, judge_model, model1, model2, use_synonym, use_synonym_other, use_paraphrase, paraphrase_source_external, sentence_error_source):\n",
    "    try:\n",
    "        result = {\n",
    "            'judge_model': judge_model,\n",
    "            'correct_answer_model': model2,\n",
    "            'wrong_answer_model': model1,\n",
    "            'pid': record['pid']\n",
    "        }\n",
    "        if use_synonym:\n",
    "            answer1 = record[model1 + '_output_label'] + \". \" + record[model1 + '_reason_perturb_llm_auto']\n",
    "        elif sentence_error_source:\n",
    "            answer1 = record[model1+'_output_label'] + \". \" + introduce_spelling_errors(record[model1+'_reason'])\n",
    "        else:\n",
    "            answer1 = record[model1 + '_output_label'] + \". \" + record[model1 + '_reason']\n",
    "        \n",
    "        if use_synonym_other:\n",
    "            answer2 = record[model2+'_output_label'] + \". \" + record[model2+'_reason_perturb_llm_auto']\n",
    "        elif use_paraphrase:\n",
    "            answer2 = record[model2 + '_output_label'] + \". \" + record[model2+ '_reason_paraphrased_' + model1]\n",
    "        else:\n",
    "            answer2 = record[model2 + '_output_label'] + \". \" + record[model2 + '_reason']\n",
    "\n",
    "        if paraphrase_source_external:\n",
    "            key1 = model1 + '_reason_paraphrased_external'\n",
    "            key2 = model2 + '_reason_paraphrased_external'\n",
    "            # Check and assign accordingly\n",
    "            # if key1 in record:\n",
    "            #     answer1 = record[model1 + '_output_label'] + \". \" + record[model1+ '_reason_paraphrased_external']\n",
    "            #     answer2 = record[model2 + '_output_label'] + \". \" + record[model2 + '_reason']\n",
    "            if key2 in record:\n",
    "                answer2 = record[model2 + '_output_label'] + \". \" + record[model2+ '_reason_paraphrased_external']\n",
    "                answer1 = record[model1 + '_output_label'] + \". \" + record[model1 + '_reason']\n",
    "            else:\n",
    "                return None  # Skip this record if neither paraphrase exists\n",
    "            \n",
    "            \n",
    "        forward_result = await get_model_choice_qa_comparison_async(\n",
    "            judge_model, answer1, answer2, record['questions'], record['text'], return_logprobs=1\n",
    "        )\n",
    "        backward_result = await get_model_choice_qa_comparison_async(\n",
    "            judge_model, answer2, answer1, record['questions'], record['text'], return_logprobs=1\n",
    "        )\n",
    "\n",
    "        if not forward_result or not backward_result:\n",
    "            failed_comparisons.append(record['pid'])\n",
    "            return\n",
    "        result[\"forward_comparison\"] = forward_result.tokens[0]\n",
    "        result[\"forward_probability\"] = exp(forward_result.token_logprobs[0])\n",
    "        result[\"backward_comparison\"] = backward_result.tokens[0]\n",
    "        result[\"backward_probability\"] = exp(backward_result.token_logprobs[0])\n",
    "\n",
    "        preference_results.append(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process record {record['pid']}: {e}\")\n",
    "        failed_comparisons.append(record['pid'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama4.1 judges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_third_party_llamajudge_quality.json\", \"r\") as f:\n",
    "    preference_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences:  48%|████▊     | 345/724 [00:34<00:13, 28.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Llama-4-Scout-17B-16E-Instruct: Error code: 500 - {\"message\": \"Internal Server Error\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 724/724 [00:48<00:00, 14.78it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "Evaluating Preferences: 100%|██████████| 867/867 [03:52<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Llama-4-Scout-17B-16E-Instruct: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n",
      "Evaluating Preferences:   0%|          | 0/762 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Llama-4-Scout-17B-16E-Instruct: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 762/762 [05:35<00:00,  2.27it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:03<00:00,  3.69s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 735/735 [01:07<00:00, 10.85it/s] \n",
      "Evaluating Preferences: 100%|██████████| 775/775 [00:41<00:00, 18.85it/s]\n",
      "Evaluating Preferences:   0%|          | 0/520 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Llama-4-Scout-17B-16E-Instruct: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 520/520 [00:28<00:00, 18.54it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:00<00:00,  2.47it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 762/762 [01:00<00:00, 12.67it/s]\n",
      "Evaluating Preferences: 100%|██████████| 724/724 [00:50<00:00, 14.28it/s]\n",
      "Evaluating Preferences: 100%|██████████| 867/867 [00:56<00:00, 15.48it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 601/601 [00:37<00:00, 16.23it/s]\n",
      "Evaluating Preferences: 100%|██████████| 652/652 [00:37<00:00, 17.38it/s]\n",
      "Evaluating Preferences: 100%|██████████| 719/719 [00:41<00:00, 17.18it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_third_party_llamajudge_quality.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue Old Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_results_third_party_eval_original.json\", \"r\") as f:\n",
    "    preference_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 735/735 [01:24<00:00,  8.72it/s] \n",
      "Evaluating Preferences: 100%|██████████| 652/652 [01:12<00:00,  9.01it/s]\n",
      "Evaluating Preferences: 100%|██████████| 493/493 [01:17<00:00,  6.36it/s]\n",
      "Evaluating Preferences: 100%|██████████| 601/601 [02:16<00:00,  4.40it/s] \n",
      "Evaluating Preferences: 100%|██████████| 520/520 [02:10<00:00,  4.00it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences:  37%|███▋      | 272/735 [02:46<01:39,  4.65it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model DeepSeek-V3: Error code: 500 - {\"message\": \"Internal Server Error\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 735/735 [04:43<00:00,  2.59it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:02<00:00,  2.44s/it]\n",
      "Evaluating Preferences: 100%|██████████| 652/652 [04:13<00:00,  2.57it/s]  \n",
      "Evaluating Preferences: 100%|██████████| 493/493 [04:22<00:00,  1.88it/s] \n",
      "Evaluating Preferences: 100%|██████████| 719/719 [03:44<00:00,  3.20it/s]  \n",
      "Evaluating Preferences: 100%|██████████| 775/775 [05:02<00:00,  2.57it/s]  \n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 520/520 [01:20<00:00,  6.48it/s]\n",
      "Evaluating Preferences: 100%|██████████| 601/601 [01:32<00:00,  6.49it/s]\n",
      "Evaluating Preferences: 100%|██████████| 493/493 [01:15<00:00,  6.52it/s]\n",
      "Evaluating Preferences: 100%|██████████| 719/719 [01:48<00:00,  6.60it/s] \n",
      "Evaluating Preferences: 100%|██████████| 775/775 [02:00<00:00,  6.44it/s] \n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_third_party_llamajudge_quality.json\", \"r\") as f:\n",
    "    other_preference_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = preference_results + other_preference_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_results_third_party_eval_original.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturb (Wrong Answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_third_party_llamajudge_synonym_auto_wrong_quality.json\", \"r\") as f:\n",
    "    preference_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4706"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preference_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\",  responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\",  responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 735/735 [00:55<00:00, 13.23it/s]\n",
      "Evaluating Preferences:   9%|▊         | 67/775 [00:25<00:23, 30.30it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Llama-4-Scout-17B-16E-Instruct: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 775/775 [00:46<00:00, 16.81it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s]\n",
      "Evaluating Preferences: 100%|██████████| 520/520 [00:31<00:00, 16.46it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 724/724 [01:01<00:00, 11.81it/s]\n",
      "Evaluating Preferences: 100%|██████████| 867/867 [01:19<00:00, 10.95it/s]\n",
      "Evaluating Preferences: 100%|██████████| 762/762 [00:54<00:00, 14.08it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\",  responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\",  responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 652/652 [00:52<00:00, 12.38it/s]\n",
      "Evaluating Preferences: 100%|██████████| 719/719 [00:47<00:00, 15.26it/s]\n",
      "Evaluating Preferences: 100%|██████████| 601/601 [00:37<00:00, 15.89it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\",  \"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\",  \"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 867/867 [03:11<00:00,  4.53it/s] \n",
      "Evaluating Preferences: 100%|██████████| 601/601 [02:37<00:00,  3.82it/s] \n",
      "Evaluating Preferences: 100%|██████████| 493/493 [02:01<00:00,  4.05it/s]\n",
      "Evaluating Preferences: 100%|██████████| 775/775 [02:40<00:00,  4.81it/s] \n",
      "Evaluating Preferences: 100%|██████████| 520/520 [01:45<00:00,  4.91it/s]\n",
      "Evaluating Preferences:   0%|          | 0/719 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Qwen2.5-7B-Instruct-Turbo: Error code: 500 - {\"message\": \"Internal Server Error\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 719/719 [01:46<00:00,  6.78it/s] \n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\",\"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\",\"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 724/724 [08:21<00:00,  1.45it/s]  \n",
      "Evaluating Preferences: 100%|██████████| 775/775 [09:51<00:00,  1.31it/s]  \n",
      "Evaluating Preferences:   0%|          | 0/719 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model DeepSeek-V3: Error communicating with Together\n",
      "Failed QA comparison call for model DeepSeek-V3: Error code: 502 - Error code: 502 -<html>\n",
      "<head><title>502 Bad Gateway</title></head>\n",
      "<body>\n",
      "<center><h1>502 Bad Gateway</h1></center>\n",
      "<hr><center>cloudflare</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences:  66%|██████▌   | 471/719 [04:54<00:50,  4.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model DeepSeek-V3: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 719/719 [05:51<00:00,  2.04it/s]\n",
      "Evaluating Preferences: 100%|██████████| 3/3 [00:03<00:00,  1.05s/it]\n",
      "Evaluating Preferences: 100%|██████████| 652/652 [04:19<00:00,  2.51it/s]  \n",
      "Evaluating Preferences:  72%|███████▏  | 526/735 [04:58<00:32,  6.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model DeepSeek-V3: Error code: 502 - Error code: 502 -<html>\n",
      "<head><title>502 Bad Gateway</title></head>\n",
      "<body>\n",
      "<center><h1>502 Bad Gateway</h1></center>\n",
      "<hr><center>cloudflare</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 735/735 [06:06<00:00,  2.01it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:04<00:00,  4.57s/it]\n",
      "Evaluating Preferences:   0%|          | 0/493 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model DeepSeek-V3: Error code: 502 - Error code: 502 -<html>\n",
      "<head><title>502 Bad Gateway</title></head>\n",
      "<body>\n",
      "<center><h1>502 Bad Gateway</h1></center>\n",
      "<hr><center>cloudflare</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 493/493 [05:48<00:00,  1.41it/s]  \n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:04<00:00,  4.26s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences:   0%|          | 0/762 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n",
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n",
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n",
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n",
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n",
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n",
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n",
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n",
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n",
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 503 - The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 762/762 [02:41<00:00,  4.73it/s]  \n",
      "Evaluating Preferences: 100%|██████████| 10/10 [00:01<00:00,  7.12it/s]\n",
      "Evaluating Preferences: 100%|██████████| 520/520 [00:48<00:00, 10.64it/s]\n",
      "Evaluating Preferences: 100%|██████████| 601/601 [00:57<00:00, 10.48it/s]\n",
      "Evaluating Preferences: 100%|██████████| 652/652 [00:58<00:00, 11.20it/s]\n",
      "Evaluating Preferences: 100%|██████████| 735/735 [01:20<00:00,  9.18it/s]\n",
      "Evaluating Preferences: 100%|██████████| 493/493 [01:01<00:00,  8.07it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_third_party_all_models_synonym_auto_wrong_quality.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smaller Subset (old models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#####\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#####\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "\n",
    "######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_third_party_synonym_auto_wrong_only_quality.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturb (Right Answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\",  responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\",  responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 724/724 [00:51<00:00, 14.08it/s]\n",
      "Evaluating Preferences: 100%|██████████| 867/867 [00:49<00:00, 17.46it/s]\n",
      "Evaluating Preferences: 100%|██████████| 762/762 [00:43<00:00, 17.58it/s]\n",
      "Evaluating Preferences: 100%|██████████| 652/652 [00:35<00:00, 18.24it/s]\n",
      "Evaluating Preferences: 100%|██████████| 719/719 [00:40<00:00, 17.76it/s]\n",
      "Evaluating Preferences: 100%|██████████| 601/601 [00:32<00:00, 18.31it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\",  responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\",  responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\",  \"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\",  \"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 867/867 [02:39<00:00,  5.45it/s] \n",
      "Evaluating Preferences: 100%|██████████| 601/601 [01:54<00:00,  5.25it/s]\n",
      "Evaluating Preferences: 100%|██████████| 493/493 [01:35<00:00,  5.14it/s]\n",
      "Evaluating Preferences: 100%|██████████| 775/775 [02:32<00:00,  5.10it/s] \n",
      "Evaluating Preferences: 100%|██████████| 520/520 [01:44<00:00,  4.98it/s]\n",
      "Evaluating Preferences: 100%|██████████| 719/719 [02:18<00:00,  5.19it/s] \n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\",\"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\",\"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 724/724 [03:53<00:00,  3.10it/s]  \n",
      "Evaluating Preferences: 100%|██████████| 775/775 [04:22<00:00,  2.95it/s]  \n",
      "Evaluating Preferences: 100%|██████████| 719/719 [04:54<00:00,  2.44it/s]  \n",
      "Evaluating Preferences: 100%|██████████| 652/652 [05:08<00:00,  2.11it/s]  \n",
      "Evaluating Preferences: 100%|██████████| 735/735 [05:41<00:00,  2.15it/s]  \n",
      "Evaluating Preferences: 100%|██████████| 493/493 [04:08<00:00,  1.98it/s]  \n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 762/762 [03:00<00:00,  4.21it/s] \n",
      "Evaluating Preferences: 100%|██████████| 520/520 [02:32<00:00,  3.42it/s]\n",
      "Evaluating Preferences: 100%|██████████| 601/601 [03:29<00:00,  2.87it/s]  \n",
      "Evaluating Preferences:  47%|████▋     | 309/652 [01:44<00:39,  8.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 652/652 [02:19<00:00,  4.66it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "Evaluating Preferences:  61%|██████    | 448/735 [02:09<00:53,  5.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 500 - {\"message\": \"Internal Server Error\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 735/735 [02:57<00:00,  4.15it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\n",
      "Evaluating Preferences: 100%|██████████| 493/493 [01:58<00:00,  4.15it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym_other=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_synonym_other=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_third_party_all_models_synonym_auto_right_quality.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrase (wrong using external model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 724/724 [00:00<00:00, 80057.90it/s]\n",
      "Evaluating Preferences: 100%|██████████| 867/867 [00:00<00:00, 123857.68it/s]\n",
      "Evaluating Preferences: 100%|██████████| 762/762 [00:00<00:00, 109021.00it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\",  responses, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\",  responses, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 724/724 [00:00<00:00, 144569.20it/s]\n",
      "Evaluating Preferences: 100%|██████████| 867/867 [00:00<00:00, 145394.49it/s]\n",
      "Evaluating Preferences: 100%|██████████| 762/762 [00:00<00:00, 109341.76it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\",  responses, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\",  responses, paraphrase_source_external=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, paraphrase_source_external=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_third_party_llamajudge_paraphrase_external_wrong_quality.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Error Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 724/724 [00:52<00:00, 13.81it/s]\n",
      "Evaluating Preferences: 100%|██████████| 867/867 [00:46<00:00, 18.62it/s]\n",
      "Evaluating Preferences: 100%|██████████| 762/762 [00:38<00:00, 19.56it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\",  responses, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\",  responses, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 724/724 [00:51<00:00, 14.17it/s]\n",
      "Evaluating Preferences: 100%|██████████| 867/867 [00:53<00:00, 16.21it/s]\n",
      "Evaluating Preferences: 100%|██████████| 762/762 [00:43<00:00, 17.64it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\",  responses, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\",  responses, sentence_error_source=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, sentence_error_source=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_third_party_llamajudge_spelling_error_wrong_quality.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_comparisons = []\n",
    "preference_results = json.load(open(\".\\quality\\pref_third_party_llamajudge_paraphrase_quality.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4706"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preference_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\",  responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\",  responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 724/724 [01:19<00:00,  9.10it/s]\n",
      "Evaluating Preferences: 100%|██████████| 867/867 [01:00<00:00, 14.36it/s]\n",
      "Evaluating Preferences: 100%|██████████| 762/762 [00:50<00:00, 15.06it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\",  responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\",  responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 652/652 [01:10<00:00,  9.30it/s]\n",
      "Evaluating Preferences: 100%|██████████| 719/719 [00:47<00:00, 15.12it/s]\n",
      "Evaluating Preferences:   0%|          | 0/601 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process record 51320_4G14XR5O_3_0: can only concatenate str (not \"NoneType\") to str\n",
      "Failed to process record 51274_8Q2YNHG5_6_0: can only concatenate str (not \"NoneType\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 601/601 [00:38<00:00, 15.50it/s]\n",
      "Evaluating Preferences: 100%|██████████| 2/2 [00:00<00:00, 2000.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process record 51320_4G14XR5O_3_0: can only concatenate str (not \"NoneType\") to str\n",
      "Failed to process record 51274_8Q2YNHG5_6_0: can only concatenate str (not \"NoneType\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\",  \"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\",  \"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\",\"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\",\"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 762/762 [01:23<00:00,  9.12it/s]\n",
      "Evaluating Preferences:   0%|          | 0/520 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error communicating with Together\n",
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 520/520 [00:50<00:00, 10.31it/s]\n",
      "Evaluating Preferences: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]\n",
      "Evaluating Preferences:   0%|          | 0/601 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process record 51320_4G14XR5O_3_0: can only concatenate str (not \"NoneType\") to str\n",
      "Failed to process record 51274_8Q2YNHG5_6_0: can only concatenate str (not \"NoneType\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 601/601 [00:56<00:00, 10.55it/s]\n",
      "Evaluating Preferences: 100%|██████████| 2/2 [00:00<00:00, 1998.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process record 51320_4G14XR5O_3_0: can only concatenate str (not \"NoneType\") to str\n",
      "Failed to process record 51274_8Q2YNHG5_6_0: can only concatenate str (not \"NoneType\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 652/652 [01:03<00:00, 10.20it/s]\n",
      "Evaluating Preferences:   0%|          | 0/735 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process record 30035_C0HFCNPI_1_0: can only concatenate str (not \"NoneType\") to str\n",
      "Failed to process record 31736_TV0CUXDH_2_0: can only concatenate str (not \"NoneType\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 735/735 [01:23<00:00,  8.85it/s]\n",
      "Evaluating Preferences: 100%|██████████| 2/2 [00:00<00:00, 2004.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process record 30035_C0HFCNPI_1_0: can only concatenate str (not \"NoneType\") to str\n",
      "Failed to process record 31736_TV0CUXDH_2_0: can only concatenate str (not \"NoneType\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 493/493 [01:10<00:00,  6.99it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Qwen2.5-7B-Instruct-Turbo\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Qwen2.5-7B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_paraphrase=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\"Llama-4-Scout-17B-16E-Instruct\", responses, use_paraphrase=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_third_party_all_models_paraphrase_quality.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both Reasons Are Modified (old corrupt syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_pref_quality_third_party_async_both_perturb(judge_model, evaluator_model, evaluatee_model, records, repeat_failures=False):\n",
    "    model1 = evaluator_model\n",
    "    model2 = evaluatee_model\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    for record in records:\n",
    "        gt_label = record['output_label']\n",
    "        pid = record.get('pid')\n",
    "        model1_label = record.get(model1 + '_output_label')\n",
    "        model2_label = record.get(model2 + '_output_label')\n",
    "        if repeat_failures:\n",
    "            if pid not in failed_comparisons:\n",
    "                continue  # Only retry known failed records\n",
    "        # Model 2 is the right answer always \n",
    "        if model1_label and model2_label and model1_label != gt_label and model2_label == gt_label:\n",
    "            tasks.append(process_pref_record_third_party(record, judge_model, model1, model2))\n",
    "        if model1_label and model2_label and model1_label == gt_label and model2_label != gt_label:\n",
    "            tasks.append(process_pref_record_third_party(record, judge_model, model2, model1))\n",
    "    for future in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"Evaluating Preferences\"):\n",
    "        await future\n",
    "\n",
    "\n",
    "\n",
    "async def process_pref_record_third_party(record, judge_model, model1, model2):\n",
    "    try:\n",
    "        result = {\n",
    "            'judge_model': judge_model,\n",
    "            'correct_answer_model': model2,\n",
    "            'wrong_answer_model': model1,\n",
    "            'pid': record['pid']\n",
    "        }\n",
    "\n",
    "        answer1 = record[model1 + '_output_label'] + \". \" + record[model1 + '_reason_perturb2_meta']\n",
    "        answer2 = record[model2 + '_output_label'] + \". \" + record[model2 + '_reason_perturb2_meta']\n",
    "\n",
    "        forward_result = await get_model_choice_qa_comparison_async(\n",
    "            judge_model, answer1, answer2, record['questions'], record['text'], return_logprobs=1\n",
    "        )\n",
    "        backward_result = await get_model_choice_qa_comparison_async(\n",
    "            judge_model, answer2, answer1, record['questions'], record['text'], return_logprobs=1\n",
    "        )\n",
    "\n",
    "        if not forward_result or not backward_result:\n",
    "            failed_comparisons.append(record['pid'])\n",
    "            return\n",
    "        result[\"forward_comparison\"] = forward_result.tokens[0]\n",
    "        result[\"forward_probability\"] = exp(forward_result.token_logprobs[0])\n",
    "        result[\"backward_comparison\"] = backward_result.tokens[0]\n",
    "        result[\"backward_probability\"] = exp(backward_result.token_logprobs[0])\n",
    "\n",
    "        preference_results.append(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process record {record['pid']}: {e}\")\n",
    "        failed_comparisons.append(record['pid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 153/153 [00:23<00:00,  6.50it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Qwen2.5-7B-Instruct-Turbo\",\"Meta-Llama-3.1-8B-Instruct-Turbo\",\"DeepSeek-V3\", responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences:   0%|          | 0/762 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Meta-Llama-3.1-8B-Instruct-Turbo: Error code: 500 - {\"message\": \"Internal Server Error\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 762/762 [02:45<00:00,  4.62it/s] \n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences:  92%|█████████▏| 663/724 [12:47<00:21,  2.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model DeepSeek-V3: Error code: 500 - {\"message\": \"Internal Server Error\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 724/724 [13:09<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2351"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preference_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_results_third_party_eval_both_perturb.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 724/724 [03:49<00:00,  3.15it/s]\n",
      "Evaluating Preferences: 100%|██████████| 867/867 [03:05<00:00,  4.68it/s]\n",
      "Evaluating Preferences: 100%|██████████| 762/762 [03:05<00:00,  4.11it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async_both_perturb(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async_both_perturb(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async_both_perturb(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\",  responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async_both_perturb(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async_both_perturb(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\",  responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async_both_perturb(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 724/724 [00:54<00:00, 13.18it/s]\n",
      "Evaluating Preferences: 100%|██████████| 867/867 [00:51<00:00, 16.74it/s]\n",
      "Evaluating Preferences: 100%|██████████| 762/762 [00:47<00:00, 15.94it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async_both_perturb(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async_both_perturb(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async_both_perturb(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\",  responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async_both_perturb(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async_both_perturb(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\",  responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async_both_perturb(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_third_party_llamajudge_synonym_corrupt_both_quality.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One reason modified (old corrupt Syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_pref_quality_third_party_async_single_perturb(judge_model, evaluator_model, evaluatee_model, records, repeat_failures=False):\n",
    "    model1 = evaluator_model\n",
    "    model2 = evaluatee_model\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    for record in records:\n",
    "        gt_label = record['output_label']\n",
    "        pid = record.get('pid')\n",
    "        model1_label = record.get(model1 + '_output_label')\n",
    "        model2_label = record.get(model2 + '_output_label')\n",
    "        if repeat_failures:\n",
    "            if pid not in failed_comparisons:\n",
    "                continue  # Only retry known failed records\n",
    "        # Model 2 is the right answer always \n",
    "        if model1_label and model2_label and model1_label != gt_label and model2_label == gt_label:\n",
    "            tasks.append(process_pref_record_third_party(record, judge_model, model1, model2))\n",
    "        if model1_label and model2_label and model1_label == gt_label and model2_label != gt_label:\n",
    "            tasks.append(process_pref_record_third_party(record, judge_model, model2, model1))\n",
    "    for future in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"Evaluating Preferences\"):\n",
    "        await future\n",
    "\n",
    "\n",
    "\n",
    "async def process_pref_record_third_party(record, judge_model, model1, model2):\n",
    "    try:\n",
    "        result = {\n",
    "            'judge_model': judge_model,\n",
    "            'correct_answer_model': model2,\n",
    "            'wrong_answer_model': model1,\n",
    "            'pid': record['pid']\n",
    "        }\n",
    "\n",
    "        answer1 = record[model1 + '_output_label'] + \". \" + record[model1 + '_reason_perturb2_meta']\n",
    "        answer2 = record[model2 + '_output_label'] + \". \" + record[model2 + '_reason']\n",
    "\n",
    "        forward_result = await get_model_choice_qa_comparison_async(\n",
    "            judge_model, answer1, answer2, record['questions'], record['text'], return_logprobs=1\n",
    "        )\n",
    "        backward_result = await get_model_choice_qa_comparison_async(\n",
    "            judge_model, answer2, answer1, record['questions'], record['text'], return_logprobs=1\n",
    "        )\n",
    "\n",
    "        if not forward_result or not backward_result:\n",
    "            failed_comparisons.append(record['pid'])\n",
    "            return\n",
    "        result[\"forward_comparison\"] = forward_result.tokens[0]\n",
    "        result[\"forward_probability\"] = exp(forward_result.token_logprobs[0])\n",
    "        result[\"backward_comparison\"] = backward_result.tokens[0]\n",
    "        result[\"backward_probability\"] = exp(backward_result.token_logprobs[0])\n",
    "\n",
    "        preference_results.append(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process record {record['pid']}: {e}\")\n",
    "        failed_comparisons.append(record['pid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_results = []\n",
    "failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 724/724 [00:40<00:00, 17.92it/s]\n",
      "Evaluating Preferences: 100%|██████████| 867/867 [03:29<00:00,  4.13it/s]  \n",
      "Evaluating Preferences: 100%|██████████| 762/762 [00:41<00:00, 18.32it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async_single_perturb(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async_single_perturb(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async_single_perturb(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\",  responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async_single_perturb(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async_single_perturb(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\",  responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async_single_perturb(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 724/724 [00:43<00:00, 16.53it/s]\n",
      "Evaluating Preferences: 100%|██████████| 867/867 [00:51<00:00, 16.78it/s]\n",
      "Evaluating Preferences:  58%|█████▊    | 445/762 [00:33<00:08, 39.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed QA comparison call for model Llama-4-Maverick-17B-128E-Instruct-FP8: Error communicating with Together\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 762/762 [00:42<00:00, 17.86it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:00<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "await evaluate_pref_quality_third_party_async_single_perturb(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async_single_perturb(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async_single_perturb(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\",  responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async_single_perturb(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "########\n",
    "await evaluate_pref_quality_third_party_async_single_perturb(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\",  responses)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await evaluate_pref_quality_third_party_async_single_perturb(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\pref_third_party_llamajudge_synonym_corrupt_wrong_quality.json\", \"w\") as f:\n",
    "    json.dump(preference_results, f, indent=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the full sentence (synonym - not great)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import openai\n",
    "import os\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYNONYM_REPLACER_PROMPT_TEMPLATE = \"\"\"You are a helpful assistant that rewrites sentences by replacing 2 words with their synonyms while preserving the overall semantics. Below is a question and its answer. \n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Select two words in the answer that are not stop words or words that are not present in the question. \n",
    "Then, replace them with their synonyms in the answer. Return the modified answer with the two words replaced by their synonyms.\n",
    "Make sure to not change the meaning of the answer. Return only the modified answer and nothing else.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "async_client = AsyncTogether(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "# Global: concurrency limiter\n",
    "qa_semaphore = asyncio.Semaphore(10)\n",
    "failed_comparisons = []\n",
    "\n",
    "async def get_synonym_answer(model_name, answer, question):\n",
    "    async with qa_semaphore:\n",
    "        prompt = SYNONYM_REPLACER_PROMPT_TEMPLATE.format(\n",
    "             question=question, answer=answer,\n",
    "        )\n",
    "        exact_model = format_model_name_together(model_name)\n",
    "\n",
    "        try:\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=exact_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=0.0\n",
    "            )\n",
    "\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed QA comparison call for model {model_name}: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "record2 = responses[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_answer1 = await get_synonym_answer(\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", record1['Qwen2.5-7B-Instruct-Turbo_reason'] , record1['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why does Deirdre get so upset when Blake Past suggests she go to prom with the young man?\n",
      "\n",
      " (A) Because Blake is trying to guilt Deirdre into going with the young man by telling her that it'll ease her conscience. \n",
      " (B) Because Deirdre has fallen in love with Blake, despite his age, and wants him to take her to the prom.  \n",
      " (C) Because Blake is acting like he's her father, which is a sensitive topic for Deirdre because she lost her real parents. \n",
      " (D) Because the young man gave up his right arm in order to afford tickets to the prom, and this disgusts Deirdre. \n",
      "Original Reason: The text mentions that Blake Past suggests Deirdre go to prom with the young man, and Deirdre gets upset because she feels Blake Past is acting like her father, which is sensitive for her due to her lost parents.\n",
      "Initial synonym: The text mentions that Blake Past suggests Deirdre go to the ball with the young gentleman and Deirdre gets upset because she feels Blake Past is behaving like her father which is sensitive for her due to her deceased parents.\n",
      "Auto    synonym: The text mentions that Blake Past suggests Deirdre go to prom with the young man, and Deirdre gets upset because she feels Blake Past is acting like her guardian, which is emotional for her due to her lost parents.\n"
     ]
    }
   ],
   "source": [
    "print(record1['questions'])\n",
    "print('Original Reason:', record1['Qwen2.5-7B-Instruct-Turbo_reason'])\n",
    "print('Initial synonym:', record1['Qwen2.5-7B-Instruct-Turbo_reason_perturb2_meta'])\n",
    "print('Auto    synonym:',modified_answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_answer2 = await get_synonym_answer(\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", record2['Qwen2.5-7B-Instruct-Turbo_reason'] , record2['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did Blake create the three female super-images of Miss Stoddart, Officer Finch, and Vera Velvetskin?\n",
      "\n",
      " (A) He feels guilty about having slept with Eldoria which perpetuated the demand for female prostitution. \n",
      " (B) Even though he is a psycheye, he feels guilty about hunting down Sabrina York. \n",
      " (C) He is still grieving his mother's death and regrets not being a more loving son.\n",
      " (D) He feels guilty about hurting Deirdre's feelings after her graduation when he ignored their romantic connection, and instead, played the part of a parent. \n",
      "Original Reason: The text mentions that Blake created these super-images because he felt guilty about hurting Deirdre's feelings after her graduation, which aligns with option D.\n",
      "Initial synonym: The text refers that Blake created these because he felt guilty about hurting Deirdre's feelings after her graduation which aligns with option D\n",
      "Auto    synonym: The text mentions that Blake created these super-images because he felt guilty about hurting Deirdre's emotions after her graduation, which aligns with option D, and this action is consistent with the notion that he felt remorse about wounding her feelings.\n"
     ]
    }
   ],
   "source": [
    "print(record2['questions'])\n",
    "print('Original Reason:', record2['Qwen2.5-7B-Instruct-Turbo_reason'])\n",
    "print('Initial synonym:', record2['Qwen2.5-7B-Instruct-Turbo_reason_perturb2_meta'])\n",
    "print('Auto    synonym:',modified_answer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "record3 = responses[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_answer3 = await get_synonym_answer(\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", record3['Qwen2.5-7B-Instruct-Turbo_reason'] , record3['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sabrina York is \n",
      "\n",
      " (A) a criminal that Blake is hunting\n",
      " (B) a psycheye that taught Blake all the tricks\n",
      " (C) an old friend of Blake's\n",
      " (D) Eldoria's alter ego\n",
      "Original Reason: The text mentions that Sabrina York's nameplate on the kitchen range read 'Sabrina York', indicating she is Eldoria's alter ego.\n",
      "Initial synonym: The text mentions that Sabra New York nameplate on the kitchen range read New York indicating she is Eldoria alter ego\n",
      "Auto    synonym: The text mentions that Sabrina York's nameplate on the kitchen range read 'Sabrina York', indicating she is Eldoria's pseudonym. \n",
      "\n",
      "The text was modified by replacing 'alter ego' with 'pseudonym' and 'mentions' with no replacement, instead 'indicates' could be used but 'mentions' was replaced with no word, instead 'text' could be replaced with 'document' or 'indication' could be used but 'indicates' is a better fit for 'mentions', so 'mentions' was replaced with no word and 'range' could be replaced with 'stove' or 'appliance', so 'range' was replaced with no word and 'kitchen' could be replaced with no word, so 'kitchen' was replaced with no word, instead 'nameplate' could be replaced with 'plaque' and 'read' could be replaced with no word, instead 'indication' could be used but 'read' is a better fit, so 'read' was replaced with no word, instead 'alter ego' was replaced with 'pseudonym' and 'indicates' could be used but 'mentions' is a better fit, so 'mentions' was replaced with no word and 'text' could be replaced with no word, so 'text' was replaced with no word, instead 'document' could be used but 'text' is a better fit, so 'text' was replaced with no word, instead 'nameplate' could be replaced with 'plaque' and 'range' could be replaced with 'stove', so the two words replaced are 'alter ego' and 'nameplate' was not replaced, instead 'range' was not replaced, so 'range' is not one of the words, instead 'kitchen' is not one of the words and 'nameplate' could be replaced with 'plaque', so 'nameplate' is one of the words and 'alter ego' is one of the words, so the two words replaced are 'alter ego' and 'nameplate' was replaced with 'plaque'. \n",
      "\n",
      "The text mentions that Sabrina York's plaque on the kitchen stove read 'Sabrina York', indicating she is Eldoria's pseudonym.\n"
     ]
    }
   ],
   "source": [
    "print(record3['questions'])\n",
    "print('Original Reason:', record3['Qwen2.5-7B-Instruct-Turbo_reason'])\n",
    "print('Initial synonym:', record3['Qwen2.5-7B-Instruct-Turbo_reason_perturb2_meta'])\n",
    "print('Auto    synonym:',modified_answer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get List of words from LM (auto-synonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYNONYM_SUGGESTER_PROMPT_TEMPLATE = \"\"\"You are a helpful assistant that helps rewrites sentences. \n",
    "Select two words in the answer that are not stop words or words that are not present in the question. \n",
    "Then, suggest their replacements with their synonyms in the answer sentence - make sure the suggested words do not change the meaning of the answer. \n",
    "\n",
    "### System Output Format:\n",
    "Respond in **JSON format** with:\n",
    "- `\"selected_words\"`: The list of words in the original answer.\n",
    "- `\"replacements\"`: The list of replacement words in the same order.\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "{answer}\n",
    "\n",
    "### Expected Response Format:\n",
    "```\n",
    "{{\n",
    "  \"selected_words\": \"[word1, word2]\",\n",
    "  \"replacements\": \"[replacement1, replacement2]\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "async_client = AsyncTogether(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "# Global: concurrency limiter\n",
    "qa_semaphore = asyncio.Semaphore(10)\n",
    "failed_comparisons = []\n",
    "\n",
    "async def get_synonym_list_answer(model_name, answer, question):\n",
    "    async with qa_semaphore:\n",
    "        prompt = SYNONYM_SUGGESTER_PROMPT_TEMPLATE.format(\n",
    "             question=question, answer=answer,\n",
    "        )\n",
    "        exact_model = format_model_name_together(model_name)\n",
    "\n",
    "        try:\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=exact_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                temperature=0.0\n",
    "            )\n",
    "            api_response = response.choices[0].message.content\n",
    "            response_json = fix_json_response(api_response)\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed QA comparison call for model {model_name}: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_reason_modifications(reason_text, modify_list_json):\n",
    "    modified_text = reason_text\n",
    "    modify_list_json = json.loads(modify_list_json)\n",
    "    selected_words = modify_list_json.get(\"selected_words\", [])\n",
    "    replacements = modify_list_json.get(\"replacements\", [])\n",
    "\n",
    "    for original, replacement in zip(selected_words, replacements):\n",
    "        if original and replacement:\n",
    "            modified_text = modified_text.replace(original, replacement)\n",
    "\n",
    "    return modified_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "record1 = responses[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_list_json = await get_synonym_list_answer(\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", record1['Qwen2.5-7B-Instruct-Turbo_reason'] , record1['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"selected_words\": [\"sensitive\", \"lost\"],\\n  \"replacements\": [\"emotional\", \"deceased\"]\\n}'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify_list_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why does Deirdre get so upset when Blake Past suggests she go to prom with the young man?\n",
      "\n",
      " (A) Because Blake is trying to guilt Deirdre into going with the young man by telling her that it'll ease her conscience. \n",
      " (B) Because Deirdre has fallen in love with Blake, despite his age, and wants him to take her to the prom.  \n",
      " (C) Because Blake is acting like he's her father, which is a sensitive topic for Deirdre because she lost her real parents. \n",
      " (D) Because the young man gave up his right arm in order to afford tickets to the prom, and this disgusts Deirdre. \n",
      "Original Reason: The text mentions that Blake Past suggests Deirdre go to prom with the young man, and Deirdre gets upset because she feels Blake Past is acting like her father, which is sensitive for her due to her lost parents.\n",
      "Initial synonym: The text mentions that Blake Past suggests Deirdre go to the ball with the young gentleman and Deirdre gets upset because she feels Blake Past is behaving like her father which is sensitive for her due to her deceased parents.\n",
      "Auto    synonym: The text mentions that Blake Past suggests Deirdre go to prom with the young man, and Deirdre gets upset because she feels Blake Past is acting like her father, which is emotional for her due to her deceased parents.\n"
     ]
    }
   ],
   "source": [
    "modified_answer1 = apply_reason_modifications(record1['Qwen2.5-7B-Instruct-Turbo_reason'], modify_list_json)\n",
    "print(record1['questions'])\n",
    "print('Original Reason:', record1['Qwen2.5-7B-Instruct-Turbo_reason'])\n",
    "print('Initial synonym:', record1['Qwen2.5-7B-Instruct-Turbo_reason_perturb2_meta'])\n",
    "print('Auto    synonym:',modified_answer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate for the whole responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "# List of model base names\n",
    "target_models = [\n",
    "    \"Qwen2.5-7B-Instruct-Turbo\",\n",
    "    \"Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    \"DeepSeek-V3\",\n",
    "    \"Llama-4-Scout-17B-16E-Instruct\",\n",
    "    \"Llama-4-Maverick-17B-128E-Instruct-FP8\"\n",
    "]\n",
    "\n",
    "# Semaphore to limit concurrency\n",
    "perturb_semaphore = asyncio.Semaphore(10)\n",
    "\n",
    "async def process_reason_perturbation(record, model_name):\n",
    "    async with perturb_semaphore:\n",
    "        question = record.get(\"questions\", \"\")\n",
    "        reason_key = f\"{model_name}_reason\"\n",
    "        perturb_key = f\"{model_name}_reason_perturb_llm_auto\"\n",
    "\n",
    "        if reason_key not in record or not record[reason_key] or not question:\n",
    "            return  # Skip if reason or question missing\n",
    "\n",
    "        try:\n",
    "            modify_list_json = await get_synonym_list_answer(\n",
    "                \"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "                record[reason_key],\n",
    "                question\n",
    "            )\n",
    "            modified_answer = apply_reason_modifications(record[reason_key], modify_list_json)\n",
    "            record[perturb_key] = modified_answer\n",
    "        except Exception as e:\n",
    "            print(f\"Failed on model {model_name}, pid {record.get('pid', 'unknown')}: {e}, modify_list_json: {modify_list_json}\")\n",
    "            record[perturb_key] = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def apply_perturbations_to_all_responses(responses):\n",
    "    tasks = []\n",
    "\n",
    "    for record in responses:\n",
    "        gt_label = record.get(\"output_label\")\n",
    "        if not gt_label:\n",
    "            continue  # Skip if ground truth is missing\n",
    "\n",
    "        # Get each model's predicted label\n",
    "        model_labels = {\n",
    "            model: record.get(f\"{model}_output_label\")\n",
    "            for model in target_models\n",
    "        }\n",
    "\n",
    "        # Only proceed if any model prediction is incorrect\n",
    "        if any(label != gt_label for label in model_labels.values() if label is not None):\n",
    "            for model in target_models:\n",
    "                perturb_key = f\"{model}_reason_perturb_llm_auto\"\n",
    "                if record.get(perturb_key) is None:  # Only if not already processed\n",
    "                    tasks.append(process_reason_perturbation(record, model))\n",
    "\n",
    "    for task in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"Perturbing reasons\"):\n",
    "        await task\n",
    "\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing reasons: 100%|██████████| 70/70 [00:19<00:00,  3.68it/s]\n"
     ]
    }
   ],
   "source": [
    "responses = await apply_perturbations_to_all_responses(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\quality_responses.json\", \"w\") as f:\n",
    "    json.dump(responses, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selecting with heurestics (selects common words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {\n",
    "    \"the\", \"his\", \"her\", \"an\", \"a\", \"this\", \"on\", \"is\", \"of\", \"and\", \"to\", \"in\", \"that\", \"it\", \n",
    "    \"with\", \"as\", \"for\", \"was\", \"were\", \"be\", \"by\", \"at\", \"or\", \"which\", \"from\", \"but\", \"not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tries = []\n",
    "\n",
    "def get_candidate_words(sentence, question, n_words=2):\n",
    "    global num_tries\n",
    "    candidate_words = []\n",
    "    tries = 0\n",
    "\n",
    "    sentence_words = sentence.split()\n",
    "    question_words = set(question.lower().split())\n",
    "\n",
    "    while len(candidate_words) < n_words and tries < 1000:\n",
    "        word = random.choice(sentence_words)\n",
    "        tries += 1\n",
    "\n",
    "        # Normalize to lowercase for comparison\n",
    "        clean_word = word.lower().strip(\".,!?\\\"'()\")\n",
    "\n",
    "        if clean_word not in stop_words and clean_word not in question_words and clean_word not in candidate_words:\n",
    "            candidate_words.append(clean_word)\n",
    "\n",
    "    num_tries.append(tries)\n",
    "    return candidate_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYNONYM_SUGGESTER_LIMITED_PROMPT_TEMPLATE = \"\"\"You are a helpful assistant that helps rewrites sentences. \n",
    "Given are two words in the answer that you need to suggest replacement with their synonyms. \n",
    "Make sure the suggested words do not change the meaning of the answer. \n",
    "\n",
    "### System Output Format:\n",
    "Respond in **JSON format** with:\n",
    "- `\"replacements\"`: The list of replacement words in the same order.\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "{answer}\n",
    "\n",
    "### Selected Words:\n",
    "{selected_words}\n",
    "\n",
    "### Expected Response Format:\n",
    "```\n",
    "{{\n",
    "  \"replacements\": \"[replacement1, replacement2]\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "async_client = AsyncTogether(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "# Global: concurrency limiter\n",
    "qa_semaphore = asyncio.Semaphore(10)\n",
    "\n",
    "async def get_synonym_from_list_answer(model_name, answer, question, selected_words):\n",
    "    async with qa_semaphore:\n",
    "        prompt = SYNONYM_SUGGESTER_LIMITED_PROMPT_TEMPLATE.format(\n",
    "             question=question, answer=answer,selected_words=selected_words\n",
    "        )\n",
    "        exact_model = format_model_name_together(model_name)\n",
    "\n",
    "        try:\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=exact_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=0.0\n",
    "            )\n",
    "\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed QA comparison call for model {model_name}: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_replace = get_candidate_words(record3['Qwen2.5-7B-Instruct-Turbo_reason'], record3['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"replacements\": [\"cooking\", \"indicated\"]\\n}'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify_list_json = await get_synonym_from_list_answer(\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", record3['Qwen2.5-7B-Instruct-Turbo_reason'] , record3['questions'], words_to_replace)\n",
    "modify_list_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_reason_modifications_given_list(reason_text, words_to_replace, modify_list_json):\n",
    "    modified_text = reason_text\n",
    "    modify_list_json = json.loads(modify_list_json)\n",
    "    selected_words = words_to_replace\n",
    "    replacements = modify_list_json.get(\"replacements\", [])\n",
    "\n",
    "    for original, replacement in zip(selected_words, replacements):\n",
    "        if original and replacement:\n",
    "            modified_text = modified_text.replace(original, replacement)\n",
    "\n",
    "    return modified_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sabrina York is \n",
      "\n",
      " (A) a criminal that Blake is hunting\n",
      " (B) a psycheye that taught Blake all the tricks\n",
      " (C) an old friend of Blake's\n",
      " (D) Eldoria's alter ego\n",
      "Original    Reason: The text mentions that Sabrina York's nameplate on the kitchen range read 'Sabrina York', indicating she is Eldoria's alter ego.\n",
      "Initial    synonym: The text mentions that Sabra New York nameplate on the kitchen range read New York indicating she is Eldoria alter ego\n",
      "Heurestics synonym: The text mentions that Sabrina York's nameplate on the cooking range indicated 'Sabrina York', indicating she is Eldoria's alter ego.\n"
     ]
    }
   ],
   "source": [
    "modified_answer3 = apply_reason_modifications_given_list(record3['Qwen2.5-7B-Instruct-Turbo_reason'], words_to_replace, modify_list_json)\n",
    "print(record3['questions'])\n",
    "print('Original    Reason:', record3['Qwen2.5-7B-Instruct-Turbo_reason'])\n",
    "print('Initial    synonym:', record3['Qwen2.5-7B-Instruct-Turbo_reason_perturb2_meta'])\n",
    "print('Heurestics synonym:',modified_answer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Report 50 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_50_valid_records(responses):\n",
    "    selected = []\n",
    "    count = 0\n",
    "\n",
    "    for record in responses:\n",
    "        if 'Qwen2.5-7B-Instruct-Turbo_reason_perturb2_meta' not in record:\n",
    "            continue\n",
    "\n",
    "        selected.append({\n",
    "            'pid': record.get('pid'),\n",
    "            'questions': record.get('questions'),\n",
    "            'Qwen2.5-7B-Instruct-Turbo_reason': record.get('Qwen2.5-7B-Instruct-Turbo_reason'),\n",
    "            'Qwen2.5-7B-Instruct-Turbo_reason_perturb2_meta': record.get('Qwen2.5-7B-Instruct-Turbo_reason_perturb2_meta')\n",
    "        })\n",
    "\n",
    "        count += 1\n",
    "        if count == 50:\n",
    "            break\n",
    "\n",
    "    return selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_50 = get_first_50_valid_records(responses)\n",
    "first_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def add_llm_perturbations(first_50, model_name=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\"):\n",
    "    for record in tqdm(first_50, desc=\"Generating LLM perturbations\"):\n",
    "        original_reason = record['Qwen2.5-7B-Instruct-Turbo_reason']\n",
    "        question = record['questions']\n",
    "\n",
    "        # === 1. LLM-AUTO ===\n",
    "        try:\n",
    "            modify_list_json = await get_synonym_list_answer(model_name, original_reason, question)\n",
    "            modified_answer1 = apply_reason_modifications(original_reason, modify_list_json)\n",
    "            record['Qwen2.5-7B-Instruct-Turbo_reason_perturb_llm_auto'] = modified_answer1\n",
    "        except Exception as e:\n",
    "            print(f\"[AUTO] Failed for pid {record['pid']}: {e}\")\n",
    "            record['Qwen2.5-7B-Instruct-Turbo_reason_perturb_llm_auto'] = None\n",
    "\n",
    "        # === 2. LLM-HEURISTICS ===\n",
    "        try:\n",
    "            words_to_replace = get_candidate_words(original_reason, question, n_words=3)\n",
    "            replacements = await get_synonym_from_list_answer(model_name, original_reason, question, words_to_replace)\n",
    "            modified_answer2 = apply_reason_modifications_given_list(original_reason, words_to_replace, replacements)\n",
    "            record['Qwen2.5-7B-Instruct-Turbo_reason_perturb_llm_heurestics'] = modified_answer2\n",
    "        except Exception as e:\n",
    "            print(f\"[HEUR] Failed for pid {record['pid']}: {e}\")\n",
    "            record['Qwen2.5-7B-Instruct-Turbo_reason_perturb_llm_heurestics'] = None\n",
    "\n",
    "    return first_50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM perturbations:  16%|█▌        | 8/50 [00:34<01:54,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEUR] Failed for pid 30029_F5N22U40_7_0: Expecting value: line 1 column 1 (char 0)\n",
      "[AUTO] Failed for pid 30029_F5N22U40_8_0: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM perturbations:  20%|██        | 10/50 [00:41<02:12,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEUR] Failed for pid 62139_J05FWZR6_3_0: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM perturbations:  26%|██▌       | 13/50 [00:50<02:08,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEUR] Failed for pid 62139_J05FWZR6_7_0: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM perturbations:  34%|███▍      | 17/50 [00:58<01:23,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEUR] Failed for pid 63523_STSHLFEA_3_0: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM perturbations:  42%|████▏     | 21/50 [01:12<02:00,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEUR] Failed for pid 63523_STSHLFEA_9_0: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM perturbations:  44%|████▍     | 22/50 [01:13<01:29,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEUR] Failed for pid 63523_STSHLFEA_10_0: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM perturbations:  50%|█████     | 25/50 [01:21<01:01,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AUTO] Failed for pid 63401_ZCP5ZDGL_6_0: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM perturbations:  56%|█████▌    | 28/50 [01:24<00:36,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEUR] Failed for pid 63401_ZCP5ZDGL_8_0: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM perturbations:  60%|██████    | 30/50 [01:27<00:30,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEUR] Failed for pid 63401_ZCP5ZDGL_10_0: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM perturbations:  64%|██████▍   | 32/50 [01:39<01:09,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AUTO] Failed for pid 62476_Z8GFDCIZ_3_0: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM perturbations:  66%|██████▌   | 33/50 [01:40<00:55,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEUR] Failed for pid 62476_Z8GFDCIZ_3_0: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM perturbations:  70%|███████   | 35/50 [01:43<00:36,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEUR] Failed for pid 62476_Z8GFDCIZ_5_0: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM perturbations:  80%|████████  | 40/50 [01:58<00:31,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEUR] Failed for pid 52845_91NAQ9LY_1_0: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM perturbations:  86%|████████▌ | 43/50 [02:03<00:14,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEUR] Failed for pid 52845_91NAQ9LY_5_0: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM perturbations:  90%|█████████ | 45/50 [02:06<00:08,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEUR] Failed for pid 52845_91NAQ9LY_7_0: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM perturbations:  96%|█████████▌| 48/50 [02:10<00:02,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEUR] Failed for pid 30029_XQTTOPHP_3_0: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM perturbations:  98%|█████████▊| 49/50 [02:12<00:01,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEUR] Failed for pid 30029_XQTTOPHP_4_0: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM perturbations: 100%|██████████| 50/50 [02:14<00:00,  2.69s/it]\n"
     ]
    }
   ],
   "source": [
    "first_50_augmented = await add_llm_perturbations(first_50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_50_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_complete_perturbations(records):\n",
    "    required_keys = [\n",
    "        'Qwen2.5-7B-Instruct-Turbo_reason_perturb_llm_auto',\n",
    "        'Qwen2.5-7B-Instruct-Turbo_reason_perturb_llm_heurestics'\n",
    "    ]\n",
    "\n",
    "    cleaned = [\n",
    "        record for record in records\n",
    "        if all(key in record and record[key] is not None for key in required_keys)\n",
    "    ]\n",
    "\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 33 fully-processed records.\n"
     ]
    }
   ],
   "source": [
    "cleaned_records = filter_complete_perturbations(first_50_augmented)\n",
    "print(f\"Retained {len(cleaned_records)} fully-processed records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load model once\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def add_similarity_scores(records):\n",
    "    for record in records:\n",
    "        try:\n",
    "            base_reason = record[\"Qwen2.5-7B-Instruct-Turbo_reason\"]\n",
    "            initial_perturb = record[\"Qwen2.5-7B-Instruct-Turbo_reason_perturb2_meta\"]\n",
    "            auto_perturb = record[\"Qwen2.5-7B-Instruct-Turbo_reason_perturb_llm_auto\"]\n",
    "            heur_perturb = record[\"Qwen2.5-7B-Instruct-Turbo_reason_perturb_llm_heurestics\"]\n",
    "\n",
    "            embeddings = model.encode([base_reason, auto_perturb, heur_perturb, initial_perturb])\n",
    "\n",
    "            # Similarity between base and auto perturbation\n",
    "            sim_auto = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "            # Similarity between base and heuristics perturbation\n",
    "            sim_heur = util.cos_sim(embeddings[0], embeddings[2]).item()\n",
    "            # similarity between base and initial perturbation\n",
    "            sim_initial = util.cos_sim(embeddings[0], embeddings[3]).item()\n",
    "            \n",
    "            record[\"Qwen2.5-7B-Instruct-Turbo_reason_perturb_llm_auto_similarity\"] = sim_auto\n",
    "            record[\"Qwen2.5-7B-Instruct-Turbo_reason_perturb_llm_heurestics_similarity\"] = sim_heur\n",
    "            record[\"Qwen2.5-7B-Instruct-Turbo_reason_perturb_llm_initial_similarity\"] = sim_initial\n",
    "        except Exception as e:\n",
    "            print(f\"Similarity computation failed for pid {record.get('pid')}: {e}\")\n",
    "\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_records = add_similarity_scores(cleaned_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def convert_to_df(scored_records):\n",
    "    rows = []\n",
    "\n",
    "    for record in scored_records:\n",
    "        rows.append({\n",
    "            \"pid\": record.get(\"pid\"),\n",
    "            \"question\": record.get(\"questions\"),\n",
    "            \"original\": record.get(\"Qwen2.5-7B-Instruct-Turbo_reason\"),\n",
    "            \"auto_perturb\": record.get(\"Qwen2.5-7B-Instruct-Turbo_reason_perturb_llm_auto\"),\n",
    "            \"auto_pertub_similarity\": record.get(\"Qwen2.5-7B-Instruct-Turbo_reason_perturb_llm_auto_similarity\"),\n",
    "            \"heurestics\": record.get(\"Qwen2.5-7B-Instruct-Turbo_reason_perturb_llm_heurestics\"),\n",
    "            \"heurestics_similarity\": record.get(\"Qwen2.5-7B-Instruct-Turbo_reason_perturb_llm_heurestics_similarity\"),\n",
    "            \"initial\": record.get(\"Qwen2.5-7B-Instruct-Turbo_reason_perturb2_meta\"),\n",
    "            \"initial_similarity\": record.get(\"Qwen2.5-7B-Instruct-Turbo_reason_perturb_llm_initial_similarity\")\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\n",
    "        'pid', 'question', 'original', 'auto_perturb', \n",
    "        'auto_pertub_similarity', 'heurestics', 'heurestics_similarity', 'initial', 'initial_similarity'\n",
    "    ])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved as 'perturbation_similarity_results.csv'\n"
     ]
    }
   ],
   "source": [
    "df = convert_to_df(scored_records)\n",
    "df.to_csv(\".\\quality\\perturbation_similarity_results.csv\", index=False)\n",
    "print(\"CSV saved as 'perturbation_similarity_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tries to get candidate words: [2, 7, 8, 8, 4, 6, 16, 9, 7, 10, 18, 9, 10, 13, 7, 7, 11, 22, 16, 12, 13, 16, 3, 9, 6, 5, 12, 13, 7, 11, 72, 8, 3, 12, 8, 8, 25, 6, 11, 6, 10, 12, 6, 10, 4, 5, 10, 16, 7, 8, 4, 7]\n",
      "Average number of tries: 10.673076923076923\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tries to get candidate words:\", num_tries)\n",
    "print(\"Average number of tries:\", sum(num_tries) / len(num_tries) if num_tries else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Paraphrasing in Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\quality_responses.json\", 'r') as file:\n",
    "    responses = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "# Models involved\n",
    "target_models = [\n",
    "    \"Qwen2.5-7B-Instruct-Turbo\",\n",
    "    \"Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    \"DeepSeek-V3\",\n",
    "    \"Llama-4-Scout-17B-16E-Instruct\",\n",
    "    \"Llama-4-Maverick-17B-128E-Instruct-FP8\"\n",
    "]\n",
    "\n",
    "# Optional: limit parallel API calls\n",
    "paraphrase_semaphore = asyncio.Semaphore(10)\n",
    "\n",
    "# Async version of paraphrasing function\n",
    "async def paraphrase_reasoning_async(reasoning, model_name):\n",
    "    async with paraphrase_semaphore:\n",
    "        exact_model = format_model_name_together(model_name)\n",
    "\n",
    "        try:\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=exact_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that paraphrases a sentence while preserving context and meaning. You paraphrase the sentence(s) given, and only reply with the paraphrased sentence and no other text.\"},\n",
    "                    {\"role\": \"user\", \"content\": f'This is a sentence which explains the reasoning behind an answer to a question. Your response is strictly the new paraphrased reasoning. The sentence is: {reasoning}'}\n",
    "                ]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Paraphrasing failed with {model_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "def create_paraphrase_task(record, model_name, paraphrasing_model, reason):\n",
    "    async def run():\n",
    "        paraphrased_reason = await paraphrase_reasoning_async(reason, paraphrasing_model)\n",
    "        key = f\"{model_name}_reason_paraphrased_{paraphrasing_model}\"\n",
    "        record[key] = paraphrased_reason\n",
    "    return run()\n",
    "\n",
    "\n",
    "# One record → all its paraphrase tasks\n",
    "async def process_record_paraphrases(record):\n",
    "    tasks = []\n",
    "    gt_label = record.get(\"output_label\")\n",
    "\n",
    "    if not gt_label:\n",
    "        return\n",
    "\n",
    "    model_labels = {\n",
    "        model: record.get(f\"{model}_output_label\")\n",
    "        for model in target_models\n",
    "    }\n",
    "\n",
    "    if not any(label and label != gt_label for label in model_labels.values()):\n",
    "        return\n",
    "\n",
    "    for model_name in target_models:\n",
    "        reason_key = f\"{model_name}_reason\"\n",
    "        reason = record.get(reason_key)\n",
    "        if not reason:\n",
    "            continue\n",
    "\n",
    "        other_models = [m for m in target_models if m != model_name]\n",
    "\n",
    "        for paraphrasing_model in other_models:\n",
    "            paraphrase_key = f\"{model_name}_reason_paraphrased_{paraphrasing_model}\"\n",
    "            if record.get(paraphrase_key) is not None:\n",
    "                continue\n",
    "\n",
    "            # ✅ Create task with proper variable binding\n",
    "            tasks.append(create_paraphrase_task(record, model_name, paraphrasing_model, reason))\n",
    "\n",
    "    await asyncio.gather(*tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main async runner\n",
    "async def run_all_paraphrasing(responses):\n",
    "    await tqdm_asyncio.gather(*[process_record_paraphrases(r) for r in responses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await run_all_paraphrasing(responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\quality_responses.json\", \"w\") as f:\n",
    "    json.dump(responses, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate 'Dupe' Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_SELECTION_DUPE_PROMPT_TEMPLATE = \"\"\"You are an AI system designed to analyze questions and their corresponding text passages to determine the reason behind an answer.\n",
    "\n",
    "### System Output Format:\n",
    "Respond in **JSON format** with:\n",
    "- `\"reason\"`: A short explanation of why this answer is correct.\n",
    "\n",
    "### Task:\n",
    "Given the **question**, **text** and the **correct_answer** below, generate a brief justification reason for the given answer.\n",
    "\n",
    "#### Question:\n",
    "{questions}\n",
    "\n",
    "#### Text:\n",
    "{text}\n",
    "\n",
    "#### Correct Answer:\n",
    "{correct_answer}\n",
    "\n",
    "### Expected Response Format:\n",
    "```\n",
    "{{\n",
    "  \"reason\": \"The text states that the event lasted 10 hours, which corresponds to option B.\"\n",
    "}}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_braces_content(s):\n",
    "    match = re.search(r'\\{(.*?)\\}', s, re.DOTALL)\n",
    "    return match.group(0) if match else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Selective Dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\quality_responses.json', 'r') as file:\n",
    "    responses = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "async_client = AsyncTogether(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "# Limit number of concurrent API calls\n",
    "semaphore = asyncio.Semaphore(20)  # <-- Set to 10, 20, or 50 depending on your system and Together API limits\n",
    "\n",
    "failed_records_id = []\n",
    "failed_records = []\n",
    "\n",
    "async def process_record_dupe(record, model_name, alternative_models):\n",
    "    async with semaphore:  # Throttle concurrent calls\n",
    "        questions = record.get(\"questions\", \"\")\n",
    "        text = record.get(\"text\", \"\")\n",
    "        current_label = record.get(model_name + \"_output_label\")\n",
    "\n",
    "        if not questions or not text or not current_label:\n",
    "            return None  # Skip invalid\n",
    "\n",
    "        # Step 1: Find the first alternate label that differs from GT\n",
    "        alternate_output_label = None\n",
    "        alternate_found = False\n",
    "        for alt_model in alternative_models:\n",
    "            alt_label = record.get(f\"{alt_model}_output_label\")\n",
    "            if alt_label and alt_label != current_label and alternate_found is False:\n",
    "                alternate_output_label = alt_label\n",
    "                \n",
    "        # print(f\"Record ID: {record.get('pid')}, Actual:{current_label} Alternate Output Label: {alternate_output_label}\")\n",
    "        \n",
    "        if not alternate_output_label:\n",
    "            return None  # All models agree with GT → skip\n",
    "\n",
    "        # Step 2: Format prompt\n",
    "        prompt = ANSWER_SELECTION_DUPE_PROMPT_TEMPLATE.format(\n",
    "            questions=questions,\n",
    "            text=text,\n",
    "            correct_answer=alternate_output_label\n",
    "        )\n",
    "        exact_model = format_model_name_together(model_name)\n",
    "\n",
    "        # Step 3: Call the model\n",
    "        try:\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=exact_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            api_response = response.choices[0].message.content\n",
    "            api_response = extract_braces_content(api_response)\n",
    "\n",
    "            key_name_output = model_name + \"_Dupe_output_label\"\n",
    "            key_name_reason = model_name + \"_Dupe_reason\"\n",
    "\n",
    "            response_json = fix_json_response(api_response)\n",
    "            record[key_name_output] = alternate_output_label\n",
    "            record[key_name_reason] = response_json.get(\"reason\")\n",
    "\n",
    "            return record\n",
    "\n",
    "        except Exception as e:\n",
    "            failed_records.append(str(e))\n",
    "            failed_records_id.append(record.get(\"pid\"))\n",
    "            return None\n",
    "\n",
    "\n",
    "async def generate_dupe_answer_selection_quality_async(model_name, alternative_models, start_index, end_index, processed_data, repeat_failures=False):\n",
    "    tasks = []\n",
    "    for record in processed_data[start_index:end_index]:\n",
    "        # If repeat_failures is True, only process the record if its 'pid' is in failed_records_id.\n",
    "        if repeat_failures and record.get(\"pid\") not in failed_records_id:\n",
    "            continue\n",
    "        tasks.append(process_record_dupe(record, model_name, alternative_models))\n",
    "    \n",
    "    results = []\n",
    "    for future in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"Processing Records\"):\n",
    "        result = await future\n",
    "        if result:\n",
    "            results.append(result)\n",
    "            \n",
    "    return results, failed_records_id, failed_records\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Records: 100%|██████████| 2086/2086 [03:16<00:00, 10.60it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Llama-4-Scout-17B-16E-Instruct\"\n",
    "alterntive_models = [\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\"]\n",
    "\n",
    "results, failed_ids, failed_content = asyncio.run(generate_dupe_answer_selection_quality_async(model_name, alterntive_models,0, len(responses), responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total _Dupe_output_label fields found: 1591\n"
     ]
    }
   ],
   "source": [
    "dupe_label_count = 0\n",
    "\n",
    "for record in responses:\n",
    "    for key in record.keys():\n",
    "        if key.endswith(\"_Dupe_output_label\") and record[key] is not None:\n",
    "            dupe_label_count += 1\n",
    "\n",
    "print(f\"Total _Dupe_output_label fields found: {dupe_label_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Records: 100%|██████████| 2086/2086 [12:31<00:00,  2.78it/s] \n"
     ]
    }
   ],
   "source": [
    "model_name = \"Llama-4-Maverick-17B-128E-Instruct-FP8\"\n",
    "alterntive_models = [\"DeepSeek-V3\",  \"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\"]\n",
    "\n",
    "results, failed_ids, failed_content = asyncio.run(generate_dupe_answer_selection_quality_async(model_name, alterntive_models,0, len(responses), responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Records: 100%|██████████| 2086/2086 [08:40<00:00,  4.01it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"DeepSeek-V3\"\n",
    "alterntive_models = [\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\"]\n",
    "\n",
    "results, failed_ids, failed_content = asyncio.run(generate_dupe_answer_selection_quality_async(model_name, alterntive_models,0, len(responses), responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Records: 100%|██████████| 2086/2086 [02:05<00:00, 16.62it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen2.5-7B-Instruct-Turbo\"\n",
    "alterntive_models = [\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\"]\n",
    "\n",
    "results, failed_ids, failed_content = asyncio.run(generate_dupe_answer_selection_quality_async(model_name, alterntive_models,0, len(responses), responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Records: 100%|██████████| 2086/2086 [05:06<00:00,  6.81it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "alterntive_models = [\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\"]\n",
    "\n",
    "results, failed_ids, failed_content = asyncio.run(generate_dupe_answer_selection_quality_async(model_name, alterntive_models,0, len(responses), responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\quality_responses.json\", \"w\") as f:\n",
    "    json.dump(responses, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix Format to include output_label in the Dupe_reason\n",
    "# Define model list\n",
    "models = [\n",
    "    \"DeepSeek-V3\", \n",
    "    \"Llama-4-Maverick-17B-128E-Instruct-FP8\", \n",
    "    \"Llama-4-Scout-17B-16E-Instruct\", \n",
    "    \"Qwen2.5-7B-Instruct-Turbo\", \n",
    "    \"Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "]\n",
    "\n",
    "# Process the responses\n",
    "for record in responses:\n",
    "    for model in models:\n",
    "        label_key = f\"{model}_Dupe_output_label\"\n",
    "        reason_key = f\"{model}_Dupe_reason\"\n",
    "        if label_key in record and reason_key in record:\n",
    "            output_label = record[label_key]\n",
    "            if output_label:\n",
    "                new_key = f\"{reason_key}_output_label_{output_label}\"\n",
    "                record[new_key] = record[reason_key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dupes for all combinations (where one is wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "async_client = AsyncTogether(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "# Limit number of concurrent API calls\n",
    "semaphore = asyncio.Semaphore(20)  # <-- Set to 10, 20, or 50 depending on your system and Together API limits\n",
    "\n",
    "failed_records_id = []\n",
    "failed_comparisons = failed_records = []\n",
    "\n",
    "async def process_record_dupe(record, model_name, model_2):\n",
    "    async with semaphore:  # Throttle concurrent calls\n",
    "        questions = record.get(\"questions\", \"\")\n",
    "        text = record.get(\"text\", \"\")\n",
    "        current_label = record.get(model_name + \"_output_label\")\n",
    "\n",
    "        if not questions or not text or not current_label:\n",
    "            return None  # Skip invalid\n",
    "\n",
    "        # Step 1: Find the first alternate label that differs from GT\n",
    "        required_label = record.get(model_2 + \"_output_label\")\n",
    "        if not required_label:\n",
    "            return None  # All models agree with GT → skip\n",
    "\n",
    "        # Step 2: Format prompt\n",
    "        prompt = ANSWER_SELECTION_DUPE_PROMPT_TEMPLATE.format(\n",
    "            questions=questions,\n",
    "            text=text,\n",
    "            correct_answer=required_label\n",
    "        )\n",
    "        exact_model = format_model_name_together(model_name)\n",
    "\n",
    "        # Step 3: Call the model\n",
    "        try:\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=exact_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            api_response = response.choices[0].message.content\n",
    "            api_response = extract_braces_content(api_response)\n",
    "\n",
    "            key_name_reason = model_name + \"_Dupe_reason_output_label_\" + required_label\n",
    "            response_json = fix_json_response(api_response)\n",
    "            record[key_name_reason] = response_json.get(\"reason\")\n",
    "\n",
    "            return record\n",
    "\n",
    "        except Exception as e:\n",
    "            failed_records.append(str(e))\n",
    "            failed_records_id.append(record.get(\"pid\"))\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_dupe_all_combinations(evaluator_model, evaluatee_model, records, harmful_subset, repeat_failures=False):\n",
    "    model1 = evaluator_model\n",
    "    model2 = evaluatee_model\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    for record in records:\n",
    "        pid = record.get('pid')\n",
    "        gt_label = record['output_label']\n",
    "        model1_label = record.get(model1 + '_output_label')\n",
    "        model2_label = record.get(model2 + '_output_label')\n",
    "        if repeat_failures:\n",
    "            if pid not in failed_records_id:\n",
    "                continue  # Only retry known failed records\n",
    "        if model1_label is None or model2_label is None:\n",
    "            continue\n",
    "        key_name_reason = model1 + \"_Dupe_reason_output_label_\" + model2_label\n",
    "        if key_name_reason in record:\n",
    "            continue\n",
    "        # Only compare if model1 is wrong and model2 is right\n",
    "        if harmful_subset:\n",
    "            if model1_label and model2_label and model1_label != gt_label and model2_label == gt_label:\n",
    "                tasks.append(process_record_dupe(record, model1, model2))\n",
    "        else:\n",
    "            if model1_label and model2_label and model1_label == gt_label and model2_label != gt_label:\n",
    "                tasks.append(process_record_dupe(record, model1, model2))\n",
    "    for future in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"Evaluating Preferences\"):\n",
    "        await future\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 0it [00:00, ?it/s]\n",
      "Evaluating Preferences: 100%|██████████| 45/45 [00:06<00:00,  7.34it/s]\n",
      "Evaluating Preferences: 100%|██████████| 26/26 [00:03<00:00,  8.33it/s]\n",
      "Evaluating Preferences: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "await generate_dupe_all_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "######\n",
    "await generate_dupe_all_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 154/154 [00:11<00:00, 13.07it/s]\n",
      "Evaluating Preferences: 100%|██████████| 31/31 [00:02<00:00, 10.82it/s]\n",
      "Evaluating Preferences: 0it [00:00, ?it/s]\n",
      "Evaluating Preferences: 100%|██████████| 23/23 [00:03<00:00,  6.96it/s]\n"
     ]
    }
   ],
   "source": [
    "await generate_dupe_all_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 74/74 [00:26<00:00,  2.79it/s]\n",
      "Evaluating Preferences: 100%|██████████| 28/28 [00:10<00:00,  2.67it/s]\n",
      "Evaluating Preferences: 100%|██████████| 32/32 [00:13<00:00,  2.39it/s]\n",
      "Evaluating Preferences: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "await generate_dupe_all_combinations(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 177/177 [00:15<00:00, 11.40it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "Evaluating Preferences: 100%|██████████| 39/39 [00:03<00:00, 11.68it/s]\n",
      "Evaluating Preferences: 100%|██████████| 91/91 [00:08<00:00, 11.06it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "await generate_dupe_all_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 176/176 [01:59<00:00,  1.47it/s]\n",
      "Evaluating Preferences: 100%|██████████| 43/43 [00:19<00:00,  2.19it/s]\n",
      "Evaluating Preferences: 100%|██████████| 92/92 [00:33<00:00,  2.76it/s]\n",
      "Evaluating Preferences: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "await generate_dupe_all_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beneificail quadrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 51/51 [00:05<00:00,  9.10it/s]\n",
      "Evaluating Preferences: 100%|██████████| 56/56 [00:06<00:00,  8.24it/s]\n",
      "Evaluating Preferences: 100%|██████████| 95/95 [00:11<00:00,  8.08it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "await generate_dupe_all_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "######\n",
    "await generate_dupe_all_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 64/64 [00:07<00:00,  9.06it/s]\n",
      "Evaluating Preferences: 100%|██████████| 139/139 [00:14<00:00,  9.75it/s]\n",
      "Evaluating Preferences: 0it [00:00, ?it/s]\n",
      "Evaluating Preferences: 100%|██████████| 71/71 [00:07<00:00,  8.89it/s]\n"
     ]
    }
   ],
   "source": [
    "await generate_dupe_all_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 148/148 [01:06<00:00,  2.24it/s]\n",
      "Evaluating Preferences: 100%|██████████| 115/115 [00:53<00:00,  2.15it/s]\n",
      "Evaluating Preferences: 100%|██████████| 112/112 [00:54<00:00,  2.06it/s]\n",
      "Evaluating Preferences: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "await generate_dupe_all_combinations(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 77/77 [00:06<00:00, 11.98it/s]\n",
      "Evaluating Preferences: 100%|██████████| 49/49 [00:04<00:00, 10.94it/s]\n",
      "Evaluating Preferences: 100%|██████████| 28/28 [00:02<00:00,  9.84it/s]\n",
      "Evaluating Preferences: 100%|██████████| 2/2 [00:01<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "await generate_dupe_all_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Preferences: 100%|██████████| 3/3 [00:01<00:00,  2.65it/s]\n",
      "Evaluating Preferences: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Evaluating Preferences: 100%|██████████| 3/3 [00:01<00:00,  2.13it/s]\n",
      "Evaluating Preferences: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "await generate_dupe_all_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_dupe_all_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_dupe_all_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\quality_responses.json\", \"w\") as f:\n",
    "    json.dump(responses, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Paraphrasing External with Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAPHRASER_PROMPT_TEMPLATE = \"\"\"You are a helpful assistant that helps rewrites sentences. \n",
    "Paraphrase the answer to the question below, writing in a manner you would express the answer but without changing the semantics or meaning of the answer. \n",
    "You are not allowed to replace words that appear in the list of fixed words, but you can change their ordering. These words help maintain the meaning of the answer.\n",
    "\n",
    "### System Output Format:\n",
    "Respond in **JSON format** with:\n",
    "- `\"paraphrased_answer\"`: The answer to the question, paraphrased.\n",
    "\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "{answer}\n",
    "\n",
    "### Fixed Words:\n",
    "{fixed_words}\n",
    "\n",
    "### Expected Response Format:\n",
    "```\n",
    "{{\n",
    "  \"paraphrased_answer\": \"another paraphrased version of the answer\",\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_braces_content(s):\n",
    "    match = re.search(r'\\{(.*?)\\}', s, re.DOTALL)\n",
    "    return match.group(0) if match else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_non_stop_words(question, response):\n",
    "    stop_words = {\n",
    "        \"the\", \"his\", \"her\", \"an\", \"a\", \"this\", \"on\", \"is\", \"of\", \"and\", \"to\", \"in\", \"that\", \"it\", \n",
    "        \"with\", \"as\", \"for\", \"was\", \"were\", \"be\", \"by\", \"at\", \"or\", \"which\", \"from\", \"but\", \"not\"\n",
    "    }\n",
    "\n",
    "    # Normalize and tokenize input\n",
    "    question_words = set(word.lower() for word in question.split())\n",
    "    response_words = [word.lower() for word in response.split()]\n",
    "\n",
    "    # Return words that are not stop words and exist in question\n",
    "    return [word for word in response_words if word not in stop_words and word in question_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "async_client = AsyncTogether(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "# Limit number of concurrent API calls\n",
    "semaphore = asyncio.Semaphore(20)  # <-- Set to 10, 20, or 50 depending on your system and Together API limits\n",
    "\n",
    "failed_comparisons = []\n",
    "\n",
    "async def process_record_paraphrase(record, model_name, model_2, paraphraser='meta-llama/Llama-3.3-70B-Instruct-Turbo'):\n",
    "    async with semaphore:  # Throttle concurrent calls\n",
    "        questions = record.get(\"questions\", \"\")\n",
    "        text = record.get(\"text\", \"\")\n",
    "        current_label = record.get(model_name + \"_output_label\")\n",
    "        response = record.get(model_name+\"_reason\")\n",
    "        fixed_words = overlap_non_stop_words(questions, response)\n",
    "        \n",
    "        if not questions or not text or not current_label:\n",
    "            return None  # Skip invalid\n",
    "\n",
    "\n",
    "        prompt = PARAPHRASER_PROMPT_TEMPLATE.format(\n",
    "            question=questions,\n",
    "            answer=response,\n",
    "            fixed_words=fixed_words\n",
    "        )\n",
    "        # use the paraphraser model to generate the paraphrased answer\n",
    "        exact_model = format_model_name_together(paraphraser)\n",
    "\n",
    "        # Step 3: Call the model\n",
    "        try:\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=exact_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            api_response = response.choices[0].message.content\n",
    "            # api_response = extract_braces_content(api_response)\n",
    "\n",
    "            key_name_reason = model_name + \"_reason_paraphrased_external\" \n",
    "            #response_json = fix_json_response(api_response)\n",
    "            response_json = json.loads(api_response)\n",
    "            record[key_name_reason] = response_json.get(\"paraphrased_answer\")\n",
    "\n",
    "            return record\n",
    "\n",
    "        except Exception as e:\n",
    "            # print(f\"Failed QA comparison call for model {model_name}: {e}\")\n",
    "            failed_comparisons.append(record.get(\"pid\"))\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_paraphrase_external_combinations(evaluator_model, evaluatee_model, records, harmful_subset, repeat_failures=False):\n",
    "    model1 = evaluator_model\n",
    "    model2 = evaluatee_model\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    for record in records:\n",
    "        pid = record.get('pid')\n",
    "        gt_label = record['output_label']\n",
    "        model1_label = record.get(model1 + '_output_label')\n",
    "        model2_label = record.get(model2 + '_output_label')\n",
    "        if repeat_failures:\n",
    "            if pid not in failed_comparisons:\n",
    "                continue  # Only retry known failed records\n",
    "        if model1_label is None or model2_label is None:\n",
    "            continue\n",
    "        key_name_reason = model1 + \"_reason_paraphrased_external\" \n",
    "        if key_name_reason in record:\n",
    "            continue\n",
    "        # Only compare if model1 is wrong and model2 is right\n",
    "        if harmful_subset:\n",
    "            if model1_label and model2_label and model1_label != gt_label and model2_label == gt_label:\n",
    "                tasks.append(process_record_paraphrase(record, model1, model2))\n",
    "        else:\n",
    "            if model1_label and model2_label and model1_label == gt_label and model2_label != gt_label:\n",
    "                tasks.append(process_record_paraphrase(record, model1, model2))\n",
    "    for future in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"Generating...\"):\n",
    "        await future\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating...: 100%|██████████| 461/461 [08:28<00:00,  1.10s/it]\n",
      "Generating...: 100%|██████████| 317/317 [06:33<00:00,  1.24s/it]\n",
      "Generating...: 100%|██████████| 202/202 [04:55<00:00,  1.46s/it]\n",
      "Generating...: 100%|██████████| 140/140 [03:57<00:00,  1.70s/it]\n",
      "Generating...: 100%|██████████| 140/140 [02:45<00:00,  1.18s/it]\n",
      "Generating...: 100%|██████████| 79/79 [02:35<00:00,  1.97s/it]\n",
      "Generating...: 100%|██████████| 113/113 [03:47<00:00,  2.01s/it]\n",
      "Generating...: 100%|██████████| 61/61 [02:30<00:00,  2.47s/it]\n"
     ]
    }
   ],
   "source": [
    "await generate_paraphrase_external_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######  \n",
    "await generate_paraphrase_external_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await generate_paraphrase_external_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating...: 100%|██████████| 167/167 [04:06<00:00,  1.47s/it]\n",
      "Generating...: 100%|██████████| 67/67 [02:43<00:00,  2.44s/it]\n",
      "Generating...: 100%|██████████| 83/83 [03:56<00:00,  2.85s/it]\n",
      "Generating...: 100%|██████████| 50/50 [01:23<00:00,  1.68s/it]\n",
      "Generating...: 100%|██████████| 93/93 [02:26<00:00,  1.58s/it]\n",
      "Generating...: 100%|██████████| 51/51 [01:21<00:00,  1.60s/it]\n",
      "Generating...: 100%|██████████| 82/82 [01:22<00:00,  1.00s/it]\n",
      "Generating...: 100%|██████████| 44/44 [00:51<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "await generate_paraphrase_external_combinations(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating...: 100%|██████████| 509/509 [05:24<00:00,  1.57it/s] \n",
      "Generating...: 100%|██████████| 286/286 [05:34<00:00,  1.17s/it]\n",
      "Generating...: 100%|██████████| 173/173 [12:59<00:00,  4.50s/it] \n",
      "Generating...: 100%|██████████| 103/103 [04:11<00:00,  2.44s/it]\n",
      "Generating...: 100%|██████████| 273/273 [07:13<00:00,  1.59s/it]\n",
      "Generating...: 100%|██████████| 160/160 [02:34<00:00,  1.04it/s]\n",
      "Generating...: 100%|██████████| 111/111 [03:22<00:00,  1.82s/it]\n",
      "Generating...: 100%|██████████| 72/72 [03:02<00:00,  2.53s/it]\n"
     ]
    }
   ],
   "source": [
    "await generate_paraphrase_external_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating...: 100%|██████████| 575/575 [17:10<00:00,  1.79s/it]\n",
      "Generating...: 100%|██████████| 347/347 [12:12<00:00,  2.11s/it]\n",
      "Generating...: 100%|██████████| 266/266 [09:30<00:00,  2.15s/it]\n",
      "Generating...: 100%|██████████| 157/157 [03:54<00:00,  1.50s/it]\n",
      "Generating...: 100%|██████████| 329/329 [04:53<00:00,  1.12it/s]\n",
      "Generating...: 100%|██████████| 209/209 [03:36<00:00,  1.03s/it]\n",
      "Generating...: 100%|██████████| 134/134 [01:58<00:00,  1.13it/s]\n",
      "Generating...: 100%|██████████| 93/93 [01:32<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "await generate_paraphrase_external_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=True, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\quality_responses.json\", \"w\") as f:\n",
    "    json.dump(responses, f, indent=4)  # indent=4 makes it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating...: 100%|██████████| 140/140 [02:47<00:00,  1.20s/it]\n",
      "Generating...: 100%|██████████| 55/55 [01:17<00:00,  1.41s/it]\n",
      "Generating...: 100%|██████████| 130/130 [02:32<00:00,  1.18s/it]\n",
      "Generating...: 100%|██████████| 49/49 [01:31<00:00,  1.87s/it]\n",
      "Generating...: 100%|██████████| 297/297 [03:06<00:00,  1.59it/s]\n",
      "Generating...: 100%|██████████| 122/122 [01:24<00:00,  1.44it/s]\n",
      "Generating...: 100%|██████████| 246/246 [01:36<00:00,  2.56it/s]\n",
      "Generating...: 100%|██████████| 105/105 [00:58<00:00,  1.80it/s]\n"
     ]
    }
   ],
   "source": [
    "await generate_paraphrase_external_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"DeepSeek-V3\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######  \n",
    "await generate_paraphrase_external_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Llama-4-Scout-17B-16E-Instruct\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating...: 100%|██████████| 353/353 [02:04<00:00,  2.83it/s]\n",
      "Generating...: 100%|██████████| 174/174 [01:25<00:00,  2.04it/s]\n",
      "Generating...: 100%|██████████| 305/305 [02:15<00:00,  2.25it/s]\n",
      "Generating...: 100%|██████████| 164/164 [01:36<00:00,  1.69it/s]\n",
      "Generating...: 100%|██████████| 356/356 [02:33<00:00,  2.32it/s]\n",
      "Generating...: 100%|██████████| 196/196 [01:46<00:00,  1.84it/s]\n",
      "Generating...: 100%|██████████| 316/316 [02:11<00:00,  2.40it/s]\n",
      "Generating...: 100%|██████████| 197/197 [02:30<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "await generate_paraphrase_external_combinations(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"DeepSeek-V3\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"DeepSeek-V3\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"DeepSeek-V3\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"DeepSeek-V3\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating...: 100%|██████████| 167/167 [01:09<00:00,  2.39it/s]\n",
      "Generating...: 100%|██████████| 78/78 [00:58<00:00,  1.32it/s]\n",
      "Generating...: 100%|██████████| 438/438 [05:36<00:00,  1.30it/s]\n",
      "Generating...: 100%|██████████| 211/211 [03:55<00:00,  1.12s/it]\n",
      "Generating...: 100%|██████████| 342/342 [08:19<00:00,  1.46s/it]\n",
      "Generating...: 100%|██████████| 196/196 [03:07<00:00,  1.05it/s]\n",
      "Generating...: 100%|██████████| 110/110 [01:35<00:00,  1.15it/s]\n",
      "Generating...: 100%|██████████| 77/77 [01:21<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "await generate_paraphrase_external_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"DeepSeek-V3\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating...: 100%|██████████| 226/226 [01:56<00:00,  1.94it/s]\n",
      "Generating...: 100%|██████████| 91/91 [00:56<00:00,  1.61it/s]\n",
      "Generating...: 100%|██████████| 146/146 [01:21<00:00,  1.78it/s]\n",
      "Generating...: 100%|██████████| 69/69 [00:53<00:00,  1.30it/s]\n",
      "Generating...: 100%|██████████| 51/51 [00:45<00:00,  1.13it/s]\n",
      "Generating...: 100%|██████████| 33/33 [00:45<00:00,  1.37s/it]\n",
      "Generating...: 100%|██████████| 229/229 [02:28<00:00,  1.54it/s]\n",
      "Generating...: 100%|██████████| 112/112 [01:11<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "source": [
    "await generate_paraphrase_external_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating...: 100%|██████████| 200/200 [01:59<00:00,  1.67it/s]\n",
      "Generating...: 100%|██████████| 94/94 [01:36<00:00,  1.03s/it]\n",
      "Generating...: 100%|██████████| 149/149 [01:26<00:00,  1.72it/s]\n",
      "Generating...: 100%|██████████| 81/81 [01:16<00:00,  1.05it/s]\n",
      "Generating...: 100%|██████████| 70/70 [01:06<00:00,  1.05it/s]\n",
      "Generating...: 100%|██████████| 45/45 [01:07<00:00,  1.51s/it]\n",
      "Generating...: 100%|██████████| 164/164 [01:38<00:00,  1.67it/s]\n",
      "Generating...: 100%|██████████| 91/91 [00:51<00:00,  1.78it/s]\n"
     ]
    }
   ],
   "source": [
    "await generate_paraphrase_external_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Maverick-17B-128E-Instruct-FP8\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-4-Scout-17B-16E-Instruct\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "#######\n",
    "await generate_paraphrase_external_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False)\n",
    "if (len(failed_comparisons) > 0):\n",
    "    await generate_paraphrase_external_combinations(\"Meta-Llama-3.1-8B-Instruct-Turbo\", \"Qwen2.5-7B-Instruct-Turbo\", responses, harmful_subset=False, repeat_failures=True)\n",
    "    failed_comparisons = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\quality\\quality_responses.json\", \"w\") as f:\n",
    "    json.dump(responses, f, indent=4)  # indent=4 makes it more readable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
