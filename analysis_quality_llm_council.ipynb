{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preference (Harmful self pref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\preference_results_llm_council_original_harmful.json', 'r') as file:\n",
    "    preference_results_llm_council_original_harmful = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------+---------+---------+---------+\n",
      "|                                  |   (1,1) |   (1,2) |   (2,1) |   (2,2) |\n",
      "+==================================+=========+=========+=========+=========+\n",
      "| DeepSeek-V3                      |   96.00 |  102.00 |   97.00 |    4.00 |\n",
      "+----------------------------------+---------+---------+---------+---------+\n",
      "| Meta-Llama-3.1-8B-Instruct-Turbo |    8.00 |   56.00 |  230.00 |  826.00 |\n",
      "+----------------------------------+---------+---------+---------+---------+\n",
      "| Qwen2.5-7B-Instruct-Turbo        |   86.00 |  116.00 |  607.00 |  125.00 |\n",
      "+----------------------------------+---------+---------+---------+---------+\n",
      "\n",
      "Percentages\n",
      "+----------------------------------+---------+---------+---------+---------+\n",
      "|                                  |   (1,1) |   (1,2) |   (2,1) |   (2,2) |\n",
      "+==================================+=========+=========+=========+=========+\n",
      "| DeepSeek-V3                      |   32.11 |   34.11 |   32.44 |    1.34 |\n",
      "+----------------------------------+---------+---------+---------+---------+\n",
      "| Meta-Llama-3.1-8B-Instruct-Turbo |    0.71 |    5.00 |   20.54 |   73.75 |\n",
      "+----------------------------------+---------+---------+---------+---------+\n",
      "| Qwen2.5-7B-Instruct-Turbo        |    9.21 |   12.42 |   64.99 |   13.38 |\n",
      "+----------------------------------+---------+---------+---------+---------+\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(preference_results_llm_council_original_harmful)\n",
    "\n",
    "# Initialize a dictionary to store counts grouped by evaluator\n",
    "grouped_counts = {}\n",
    "\n",
    "# Group by evaluator and count occurrences\n",
    "for evaluator, group in df.groupby(\"evaluator\"):\n",
    "    count_1_1 = ((group[\"forward_comparison\"] == \"1\") & (group[\"backward_comparison\"] == \"1\")).sum()\n",
    "    count_1_2 = ((group[\"forward_comparison\"] == \"1\") & (group[\"backward_comparison\"] == \"2\")).sum()\n",
    "    count_2_1 = ((group[\"forward_comparison\"] == \"2\") & (group[\"backward_comparison\"] == \"1\")).sum()\n",
    "    count_2_2 = ((group[\"forward_comparison\"] == \"2\") & (group[\"backward_comparison\"] == \"2\")).sum()\n",
    "\n",
    "    grouped_counts[evaluator] = {\n",
    "        \"(1,1)\": count_1_1,\n",
    "        \"(1,2)\": count_1_2,\n",
    "        \"(2,1)\": count_2_1,\n",
    "        \"(2,2)\": count_2_2,\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame for display\n",
    "grouped_counts_df = pd.DataFrame.from_dict(grouped_counts, orient=\"index\")\n",
    "print(tabulate(grouped_counts_df, headers=\"keys\", tablefmt=\"grid\", floatfmt=\".2f\"))\n",
    "print(\"\\nPercentages\")\n",
    "# Convert counts to percentages\n",
    "grouped_counts_df_percentage = grouped_counts_df.div(grouped_counts_df.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Print the result in table format\n",
    "print(tabulate(grouped_counts_df_percentage, headers=\"keys\", tablefmt=\"grid\", floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\self_recog_quality.json', 'r') as file:\n",
    "    results = json.load(file)\n",
    "recog_results = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(recog_results)\n",
    "\n",
    "# Initialize a dictionary to store counts grouped by evaluator\n",
    "grouped_counts = {}\n",
    "print(\"DETECTION\")\n",
    "# Group by evaluator and count occurrences\n",
    "for evaluator, group in df.groupby(\"evaluator\"):\n",
    "    count_1_1 = ((group[\"forward_detection\"] == \"1\") & (group[\"backward_detection\"] == \"1\")).sum()\n",
    "    count_1_2 = ((group[\"forward_detection\"] == \"1\") & (group[\"backward_detection\"] == \"2\")).sum()\n",
    "    count_2_1 = ((group[\"forward_detection\"] == \"2\") & (group[\"backward_detection\"] == \"1\")).sum()\n",
    "    count_2_2 = ((group[\"forward_detection\"] == \"2\") & (group[\"backward_detection\"] == \"2\")).sum()\n",
    "\n",
    "    grouped_counts[evaluator] = {\n",
    "        \"(1,1)\": count_1_1,\n",
    "        \"(1,2)\": count_1_2,\n",
    "        \"(2,1)\": count_2_1,\n",
    "        \"(2,2)\": count_2_2,\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame for display\n",
    "grouped_counts_df = pd.DataFrame.from_dict(grouped_counts, orient=\"index\")\n",
    "\n",
    "# Convert counts to percentages\n",
    "grouped_counts_df_percentage = grouped_counts_df.div(grouped_counts_df.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Print the result in table format\n",
    "print(tabulate(grouped_counts_df_percentage, headers=\"keys\", tablefmt=\"grid\", floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preference/Recog (both correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\pref_both_correct_quality.json', 'r') as file:\n",
    "    preference_results_both_correct = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preference/Recog (both wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\pref_both_wrong_quality.json', 'r') as file:\n",
    "    preference_results_both_wrong = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pref Other wrong (source correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\pref_other_wrong_quality.json', 'r') as file:\n",
    "    preference_results_other_wrong = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer and Judge Acc: Overall vs Harmful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pref = preference_results + preference_results_both_correct + preference_results_both_wrong + preference_results_other_wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON file into a variable\n",
    "with open('.\\quality\\paraphrased_by_others.json', 'r') as file:\n",
    "    responses = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names\n",
    "model_names = [\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\"]\n",
    "\n",
    "# Initialize result tracking\n",
    "accuracy_results = {model: {'correct': 0, 'total': 0} for model in model_names}\n",
    "\n",
    "# Calculate accuracy for each model\n",
    "for record in responses[0]:\n",
    "    gt_label = record.get('output_label')\n",
    "    for model in model_names:\n",
    "        model_label = record.get(f\"{model}_output_label\")\n",
    "        if model_label is not None:\n",
    "            accuracy_results[model]['total'] += 1\n",
    "            if model_label == gt_label:\n",
    "                accuracy_results[model]['correct'] += 1\n",
    "\n",
    "# Create a DataFrame with counts and accuracy percentages\n",
    "accuracy_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": model,\n",
    "        \"Correct\": result[\"correct\"],\n",
    "        \"Total\": result[\"total\"],\n",
    "        \"Answer Accuracy (%)\": f\"{(result['correct'] / result['total'] * 100):.2f}%\" if result[\"total\"] > 0 else \"N/A\"\n",
    "    }\n",
    "    for model, result in accuracy_results.items()\n",
    "])\n",
    "\n",
    "# Print the result\n",
    "print(tabulate(accuracy_df, headers=\"keys\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model list\n",
    "model_names = [\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\"]\n",
    "\n",
    "# Initialize tracking dictionaries\n",
    "result_stats = {model: {\"correct\": 0, \"total\": 0} for model in model_names}\n",
    "\n",
    "# Process preference_results\n",
    "for record in preference_results:\n",
    "    model = record[\"evaluator\"]\n",
    "    if model in result_stats:\n",
    "        result_stats[model][\"total\"] += 1\n",
    "        if record[\"forward_comparison\"] == \"2\" and record[\"backward_comparison\"] == \"1\":\n",
    "            result_stats[model][\"correct\"] += 1\n",
    "\n",
    "# Process preference_results_other_wrong\n",
    "for record in preference_results_other_wrong:\n",
    "    model = record[\"evaluator\"]\n",
    "    if model in result_stats:\n",
    "        result_stats[model][\"total\"] += 1\n",
    "        if record[\"forward_comparison\"] == \"1\" and record[\"backward_comparison\"] == \"2\":\n",
    "            result_stats[model][\"correct\"] += 1\n",
    "\n",
    "# Create DataFrame to display results\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": model,\n",
    "        \"Correct\": stats[\"correct\"],\n",
    "        \"Total\": stats[\"total\"],\n",
    "        \"Accuracy (%)\": f\"{(stats['correct'] / stats['total'] * 100):.2f}%\" if stats[\"total\"] > 0 else \"N/A\"\n",
    "    }\n",
    "    for model, stats in result_stats.items()\n",
    "])\n",
    "\n",
    "print(\"\\nEvaluator Accuracy Results\")\n",
    "# Display the table\n",
    "print(tabulate(results_df, headers=\"keys\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Harmful Quadrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluator Accuracy Results\n",
      "+----+----------------------------------+-----------+---------+----------------+\n",
      "|    | Model                            |   Correct |   Total | Accuracy (%)   |\n",
      "+====+==================================+===========+=========+================+\n",
      "|  0 | Qwen2.5-7B-Instruct-Turbo        |       607 |     934 | 64.99%         |\n",
      "+----+----------------------------------+-----------+---------+----------------+\n",
      "|  1 | Meta-Llama-3.1-8B-Instruct-Turbo |       230 |    1120 | 20.54%         |\n",
      "+----+----------------------------------+-----------+---------+----------------+\n",
      "|  2 | DeepSeek-V3                      |        97 |     299 | 32.44%         |\n",
      "+----+----------------------------------+-----------+---------+----------------+\n"
     ]
    }
   ],
   "source": [
    "# Model list\n",
    "model_names = [\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\"]\n",
    "\n",
    "# Initialize tracking dictionaries\n",
    "result_stats = {model: {\"correct\": 0, \"total\": 0} for model in model_names}\n",
    "\n",
    "# Process preference_results\n",
    "for record in preference_results_llm_council_original_harmful:\n",
    "    model = record[\"evaluator\"]\n",
    "    if model in result_stats:\n",
    "        result_stats[model][\"total\"] += 1\n",
    "        if record[\"forward_comparison\"] == \"2\" and record[\"backward_comparison\"] == \"1\":\n",
    "            result_stats[model][\"correct\"] += 1\n",
    "\n",
    "\n",
    "# Create DataFrame to display results\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": model,\n",
    "        \"Correct\": stats[\"correct\"],\n",
    "        \"Total\": stats[\"total\"],\n",
    "        \"Accuracy (%)\": f\"{(stats['correct'] / stats['total'] * 100):.2f}%\" if stats[\"total\"] > 0 else \"N/A\"\n",
    "    }\n",
    "    for model, stats in result_stats.items()\n",
    "])\n",
    "\n",
    "print(\"\\nEvaluator Accuracy Results\")\n",
    "# Display the table\n",
    "print(tabulate(results_df, headers=\"keys\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Format Prompt (A>B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\preference_results_llm_council_format_original_harmful.json', 'r') as file:\n",
    "    preference_results_llm_council_format_original_harmful = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------+---------+---------+---------+\n",
      "|                                  |   (1,1) |   (1,2) |   (2,1) |   (2,2) |\n",
      "+==================================+=========+=========+=========+=========+\n",
      "| DeepSeek-V3                      |    0.00 |    0.00 |    0.00 |    0.00 |\n",
      "+----------------------------------+---------+---------+---------+---------+\n",
      "| Meta-Llama-3.1-8B-Instruct-Turbo |    0.00 |    0.00 |    0.00 |    0.00 |\n",
      "+----------------------------------+---------+---------+---------+---------+\n",
      "| Qwen2.5-7B-Instruct-Turbo        |    0.00 |    0.00 |    0.00 |    0.00 |\n",
      "+----------------------------------+---------+---------+---------+---------+\n",
      "\n",
      "Percentages\n",
      "+----------------------------------+---------+---------+---------+---------+\n",
      "|                                  |   (1,1) |   (1,2) |   (2,1) |   (2,2) |\n",
      "+==================================+=========+=========+=========+=========+\n",
      "| DeepSeek-V3                      |     nan |     nan |     nan |     nan |\n",
      "+----------------------------------+---------+---------+---------+---------+\n",
      "| Meta-Llama-3.1-8B-Instruct-Turbo |     nan |     nan |     nan |     nan |\n",
      "+----------------------------------+---------+---------+---------+---------+\n",
      "| Qwen2.5-7B-Instruct-Turbo        |     nan |     nan |     nan |     nan |\n",
      "+----------------------------------+---------+---------+---------+---------+\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(preference_results_llm_council_format_original_harmful)\n",
    "\n",
    "# Initialize a dictionary to store counts grouped by evaluator\n",
    "grouped_counts = {}\n",
    "\n",
    "# Group by evaluator and count occurrences\n",
    "for evaluator, group in df.groupby(\"evaluator\"):\n",
    "    count_1_1 = ((group[\"forward_comparison\"].str.contains(\"A>B\")) & (group[\"backward_comparison\"].str.contains(\"A>B\"))).sum()\n",
    "    count_1_2 = ((group[\"forward_comparison\"].str.contains(\"A>B\")) & (group[\"backward_comparison\"].str.contains(\"B>A\"))).sum()\n",
    "    count_2_1 = ((group[\"forward_comparison\"].str.contains(\"B>A\")) & (group[\"backward_comparison\"].str.contains(\"A>B\"))).sum()\n",
    "    count_2_2 = ((group[\"forward_comparison\"].str.contains(\"B>A\")) & (group[\"backward_comparison\"].str.contains(\"B>A\"))).sum()\n",
    "\n",
    "    grouped_counts[evaluator] = {\n",
    "        \"(1,1)\": count_1_1,\n",
    "        \"(1,2)\": count_1_2,\n",
    "        \"(2,1)\": count_2_1,\n",
    "        \"(2,2)\": count_2_2,\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame for display\n",
    "grouped_counts_df = pd.DataFrame.from_dict(grouped_counts, orient=\"index\")\n",
    "print(tabulate(grouped_counts_df, headers=\"keys\", tablefmt=\"grid\", floatfmt=\".2f\"))\n",
    "print(\"\\nPercentages\")\n",
    "# Convert counts to percentages\n",
    "grouped_counts_df_percentage = grouped_counts_df.div(grouped_counts_df.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Print the result in table format\n",
    "print(tabulate(grouped_counts_df_percentage, headers=\"keys\", tablefmt=\"grid\", floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m result_stats:\n\u001b[0;32m     11\u001b[0m         result_stats[model][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward_comparison\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB>A\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m record[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackward_comparison\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA>B\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     13\u001b[0m             result_stats[model][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create DataFrame to display results\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'str'"
     ]
    }
   ],
   "source": [
    "# Model list\n",
    "model_names = [\"Qwen2.5-7B-Instruct-Turbo\", \"Meta-Llama-3.1-8B-Instruct-Turbo\", \"DeepSeek-V3\"]\n",
    "\n",
    "# Initialize tracking dictionaries\n",
    "result_stats = {model: {\"correct\": 0, \"total\": 0} for model in model_names}\n",
    "\n",
    "# Process preference_results\n",
    "for record in preference_results_llm_council_original_harmful:\n",
    "    model = record[\"evaluator\"]\n",
    "    if model in result_stats:\n",
    "        result_stats[model][\"total\"] += 1\n",
    "        if record[\"forward_comparison\"].str.contains(\"B>A\") and record[\"backward_comparison\"].str.contains(\"A>B\"):\n",
    "            result_stats[model][\"correct\"] += 1\n",
    "\n",
    "\n",
    "# Create DataFrame to display results\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": model,\n",
    "        \"Correct\": stats[\"correct\"],\n",
    "        \"Total\": stats[\"total\"],\n",
    "        \"Accuracy (%)\": f\"{(stats['correct'] / stats['total'] * 100):.2f}%\" if stats[\"total\"] > 0 else \"N/A\"\n",
    "    }\n",
    "    for model, stats in result_stats.items()\n",
    "])\n",
    "\n",
    "print(\"\\nEvaluator Accuracy Results\")\n",
    "# Display the table\n",
    "print(tabulate(results_df, headers=\"keys\", tablefmt=\"grid\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
