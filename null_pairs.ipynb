{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\G25971483\\Desktop\\Projects\\LLM\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data import load_data, SOURCES, save_to_json, load_from_json\n",
    "from models import (\n",
    "    get_gpt_recognition_logprobs,\n",
    "    get_model_choice,\n",
    "    get_logprobs_choice_with_sources,\n",
    "    get_gpt_score,\n",
    "    get_gpt_summary_similarity,\n",
    "    get_gpt_paraphrase,\n",
    "    get_gpt_choice_null_baseline\n",
    ")\n",
    "\n",
    "from math import exp\n",
    "from pprint import pprint\n",
    "from random import shuffle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-ATjvcd6TmwP3SpMZgz5ELc2TdzE9N', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='3', bytes=[51], logprob=-0.0016588744, top_logprobs=[TopLogprob(token='3', bytes=[51], logprob=-0.0016588744), TopLogprob(token='1', bytes=[49], logprob=-6.4954944), TopLogprob(token='2', bytes=[50], logprob=-8.830752)])], refusal=None), message=ChatCompletionMessage(content='3', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1731651424, model='gpt-4-1106-preview', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=212, total_tokens=213, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0, audio_tokens=0, accepted_prediction_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details={'cached_tokens': 0, 'audio_tokens': 0}))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TopLogprob(token='3', bytes=[51], logprob=-0.0016588744),\n",
       " TopLogprob(token='1', bytes=[49], logprob=-6.4954944),\n",
       " TopLogprob(token='2', bytes=[50], logprob=-8.830752)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].logprobs.content[0].top_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].logprobs.content[0].top_logprobs[0].token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The logprob for token '3' is: -0.0016588744\n",
      "The prob for token '3' is: 0.99834250077162\n"
     ]
    }
   ],
   "source": [
    "# Extract the logprob for the token '3'\n",
    "logprob_for_3 = next((item.logprob for item in response.choices[0].logprobs.content[0].top_logprobs if item.token == '3'), None)\n",
    "\n",
    "# Output the result\n",
    "print(f\"The logprob for token '3' is: {logprob_for_3}\")\n",
    "print(f\"The prob for token '3' is: {exp(logprob_for_3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt_detect_null(\n",
    "    dataset,\n",
    "    model,\n",
    "    comparison_models,\n",
    "    starting_idx=0,\n",
    "    ending_idx = 1000\n",
    "):\n",
    "    exact_model = model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "\n",
    "    results = []  \n",
    "\n",
    "    for key in tqdm(keys[starting_idx:ending_idx], desc=\"Processing keys\"):\n",
    "        article = articles[key]\n",
    "        first_summary = responses[comparison_models[0]][key]\n",
    "        second_summary = responses[comparison_models[1]][key]\n",
    "        # Shuffle at every key\n",
    "        summaries = [first_summary, second_summary]\n",
    "        random.shuffle(summaries)\n",
    "        first_summary, second_summary = summaries\n",
    "        \n",
    "        result = {\"key\": key}\n",
    "\n",
    "        try:\n",
    "            forward_result = get_gpt_choice_null_baseline(\n",
    "                first_summary,\n",
    "                second_summary,\n",
    "                article,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "\n",
    "            # forward_choice = forward_result[0].token\n",
    "\n",
    "            logprob_for_3 = next((item.logprob for item in forward_result if item.token == '3'), None)\n",
    "            result[\"detection_score\"] = exp(logprob_for_3)\n",
    "\n",
    "        except ValueError:\n",
    "            print('Error:', key)\n",
    "        \n",
    "        results.append(result)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gpt4\n",
      "['human', 'gpt35']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 500/500 [06:28<00:00,  1.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt4_results_null_pair_human_gpt35.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    \n",
    "    print(f\"Starting {model}\")\n",
    "    comparison_models =  [\"human\", \"gpt35\"]\n",
    "    print(comparison_models)\n",
    "    results = generate_gpt_detect_null(\n",
    "        \"cnn\", model,comparison_models, starting_idx=500\n",
    "    )\n",
    "    #Save results\n",
    "    file_name = f\"{model}_results_null_pair_{comparison_models[0]}_{comparison_models[1]}.json\"\n",
    "    print(file_name)\n",
    "    path = os.path.join(\"results\",\"cnn\",\"null_pair\",file_name)\n",
    "    save_to_json(results,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
