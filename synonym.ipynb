{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\G25971483\\\\Desktop\\\\Projects\\\\LLM\\\\self_recognition'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "random.seed(123)\n",
    "\n",
    "all_words = []\n",
    "all_syn = []\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"\n",
    "    Finds and returns synonyms for a given word using WordNet.\n",
    "    This function takes a word as input, searches for its synonyms \n",
    "    using the WordNet synsets, and returns a list of synonyms.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word for which to find synonyms.\n",
    "    Returns:\n",
    "        list: A list of synonyms for the input word. Returns an \n",
    "              empty list if no synonyms are found.\n",
    "    \"\"\"\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for name in syn.lemma_names():\n",
    "            # Exclude the original word to avoid replacing it with itself\n",
    "            name = name.replace('_',' ')\n",
    "            if name.lower() != word.lower():\n",
    "                synonyms.append(name)\n",
    "    return synonyms\n",
    "\n",
    "def replace_with_synonyms(sentence, num_words_to_replace):\n",
    "    \"\"\"\n",
    "    Replaces a specified number of words in a sentence with their synonyms.\n",
    "\n",
    "    This function takes a sentence and an integer specifying the number of words \n",
    "    to replace with synonyms. It randomly samples 2x the required number of words \n",
    "    to ensure replacements are possible even if some words do not have synonyms.\n",
    "    It uses the `get_synonyms` function to find synonyms for each sampled word,\n",
    "    and replaces words in the sentence until the specified number is reached.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence from which words will be replaced.\n",
    "        num_words_to_replace (int): The number of words in the sentence to be replaced by synonyms.\n",
    "\n",
    "    Returns:\n",
    "        str: The modified sentence with the specified number of words replaced by synonyms.\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "    # Filter out non-alphabetic tokens (like punctuation)\n",
    "    words_alpha = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    # Randomly sample words to replace - i use 2x words just to account for words without synonym\n",
    "    words_to_replace = random.sample(words_alpha, min(2*num_words_to_replace, len(words_alpha)))\n",
    "    \n",
    "    # Create a new sentence with synonyms replaced\n",
    "    words_replaced = 0\n",
    "    new_sentence = []\n",
    "    for word in words:\n",
    "        if word in words_to_replace:\n",
    "            synonyms = get_synonyms(word)\n",
    "            if synonyms and words_replaced < num_words_to_replace:\n",
    "                # Replace with a random synonym\n",
    "                new_word = random.choice(synonyms)\n",
    "                new_sentence.append(new_word)\n",
    "                #operational\n",
    "                all_words.append(word)\n",
    "                all_syn.append(synonyms)\n",
    "                words_replaced +=1\n",
    "            else:\n",
    "                # If no synonym is found, keep the original word\n",
    "                new_sentence.append(word)\n",
    "        else:\n",
    "            new_sentence.append(word)\n",
    "    \n",
    "    return ' '.join(new_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\G25971483\\Desktop\\Projects\\LLM\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data import load_data, SOURCES, save_to_json, load_from_json\n",
    "responses, articles, keys = load_data(\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Former government contractor indicted for stealing nuclear materials\n",
      "Roy Lynn Oakley accused of attempting to sell restricted uranium enrichment components\n",
      "Oakley faces up to 10 years in prison and a $250,000 fine per count\n",
      "FBI sting operation prevented the materials from reaching foreign entities\n"
     ]
    }
   ],
   "source": [
    "key = keys[22]\n",
    "random_summary = responses[\"gpt4\"][key]\n",
    "print(random_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Former government declarer indict for thievery atomic materials Roy Lynn Oakley accused of attempting to sell restricted atomic number 92 enrichment components Oakley faces up to 10 years in prison and a $ 250,000 fine per count FBI sting operation prevented the materials from reaching foreign entities\n"
     ]
    }
   ],
   "source": [
    "new_summary = replace_with_synonyms(random_summary, 5)  # Replace 5of the words\n",
    "print(new_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_words)):\n",
    "    print('word:', all_words[i])\n",
    "    print(all_syn[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply to Dataset and Check Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\G25971483\\Desktop\\Projects\\LLM\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data import load_data, SOURCES, save_to_json, load_from_json\n",
    "from models import (\n",
    "    get_gpt_recognition_logprobs,\n",
    "    get_model_choice,\n",
    "    get_logprobs_choice_with_sources,\n",
    "    get_gpt_score,\n",
    ")\n",
    "\n",
    "from math import exp\n",
    "from pprint import pprint\n",
    "from random import shuffle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only suitable for GPT models\n",
    "def generate_gpt_detect_recognition_synonym(\n",
    "    dataset,\n",
    "    model,\n",
    "    starting_idx=0,\n",
    "    detection_type=\"detection\",\n",
    "    replace_synonym = False,\n",
    "    num_words_to_replace = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates detection scores for GPT model outputs compared to other summaries.\n",
    "\n",
    "    This function takes a dataset name, a base model for inference, a starting index \n",
    "    from which to begin enumeration of the dataset, and various options for detection \n",
    "    and synonym replacement. It makes API calls to GPT models using the OpenAI API key \n",
    "    to evaluate the similarity of each summary against all other summaries. If synonym \n",
    "    replacement is enabled, a specified number of words are replaced before comparison.\n",
    "    \n",
    "    The function performs inference using the base model, compares generated summaries \n",
    "    in forward and backward order, and returns a JSON object containing detection results.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): The name of the dataset (e.g., \"cnn\") containing the articles.\n",
    "        model (str): The base model on which inference will be performed.\n",
    "        starting_idx (int, optional): The index to start processing articles from. Defaults to 0.\n",
    "        detection_type (str, optional): The type of detection to perform. Defaults to \"detection\".\n",
    "        replace_synonym (bool, optional): Whether to replace words in the summaries with synonyms. Defaults to False.\n",
    "        num_words_to_replace (int, optional): The number of words to replace with synonyms if `replace_synonym` is True. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        dict: A JSON object containing information about:\n",
    "            - Model compared against\n",
    "            - Key of the article\n",
    "            - Forward detection + probability\n",
    "            - Backward detection + probability\n",
    "            - Overall detection score\n",
    "\n",
    "    \"\"\"\n",
    "    # For retrieving summaries, the specific fine-tuning version isn't needed\n",
    "    exact_model = model\n",
    "    model = \"gpt35\" if model.endswith(\"gpt35\") else model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "    results = []  # load_from_json(f\"results/{model}_results.json\")\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:], desc=\"Processing keys\"):\n",
    "        article = articles[key]\n",
    "\n",
    "        source_summary = responses[model][key]\n",
    "\n",
    "        # replace synonym\n",
    "        if replace_synonym:\n",
    "            source_summary = replace_with_synonyms(source_summary, num_words_to_replace)\n",
    "\n",
    "        for other in [s for s in SOURCES if s != model]:\n",
    "            result = {\"key\": key, \"model\": other}\n",
    "            other_summary = responses[other][key]\n",
    "\n",
    "            # Detection\n",
    "            forward_result = get_model_choice(\n",
    "                source_summary,\n",
    "                other_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "            backward_result = get_model_choice(\n",
    "                other_summary,\n",
    "                source_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "\n",
    "            forward_choice = forward_result[0].token\n",
    "            backward_choice = backward_result[0].token\n",
    "\n",
    "            result[\"forward_detection\"] = forward_choice\n",
    "            result[\"forward_detection_probability\"] = exp(forward_result[0].logprob)\n",
    "            result[\"backward_detection\"] = backward_choice\n",
    "            result[\"backward_detection_probability\"] = exp(backward_result[0].logprob)\n",
    "\n",
    "            match (forward_choice, backward_choice):\n",
    "                case (\"1\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"1\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "\n",
    "            results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'claude', 'gpt35', 'gpt4', 'llama']\n",
      "Starting gpt4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 50/50 [13:35<00:00, 16.32s/it]\n"
     ]
    }
   ],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    print(SOURCES)\n",
    "    print(f\"Starting {model}\")\n",
    "    num_synonym = 5\n",
    "    results = generate_gpt_detect_recognition_synonym(\n",
    "        \"cnn\", model,replace_synonym=True, num_words_to_replace=num_synonym, starting_idx=950\n",
    "    )\n",
    "    #Save results\n",
    "    file_name = f\"{model}_results_{num_synonym}_replace_50_sentence.json\"\n",
    "    path = os.path.join(\"results\",\"cnn\",\"synonym\",file_name)\n",
    "    save_to_json(results,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR THE OTHER 450 data points\n",
    "def generate_gpt_detect_recognition_synonym(\n",
    "    dataset,\n",
    "    model,\n",
    "    starting_idx=0,\n",
    "    ending_idx=1000,\n",
    "    detection_type=\"detection\",\n",
    "    replace_synonym = False,\n",
    "    num_words_to_replace = 0\n",
    "):\n",
    "\n",
    "    # For retrieving summaries, the specific fine-tuning version isn't needed\n",
    "    exact_model = model\n",
    "    model = \"gpt35\" if model.endswith(\"gpt35\") else model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "    results = []  # load_from_json(f\"results/{model}_results.json\")\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:ending_idx], desc=\"Processing keys\"):\n",
    "        article = articles[key]\n",
    "\n",
    "        source_summary = responses[model][key]\n",
    "\n",
    "        # replace synonym\n",
    "        if replace_synonym:\n",
    "            source_summary = replace_with_synonyms(source_summary, num_words_to_replace)\n",
    "\n",
    "        for other in [s for s in SOURCES if s != model]:\n",
    "            result = {\"key\": key, \"model\": other}\n",
    "            other_summary = responses[other][key]\n",
    "\n",
    "            # Detection\n",
    "            forward_result = get_model_choice(\n",
    "                source_summary,\n",
    "                other_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "            backward_result = get_model_choice(\n",
    "                other_summary,\n",
    "                source_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "\n",
    "            forward_choice = forward_result[0].token\n",
    "            backward_choice = backward_result[0].token\n",
    "\n",
    "            result[\"forward_detection\"] = forward_choice\n",
    "            result[\"forward_detection_probability\"] = exp(forward_result[0].logprob)\n",
    "            result[\"backward_detection\"] = backward_choice\n",
    "            result[\"backward_detection_probability\"] = exp(backward_result[0].logprob)\n",
    "\n",
    "            match (forward_choice, backward_choice):\n",
    "                case (\"1\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"1\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "\n",
    "            results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply synonym to 'other' example as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only suitable for GPT models\n",
    "def generate_gpt_detect_recognition_dual_synonym(\n",
    "    dataset,\n",
    "    model,\n",
    "    starting_idx=0,\n",
    "    detection_type=\"detection\",\n",
    "    replace_synonym = False,\n",
    "    num_words_to_replace = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates detection scores for GPT model outputs compared to other summaries using a dual synyonym replacement strategy.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): The name of the dataset (e.g., \"cnn\") containing the articles.\n",
    "        model (str): The base model on which inference will be performed.\n",
    "        starting_idx (int, optional): The index to start processing articles from. Defaults to 0.\n",
    "        detection_type (str, optional): The type of detection to perform. Defaults to \"detection\".\n",
    "        replace_synonym (bool, optional): Whether to replace words in the summaries with synonyms. Defaults to False.\n",
    "        num_words_to_replace (int, optional): The number of words to replace with synonyms if `replace_synonym` is True. Defaults to 0. Replaces the same number in both sentences being compared.\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # For retrieving summaries, the specific fine-tuning version isn't needed\n",
    "    exact_model = model\n",
    "    model = \"gpt35\" if model.endswith(\"gpt35\") else model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "    results = []  # load_from_json(f\"results/{model}_results.json\")\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:], desc=\"Processing keys\"):\n",
    "        article = articles[key]\n",
    "\n",
    "        source_summary = responses[model][key]\n",
    "\n",
    "        # replace synonym\n",
    "        if replace_synonym:\n",
    "            source_summary = replace_with_synonyms(source_summary, num_words_to_replace)\n",
    "\n",
    "        for other in [s for s in SOURCES if s != model]:\n",
    "            result = {\"key\": key, \"model\": other}\n",
    "            other_summary = responses[other][key]\n",
    "\n",
    "            # replace synonym\n",
    "            if replace_synonym:\n",
    "                other_summary = replace_with_synonyms(other_summary, num_words_to_replace)\n",
    "\n",
    "            # Detection\n",
    "            forward_result = get_model_choice(\n",
    "                source_summary,\n",
    "                other_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "            backward_result = get_model_choice(\n",
    "                other_summary,\n",
    "                source_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "\n",
    "            forward_choice = forward_result[0].token\n",
    "            backward_choice = backward_result[0].token\n",
    "\n",
    "            result[\"forward_detection\"] = forward_choice\n",
    "            result[\"forward_detection_probability\"] = exp(forward_result[0].logprob)\n",
    "            result[\"backward_detection\"] = backward_choice\n",
    "            result[\"backward_detection_probability\"] = exp(backward_result[0].logprob)\n",
    "\n",
    "            match (forward_choice, backward_choice):\n",
    "                case (\"1\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"1\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "\n",
    "            results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'claude', 'gpt35', 'gpt4', 'llama']\n",
      "Starting gpt4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 50/50 [13:51<00:00, 16.63s/it]\n"
     ]
    }
   ],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    print(SOURCES)\n",
    "    print(f\"Starting {model}\")\n",
    "    num_synonym = 5\n",
    "    results = generate_gpt_detect_recognition_dual_synonym(\n",
    "        \"cnn\", model,replace_synonym=True, num_words_to_replace=num_synonym, starting_idx=950\n",
    "    )\n",
    "    #Save results\n",
    "    file_name = f\"{model}_results_{num_synonym}_replace_bothsentences_50_sentence.json\"\n",
    "    path = os.path.join(\"results\",\"cnn\",\"synonym\",file_name)\n",
    "    save_to_json(results,path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
