{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('punkt_tab')\n",
    "# ! pip install scikit-learn\n",
    "# ! pip install sentence-transformers scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import load_data, SOURCES, save_to_json, load_from_json\n",
    "from models import (\n",
    "    get_gpt_recognition_logprobs,\n",
    "    get_model_choice,\n",
    "    get_logprobs_choice_with_sources,\n",
    "    get_gpt_score,\n",
    "    get_gpt_summary_similarity,\n",
    "    get_gpt_paraphrase,\n",
    "    get_claude_paraphrase\n",
    ")\n",
    "\n",
    "from math import exp\n",
    "from pprint import pprint\n",
    "from random import shuffle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonym Wordnet Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "random.seed(123)\n",
    "\n",
    "all_words = []\n",
    "all_syn = []\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"\n",
    "    Finds and returns synonyms for a given word using WordNet.\n",
    "    This function takes a word as input, searches for its synonyms \n",
    "    using the WordNet synsets, and returns a list of synonyms.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word for which to find synonyms.\n",
    "    Returns:\n",
    "        list: A list of synonyms for the input word. Returns an \n",
    "              empty list if no synonyms are found.\n",
    "    \"\"\"\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for name in syn.lemma_names():\n",
    "            # Exclude the original word to avoid replacing it with itself\n",
    "            name = name.replace('_',' ')\n",
    "            if name.lower() != word.lower():\n",
    "                synonyms.append(name)\n",
    "    return synonyms\n",
    "\n",
    "def replace_with_synonyms(sentence, num_words_to_replace):\n",
    "    \"\"\"\n",
    "    Replaces a specified number of words in a sentence with their synonyms.\n",
    "\n",
    "    This function takes a sentence and an integer specifying the number of words \n",
    "    to replace with synonyms. It randomly samples 2x the required number of words \n",
    "    to ensure replacements are possible even if some words do not have synonyms.\n",
    "    It uses the `get_synonyms` function to find synonyms for each sampled word,\n",
    "    and replaces words in the sentence until the specified number is reached.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence from which words will be replaced.\n",
    "        num_words_to_replace (int): The number of words in the sentence to be replaced by synonyms.\n",
    "\n",
    "    Returns:\n",
    "        str: The modified sentence with the specified number of words replaced by synonyms.\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "    # Filter out non-alphabetic tokens (like punctuation)\n",
    "    words_alpha = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    # Randomly sample words to replace - i use 2x words just to account for words without synonym\n",
    "    words_to_replace = random.sample(words_alpha, min(2*num_words_to_replace, len(words_alpha)))\n",
    "    \n",
    "    # Create a new sentence with synonyms replaced\n",
    "    words_replaced = 0\n",
    "    new_sentence = []\n",
    "    for word in words:\n",
    "        if word in words_to_replace:\n",
    "            synonyms = get_synonyms(word)\n",
    "            if synonyms and words_replaced < num_words_to_replace:\n",
    "                # Replace with a random synonym\n",
    "                new_word = random.choice(synonyms)\n",
    "                new_sentence.append(new_word)\n",
    "                #operational\n",
    "                all_words.append(word)\n",
    "                all_syn.append(synonyms)\n",
    "                words_replaced +=1\n",
    "            else:\n",
    "                # If no synonym is found, keep the original word\n",
    "                new_sentence.append(word)\n",
    "        else:\n",
    "            new_sentence.append(word)\n",
    "    \n",
    "    return ' '.join(new_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\G25971483\\Desktop\\Projects\\LLM\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data import load_data, SOURCES, save_to_json, load_from_json\n",
    "responses, articles, keys = load_data(\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Former government contractor indicted for stealing nuclear materials\n",
      "Roy Lynn Oakley accused of attempting to sell restricted uranium enrichment components\n",
      "Oakley faces up to 10 years in prison and a $250,000 fine per count\n",
      "FBI sting operation prevented the materials from reaching foreign entities\n"
     ]
    }
   ],
   "source": [
    "key = keys[22]\n",
    "random_summary = responses[\"gpt4\"][key]\n",
    "print(random_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Former government declarer indict for thievery atomic materials Roy Lynn Oakley accused of attempting to sell restricted atomic number 92 enrichment components Oakley faces up to 10 years in prison and a $ 250,000 fine per count FBI sting operation prevented the materials from reaching foreign entities\n"
     ]
    }
   ],
   "source": [
    "new_summary = replace_with_synonyms(random_summary, 5)  # Replace 5of the words\n",
    "print(new_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_words)):\n",
    "    print('word:', all_words[i])\n",
    "    print(all_syn[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply to Dataset and Check Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only suitable for GPT models\n",
    "def generate_gpt_detect_recognition_synonym(\n",
    "    dataset,\n",
    "    model,\n",
    "    starting_idx=0,\n",
    "    detection_type=\"detection\",\n",
    "    replace_synonym = False,\n",
    "    num_words_to_replace = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates detection scores for GPT model outputs compared to other summaries.\n",
    "\n",
    "    This function takes a dataset name, a base model for inference, a starting index \n",
    "    from which to begin enumeration of the dataset, and various options for detection \n",
    "    and synonym replacement. It makes API calls to GPT models using the OpenAI API key \n",
    "    to evaluate the similarity of each summary against all other summaries. If synonym \n",
    "    replacement is enabled, a specified number of words are replaced before comparison.\n",
    "    \n",
    "    The function performs inference using the base model, compares generated summaries \n",
    "    in forward and backward order, and returns a JSON object containing detection results.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): The name of the dataset (e.g., \"cnn\") containing the articles.\n",
    "        model (str): The base model on which inference will be performed.\n",
    "        starting_idx (int, optional): The index to start processing articles from. Defaults to 0.\n",
    "        detection_type (str, optional): The type of detection to perform. Defaults to \"detection\".\n",
    "        replace_synonym (bool, optional): Whether to replace words in the summaries with synonyms. Defaults to False.\n",
    "        num_words_to_replace (int, optional): The number of words to replace with synonyms if `replace_synonym` is True. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        dict: A JSON object containing information about:\n",
    "            - Model compared against\n",
    "            - Key of the article\n",
    "            - Forward detection + probability\n",
    "            - Backward detection + probability\n",
    "            - Overall detection score\n",
    "\n",
    "    \"\"\"\n",
    "    # For retrieving summaries, the specific fine-tuning version isn't needed\n",
    "    exact_model = model\n",
    "    model = \"gpt35\" if model.endswith(\"gpt35\") else model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "    results = []  # load_from_json(f\"results/{model}_results.json\")\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:], desc=\"Processing keys\"):\n",
    "        article = articles[key]\n",
    "\n",
    "        source_summary = responses[model][key]\n",
    "\n",
    "        # replace synonym\n",
    "        if replace_synonym:\n",
    "            source_summary = replace_with_synonyms(source_summary, num_words_to_replace)\n",
    "\n",
    "        for other in [s for s in SOURCES if s != model]:\n",
    "            result = {\"key\": key, \"model\": other}\n",
    "            other_summary = responses[other][key]\n",
    "\n",
    "            # Detection\n",
    "            forward_result = get_model_choice(\n",
    "                source_summary,\n",
    "                other_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "            backward_result = get_model_choice(\n",
    "                other_summary,\n",
    "                source_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "\n",
    "            forward_choice = forward_result[0].token\n",
    "            backward_choice = backward_result[0].token\n",
    "\n",
    "            result[\"forward_detection\"] = forward_choice\n",
    "            result[\"forward_detection_probability\"] = exp(forward_result[0].logprob)\n",
    "            result[\"backward_detection\"] = backward_choice\n",
    "            result[\"backward_detection_probability\"] = exp(backward_result[0].logprob)\n",
    "\n",
    "            match (forward_choice, backward_choice):\n",
    "                case (\"1\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"1\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "\n",
    "            results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    print(SOURCES)\n",
    "    print(f\"Starting {model}\")\n",
    "    num_synonym = 5\n",
    "    results = generate_gpt_detect_recognition_synonym(\n",
    "        \"cnn\", model,replace_synonym=True, num_words_to_replace=num_synonym, starting_idx=950\n",
    "    )\n",
    "    #Save results\n",
    "    file_name = f\"{model}_results_{num_synonym}_replace_50_sentence.json\"\n",
    "    path = os.path.join(\"results\",\"cnn\",\"synonym\",file_name)\n",
    "    save_to_json(results,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR THE OTHER 450 data points\n",
    "def generate_gpt_detect_recognition_synonym(\n",
    "    dataset,\n",
    "    model,\n",
    "    starting_idx=0,\n",
    "    ending_idx=1000,\n",
    "    detection_type=\"detection\",\n",
    "    replace_synonym = False,\n",
    "    num_words_to_replace = 0\n",
    "):\n",
    "\n",
    "    # For retrieving summaries, the specific fine-tuning version isn't needed\n",
    "    exact_model = model\n",
    "    model = \"gpt35\" if model.endswith(\"gpt35\") else model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "    results = []  # load_from_json(f\"results/{model}_results.json\")\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:ending_idx], desc=\"Processing keys\"):\n",
    "        article = articles[key]\n",
    "\n",
    "        source_summary = responses[model][key]\n",
    "\n",
    "        # replace synonym\n",
    "        if replace_synonym:\n",
    "            source_summary = replace_with_synonyms(source_summary, num_words_to_replace)\n",
    "\n",
    "        for other in [s for s in SOURCES if s != model]:\n",
    "            result = {\"key\": key, \"model\": other}\n",
    "            other_summary = responses[other][key]\n",
    "\n",
    "            # Detection\n",
    "            forward_result = get_model_choice(\n",
    "                source_summary,\n",
    "                other_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "            backward_result = get_model_choice(\n",
    "                other_summary,\n",
    "                source_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "\n",
    "            forward_choice = forward_result[0].token\n",
    "            backward_choice = backward_result[0].token\n",
    "\n",
    "            result[\"forward_detection\"] = forward_choice\n",
    "            result[\"forward_detection_probability\"] = exp(forward_result[0].logprob)\n",
    "            result[\"backward_detection\"] = backward_choice\n",
    "            result[\"backward_detection_probability\"] = exp(backward_result[0].logprob)\n",
    "\n",
    "            match (forward_choice, backward_choice):\n",
    "                case (\"1\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"1\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "\n",
    "            results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'claude', 'gpt35', 'gpt4', 'llama']\n",
      "Starting gpt4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 450/450 [1:59:31<00:00, 15.94s/it]  \n"
     ]
    }
   ],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    print(SOURCES)\n",
    "    print(f\"Starting {model}\")\n",
    "    num_synonym = 2\n",
    "    results = generate_gpt_detect_recognition_synonym(\n",
    "        \"cnn\", model,replace_synonym=True, num_words_to_replace=num_synonym, starting_idx=500, ending_idx=950\n",
    "    )\n",
    "    #Save results\n",
    "    file_name = f\"{model}_results_{num_synonym}_replace_450_sentence.json\"\n",
    "    path = os.path.join(\"results\",\"cnn\",\"synonym\",file_name)\n",
    "    save_to_json(results,path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply synonym to 'other' example as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only suitable for GPT models\n",
    "def generate_gpt_detect_recognition_dual_synonym(\n",
    "    dataset,\n",
    "    model,\n",
    "    starting_idx=0,\n",
    "    detection_type=\"detection\",\n",
    "    replace_synonym = False,\n",
    "    num_words_to_replace = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates detection scores for GPT model outputs compared to other summaries using a dual synyonym replacement strategy.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): The name of the dataset (e.g., \"cnn\") containing the articles.\n",
    "        model (str): The base model on which inference will be performed.\n",
    "        starting_idx (int, optional): The index to start processing articles from. Defaults to 0.\n",
    "        detection_type (str, optional): The type of detection to perform. Defaults to \"detection\".\n",
    "        replace_synonym (bool, optional): Whether to replace words in the summaries with synonyms. Defaults to False.\n",
    "        num_words_to_replace (int, optional): The number of words to replace with synonyms if `replace_synonym` is True. Defaults to 0. Replaces the same number in both sentences being compared.\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # For retrieving summaries, the specific fine-tuning version isn't needed\n",
    "    exact_model = model\n",
    "    model = \"gpt35\" if model.endswith(\"gpt35\") else model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "    results = []  # load_from_json(f\"results/{model}_results.json\")\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:], desc=\"Processing keys\"):\n",
    "        article = articles[key]\n",
    "\n",
    "        source_summary = responses[model][key]\n",
    "\n",
    "        # replace synonym\n",
    "        if replace_synonym:\n",
    "            source_summary = replace_with_synonyms(source_summary, num_words_to_replace)\n",
    "\n",
    "        for other in [s for s in SOURCES if s != model]:\n",
    "            result = {\"key\": key, \"model\": other}\n",
    "            other_summary = responses[other][key]\n",
    "\n",
    "            # replace synonym\n",
    "            if replace_synonym:\n",
    "                other_summary = replace_with_synonyms(other_summary, num_words_to_replace)\n",
    "\n",
    "            # Detection\n",
    "            forward_result = get_model_choice(\n",
    "                source_summary,\n",
    "                other_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "            backward_result = get_model_choice(\n",
    "                other_summary,\n",
    "                source_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "\n",
    "            forward_choice = forward_result[0].token\n",
    "            backward_choice = backward_result[0].token\n",
    "\n",
    "            result[\"forward_detection\"] = forward_choice\n",
    "            result[\"forward_detection_probability\"] = exp(forward_result[0].logprob)\n",
    "            result[\"backward_detection\"] = backward_choice\n",
    "            result[\"backward_detection_probability\"] = exp(backward_result[0].logprob)\n",
    "\n",
    "            match (forward_choice, backward_choice):\n",
    "                case (\"1\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"1\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "\n",
    "            results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'claude', 'gpt35', 'gpt4', 'llama']\n",
      "Starting gpt4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 50/50 [13:51<00:00, 16.63s/it]\n"
     ]
    }
   ],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    print(SOURCES)\n",
    "    print(f\"Starting {model}\")\n",
    "    num_synonym = 5\n",
    "    results = generate_gpt_detect_recognition_dual_synonym(\n",
    "        \"cnn\", model,replace_synonym=True, num_words_to_replace=num_synonym, starting_idx=950\n",
    "    )\n",
    "    #Save results\n",
    "    file_name = f\"{model}_results_{num_synonym}_replace_bothsentences_50_sentence.json\"\n",
    "    path = os.path.join(\"results\",\"cnn\",\"synonym\",file_name)\n",
    "    save_to_json(results,path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding semantically similar indexes in the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only suitable for GPT models\n",
    "def get_similar_sentence_index(\n",
    "    dataset,\n",
    "    model,\n",
    "    starting_idx=0,\n",
    "):\n",
    "    exact_model = model\n",
    "    model = \"gpt35\" if model.endswith(\"gpt35\") else model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "    results = []  # load_from_json(f\"results/{model}_results.json\")\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:], desc=\"Processing keys\"):\n",
    "        summaries = []\n",
    "        result = {\"key\": key}\n",
    "        for other in [s for s in SOURCES]:\n",
    "            summaries.append(responses[other][key])\n",
    "            \n",
    "        result_json = get_gpt_summary_similarity(summaries[0],summaries[1],summaries[2],summaries[3],summaries[4], index=True)\n",
    "        result[\"indexes\"]= result_json \n",
    "        results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    print(SOURCES)\n",
    "    num_synonym = 2\n",
    "    results = get_similar_sentence_index(\"cnn\", model, starting_idx=998)\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'key': '9177e5ac94f038749e8d4eb526a65461e0f6df4c',\n",
       "  'indexes': '[\"Summary1:0\", \"Summary2:0\", \"Summary3:0\", \"Summary4:0\", \"Summary5:1\"]'},\n",
       " {'key': 'f12e4bbb07211de7d43b4e331dc73404aa804562',\n",
       "  'indexes': '[\"Summary1:2\", \"Summary2:1\", \"Summary3:2\", \"Summary4:1\", \"Summary5:0\"]'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [{'key': '9177e5ac94f038749e8d4eb526a65461e0f6df4c',\n",
    "  'indexes': '[\"Summary1:0\", \"Summary2:0\", \"Summary3:0\", \"Summary4:0\", \"Summary5:1\"]'},\n",
    " {'key': 'f12e4bbb07211de7d43b4e331dc73404aa804562',\n",
    "  'indexes': '[\"Summary1:2\", \"Summary2:1\", \"Summary3:2\", \"Summary4:1\", \"Summary5:0\"]'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize what is similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract a record by key\n",
    "def get_record_by_key(results, search_key):\n",
    "    for record in results:\n",
    "        if record['key'] == search_key:\n",
    "            return record\n",
    "    return None  # Return None if key is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract indexes from the record\n",
    "def extract_indexes(record):\n",
    "    if record and 'indexes' in record:\n",
    "        indexes_str = record['indexes']\n",
    "        # Convert the indexes string to a list using json.loads\n",
    "        indexes_list = json.loads(indexes_str)\n",
    "        return indexes_list\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only suitable for GPT models\n",
    "def visualize_similar(\n",
    "    dataset,\n",
    "    model,\n",
    "    starting_idx=0,\n",
    "):\n",
    "    exact_model = model\n",
    "    model = \"gpt35\" if model.endswith(\"gpt35\") else model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:], desc=\"Processing keys\"):\n",
    "        summaries = []\n",
    "        result = get_record_by_key(results,key)\n",
    "        count = 1\n",
    "        for idx, other in enumerate([s for s in SOURCES]):\n",
    "            print(other)\n",
    "            summary = responses[other][key]\n",
    "            summary = summary.split('\\n')\n",
    "            indexes = extract_indexes(result)\n",
    "            item = indexes[idx]\n",
    "            summary_value = int(item.split(\":\")[1])\n",
    "            summary = summary[summary_value]\n",
    "            print(summary)\n",
    "            \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'claude', 'gpt35', 'gpt4', 'llama']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 2/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human\n",
      "Judge on Heather Mills: Level of premarital wealth \"exaggerated\"\n",
      "claude\n",
      "Judge rejects Mills' claim that she was wealthy before meeting McCartney in 1999\n",
      "gpt35\n",
      "Judge rejects Heather Mills' claim of wealth before marriage to Paul McCartney\n",
      "gpt4\n",
      "Judge finds Heather Mills' claims of wealth in 1999 exaggerated and rejects her portrayal as Paul McCartney's business partner\n",
      "llama\n",
      "Judge finds Mills' wealth exaggerated and her living style unrealistic\n",
      "human\n",
      "President Taylor's daughter married future president of an enemy power\n",
      "claude\n",
      "Elizabeth Harrison Walker, daughter of President Benjamin Harrison, was an economic expert who appeared on radio and TV shows\n",
      "gpt35\n",
      "Elizabeth Harrison Walker: Accomplished woman, lawyer, economist, and media personality\n",
      "gpt4\n",
      "Elizabeth Harrison Walker, daughter of President Benjamin Harrison, became a lawyer and economic expert, appearing on radio and TV\n",
      "llama\n",
      "Sarah Knox Taylor Davis - Died at 21 after falling ill with malaria while visiting her husband's relatives in Louisiana\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    print(SOURCES)\n",
    "    visualize_similar(\"cnn\", model, starting_idx=998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'claude', 'gpt35', 'gpt4', 'llama']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 2/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human\n",
      "Judge on Heather Mills: Level of premarital wealth \"exaggerated\"\n",
      "McCartney gave Mills several hundred thousand dollars each year\n",
      "Judge: Mills' case \"boils down to ... 'if he has it, I want it too' \"\n",
      "claude\n",
      "Judge rejects Mills' claim that she was wealthy before meeting McCartney in 1999\n",
      "Judge finds McCartney's account more credible regarding Mills' role in his life\n",
      "Judge rules the 30 paintings in their home were lent by McCartney, not gifts to Mills\n",
      "Judge believes Mills has future earning capacity despite her claim it is now zero\n",
      "gpt35\n",
      "Judge rejects Heather Mills' claim of wealth before marriage to Paul McCartney\n",
      "Mills' portrayal as McCartney's business partner is deemed \"make-belief\" by judge\n",
      "McCartney's total wealth estimated at approximately £400 million ($800 million)\n",
      "gpt4\n",
      "Judge finds Heather Mills' claims of wealth in 1999 exaggerated and rejects her portrayal as Paul McCartney's business partner\n",
      "McCartney was generous, giving Mills substantial capital and gifts, but did not give her 30 valuable paintings\n",
      "McCartney's fortune estimated at £400 million, not the £800 million claimed by Mills; her expectation to live like McCartney deemed unrealistic\n",
      "Mills' claim of zero earning capacity dismissed; judge believes her career was advanced, not hindered by McCartney\n",
      "llama\n",
      "Paul McCartney's ex-wife Heather Mills' case against him for financial support is rejected\n",
      "Judge finds Mills' wealth exaggerated and her living style unrealistic\n",
      "McCartney's fortune estimated at £400 million ($800 million)\n",
      "Mills' earning capacity not zero, despite adverse publicity\n",
      "human\n",
      "President Harding's illegitimate daughter was conceived on couch in Senate office\n",
      "Review of Harry Truman's daughter prompted presidential threat against reporter\n",
      "President Taylor's daughter married future president of an enemy power\n",
      "Woodrow Wilson's daughter followed a guru to India\n",
      "claude\n",
      "Sarah Knox Taylor Davis married future Confederate President Jefferson Davis, but died of malaria after 3 months of marriage\n",
      "Elizabeth Harrison Walker, daughter of President Benjamin Harrison, was an economic expert who appeared on radio and TV shows\n",
      "Margaret Woodrow Wilson, Woodrow Wilson's daughter, was a singer who had a mental breakdown during WW1 and later followed a guru in India\n",
      "Elizabeth Ann Christian Blaes\n",
      "gpt35\n",
      "Presidential daughters' fascinating stories under their influential fathers\n",
      "Sarah Knox Taylor Davis: Tragic life, married Confederate President Jefferson Davis\n",
      "Elizabeth Harrison Walker: Accomplished woman, lawyer, economist, and media personality\n",
      "Margaret Woodrow Wilson: Pursued music, advocate of local participatory democracy, and follower of Sri Aurobindo\n",
      "gpt4\n",
      "Sarah Knox Taylor Davis, daughter of President Zachary Taylor, died of malaria three months after marrying Jefferson Davis\n",
      "Elizabeth Harrison Walker, daughter of President Benjamin Harrison, became a lawyer and economic expert, appearing on radio and TV\n",
      "Margaret Woodrow Wilson, daughter of President Woodrow Wilson, advocated for democracy and lived in an Indian ashram until her death\n",
      "Elizabeth Ann Christian Blaesing, daughter of President Warren G. Harding, lived a quiet life despite the scandal of her birth\n",
      "llama\n",
      "Sarah Knox Taylor Davis - Died at 21 after falling ill with malaria while visiting her husband's relatives in Louisiana\n",
      "Elizabeth Harrison Walker - Married James Blaine Walker and pursued a career in law and journalism, publishing a monthly newsletter and appearing on radio and television\n",
      "Margaret Woodrow Wilson - Became a singing sensation after her father's presidency, performing in front of Allied troops in France during World War I\n",
      "Elizabeth Ann Christian Blaesing - Born to Warren G. Harding's mistress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# What are the splits for each sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonym - return full sentence via GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only suitable for GPT models\n",
    "def get_similar_sentence(\n",
    "    dataset,\n",
    "    model,\n",
    "    starting_idx=0,\n",
    "):\n",
    "    exact_model = model\n",
    "    model = \"gpt35\" if model.endswith(\"gpt35\") else model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "    results = []  \n",
    "\n",
    "    for key in tqdm(keys[starting_idx:], desc=\"Processing keys\"):\n",
    "        summaries = []\n",
    "        result = {\"key\": key}\n",
    "        for other in [s for s in SOURCES]:\n",
    "            summaries.append(responses[other][key])\n",
    "            \n",
    "        result_json = get_gpt_summary_similarity(summaries[0],summaries[1],summaries[2],summaries[3],summaries[4])\n",
    "        result[\"sentences\"]= result_json \n",
    "        results.append(result)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'claude', 'gpt35', 'gpt4', 'llama']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 3/3 [00:07<00:00,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'key': '5f02aa32bd1dc95e47355755398e31550b232f8a', 'sentences': '[\"Eight Florida teens to be tried as adults in videotaped beating case\", \"Eight Florida teenagers face kidnapping and battery charges for videotaped group beating of a 16-year-old girl.\", \"Eight Florida teens face life in prison for assaulting another teen\", \"Eight Florida teens charged as adults for videotaped beating of another teen, facing life in prison\", \"Eight Florida teens aged 14-18 will be tried as adults for beating another teenager in a viral video\"]'}, {'key': '9177e5ac94f038749e8d4eb526a65461e0f6df4c', 'sentences': '[\"Judge on Heather Mills: Level of premarital wealth\", \"Judge rejects Mills\\' claim that she was wealthy\", \"Judge rejects Heather Mills\\' claim of wealth before\", \"Judge finds Heather Mills\\' claims of wealth in\", \"Judge finds Mills\\' wealth exaggerated and her living\"]'}, {'key': 'f12e4bbb07211de7d43b4e331dc73404aa804562', 'sentences': '[\"President Taylor\\'s daughter married future\", \"Sarah Knox Taylor Davis married future\", \"Sarah Knox Taylor Davis: Tragic life,\", \"Sarah Knox Taylor Davis, daughter of\", \"Sarah Knox Taylor Davis - Died\"]'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    print(SOURCES)\n",
    "    results = get_similar_sentence(\"cnn\", model, starting_idx=997)\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"Eight Florida teens to be tried as adults in videotaped beating case\", \"Eight Florida teenagers face kidnapping and battery charges for videotaped group beating of a 16-year-old girl.\", \"Eight Florida teens face life in prison for assaulting another teen\", \"Eight Florida teens charged as adults for videotaped beating of another teen, facing life in prison\", \"Eight Florida teens aged 14-18 will be tried as adults for beating another teenager in a viral video\"]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "results[1]['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"President Taylor's daughter married future\", 'Sarah Knox Taylor Davis married future', 'Sarah Knox Taylor Davis: Tragic life,', 'Sarah Knox Taylor Davis, daughter of', 'Sarah Knox Taylor Davis - Died']\n"
     ]
    }
   ],
   "source": [
    "# Example string\n",
    "input_string = results[2]['sentences'] \n",
    "\n",
    "# # Use regular expressions to find all text between double quotes\n",
    "# extracted_list = re.findall(r'\"(.*?)\"', input_string)\n",
    "\n",
    "extracted_list = json.loads(input_string)\n",
    "\n",
    "# Print the result\n",
    "print(extracted_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Judge on Heather Mills: Level of premarital wealth \"exaggerated\"'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is similar?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract a record by key\n",
    "def get_record_by_key(results, search_key):\n",
    "    for record in results:\n",
    "        if record['key'] == search_key:\n",
    "            return record\n",
    "    return None  # Return None if key is not found\n",
    "\n",
    "def find_most_similar_sentence(sentences, reference_sentence):\n",
    "    # Combine all sentences\n",
    "    all_sentences = sentences + [reference_sentence]\n",
    "    \n",
    "    # Initialize the TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer().fit_transform(all_sentences)\n",
    "    \n",
    "    # Compute cosine similarity between the reference and all other sentences\n",
    "    similarity_matrix = cosine_similarity(vectorizer[-1], vectorizer[:-1])\n",
    "    \n",
    "    # Get the index of the most similar sentence\n",
    "    most_similar_index = similarity_matrix.argsort()[0][-1]\n",
    "    \n",
    "    return sentences[most_similar_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only suitable for GPT models\n",
    "def visualize_similar(\n",
    "    dataset,\n",
    "    model,\n",
    "    results,\n",
    "    starting_idx=0\n",
    "):\n",
    "    exact_model = model\n",
    "    model = \"gpt35\" if model.endswith(\"gpt35\") else model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:], desc=\"Processing keys\"):\n",
    "        summaries = []\n",
    "        result = get_record_by_key(results,key)\n",
    "        input_string = result['sentences']\n",
    "        extracted_list = json.loads(input_string)\n",
    "\n",
    "        for idx, other in enumerate([s for s in SOURCES]):\n",
    "            print(other)\n",
    "            summary = responses[other][key]\n",
    "            summary = summary.split('\\n')\n",
    "           \n",
    "            reference_sentence = extracted_list[idx]\n",
    "            print('ref', reference_sentence)\n",
    "            similar_in_key = find_most_similar_sentence(summary, reference_sentence)\n",
    "            print(similar_in_key)\n",
    "            \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'claude', 'gpt35', 'gpt4', 'llama']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 3/3 [00:00<00:00, 221.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human\n",
      "ref Eight Florida teens to be tried as adults in videotaped beating case\n",
      "Eight Florida teens to be tried as adults in videotaped beating case\n",
      "claude\n",
      "ref Eight Florida teenagers face kidnapping and battery charges for videotaped group beating of a 16-year-old girl.\n",
      "Eight Florida teenagers face kidnapping and battery charges for videotaped group beating of a 16-year-old girl. They could be sentenced to life in prison if convicted\n",
      "gpt35\n",
      "ref Eight Florida teens face life in prison for assaulting another teen\n",
      "Eight Florida teens face life in prison for assaulting another teen\n",
      "gpt4\n",
      "ref Eight Florida teens charged as adults for videotaped beating of another teen, facing life in prison\n",
      "Eight Florida teens charged as adults for videotaped beating of another teen, facing life in prison\n",
      "llama\n",
      "ref Eight Florida teens aged 14-18 will be tried as adults for beating another teenager in a viral video\n",
      "Eight Florida teens aged 14-18 will be tried as adults for beating another teenager in a viral video\n",
      "human\n",
      "ref Judge on Heather Mills: Level of premarital wealth\n",
      "Judge on Heather Mills: Level of premarital wealth \"exaggerated\"\n",
      "claude\n",
      "ref Judge rejects Mills' claim that she was wealthy\n",
      "Judge rejects Mills' claim that she was wealthy before meeting McCartney in 1999\n",
      "gpt35\n",
      "ref Judge rejects Heather Mills' claim of wealth before\n",
      "Judge rejects Heather Mills' claim of wealth before marriage to Paul McCartney\n",
      "gpt4\n",
      "ref Judge finds Heather Mills' claims of wealth in\n",
      "Judge finds Heather Mills' claims of wealth in 1999 exaggerated and rejects her portrayal as Paul McCartney's business partner\n",
      "llama\n",
      "ref Judge finds Mills' wealth exaggerated and her living\n",
      "Judge finds Mills' wealth exaggerated and her living style unrealistic\n",
      "human\n",
      "ref President Taylor's daughter married future\n",
      "President Taylor's daughter married future president of an enemy power\n",
      "claude\n",
      "ref Sarah Knox Taylor Davis married future\n",
      "Sarah Knox Taylor Davis married future Confederate President Jefferson Davis, but died of malaria after 3 months of marriage\n",
      "gpt35\n",
      "ref Sarah Knox Taylor Davis: Tragic life,\n",
      "Sarah Knox Taylor Davis: Tragic life, married Confederate President Jefferson Davis\n",
      "gpt4\n",
      "ref Sarah Knox Taylor Davis, daughter of\n",
      "Sarah Knox Taylor Davis, daughter of President Zachary Taylor, died of malaria three months after marrying Jefferson Davis\n",
      "llama\n",
      "ref Sarah Knox Taylor Davis - Died\n",
      "Sarah Knox Taylor Davis - Died at 21 after falling ill with malaria while visiting her husband's relatives in Louisiana\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    print(SOURCES)\n",
    "    visualize_similar(\"cnn\", model, results, starting_idx=997)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase - Using Sentence Transformer and Replacing using GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec5518a409344c58163eb8d842521fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\G34371231\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f69f91af06848a4ba0b5e6127e3b283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97df72dac804405f9e4b223477a27446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065c7d172e7244229ddf3c1e509985eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2988cba1254f8d94eb503e367eb5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b3a4f278294aafbc7d2fead5f54046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571d7a3eadda4768b38b6ac4542c3c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc692c303a3c49c7a44baf0b3afd86cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a852099ba3fb48408cd0d95e67604d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01459b5da32f4c17bc357924c840ec58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a4814e76b548f6b1e0602627a03b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a pre-trained Sentence-BERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to find semantically similar sentences across multiple sources\n",
    "def find_similar_sentence_from_each_source(sources):\n",
    "    # Create an empty list to hold all candidate sentences and keep track of their source\n",
    "    all_sentences = []\n",
    "    sentence_source_mapping = []\n",
    "\n",
    "    for idx, source in enumerate(sources):\n",
    "        all_sentences.extend(source)\n",
    "        sentence_source_mapping.extend([idx] * len(source))\n",
    "    \n",
    "    # Generate embeddings for all sentences\n",
    "    embeddings = model.encode(all_sentences, convert_to_tensor=True)\n",
    "\n",
    "    # Calculate pairwise cosine similarity\n",
    "    similarity_matrix = cosine_similarity(embeddings.cpu().numpy())\n",
    "\n",
    "    # Find the maximum similarity while ensuring at least one sentence from each source\n",
    "    best_sentences = []\n",
    "    used_sources = set()\n",
    "    \n",
    "    # Iterate over all sentences and try to find one from each source\n",
    "    for idx, sentence in enumerate(all_sentences):\n",
    "        current_source = sentence_source_mapping[idx]\n",
    "        if current_source not in used_sources:\n",
    "            # Find the most similar sentence from this source to any other sentence from other sources\n",
    "            best_match_score = -1\n",
    "            best_match_sentence = None\n",
    "\n",
    "            for jdx in range(len(all_sentences)):\n",
    "                if idx != jdx and sentence_source_mapping[jdx] != current_source:\n",
    "                    similarity_score = similarity_matrix[idx][jdx]\n",
    "                    if similarity_score > best_match_score:\n",
    "                        best_match_score = similarity_score\n",
    "                        best_match_sentence = sentence\n",
    "            \n",
    "            if best_match_sentence:\n",
    "                best_sentences.append(best_match_sentence)\n",
    "                used_sources.add(current_source)\n",
    "\n",
    "            if len(used_sources) == len(sources):\n",
    "                break\n",
    "\n",
    "    return best_sentences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_sentence_transformer(\n",
    "    dataset,\n",
    "    starting_idx=0,\n",
    "):\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "    result = {}\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:], desc=\"Processing keys\"):\n",
    "        summaries = []\n",
    "        result[key]= {}\n",
    "        for idx, other in enumerate([s for s in SOURCES]):\n",
    "            summary = responses[other][key]\n",
    "            summary = summary.split('\\n')\n",
    "            summaries.append(summary)\n",
    "\n",
    "        similar_sentences = find_similar_sentence_from_each_source(summaries)\n",
    "        for idx, other in enumerate([s for s in SOURCES]):\n",
    "            result[key][other] = similar_sentences[idx]\n",
    "\n",
    "            \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 500/500 [00:18<00:00, 26.69it/s]\n"
     ]
    }
   ],
   "source": [
    "similar_sentences_cnn = get_similar_sentence_transformer(\"cnn\", starting_idx=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': 'The two-hour interview takes place in the Netherlands',\n",
       " 'claude': \"Aruban authorities questioned Joran van der Sloot about Natalee Holloway's disappearance\",\n",
       " 'gpt35': \"Joran van der Sloot questioned in Netherlands about Natalee Holloway's disappearance\",\n",
       " 'gpt4': \"Joran van der Sloot was questioned in the Netherlands about Natalee Holloway's disappearance\",\n",
       " 'llama': \"Aruban authorities questioned Joran van der Sloot in the Netherlands about Natalee Holloway's disappearance\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_sentences_cnn['ffb817ce85d7c19720ebbf0b43b01d0da61e9c06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_gpt_paraphraser(\n",
    "    dataset,\n",
    "    starting_idx=0,\n",
    "):\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:], desc=\"Processing keys\"):\n",
    "        for idx, other in enumerate([s for s in SOURCES]):\n",
    "            summary = responses[other][key]\n",
    "            sentence_to_paraphrase = similar_sentences_cnn[key][other]\n",
    "            alternate = get_gpt_paraphrase(sentence_to_paraphrase)\n",
    "            summary = summary.replace(sentence_to_paraphrase, alternate)\n",
    "            responses[other][key] = summary\n",
    "\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_claude_paraphraser(\n",
    "    dataset,\n",
    "    starting_idx=0,\n",
    "):\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:], desc=\"Processing keys\"):\n",
    "        for idx, other in enumerate([s for s in SOURCES]):\n",
    "            summary = responses[other][key]\n",
    "            sentence_to_paraphrase = similar_sentences_cnn[key][other]\n",
    "            alternate = get_claude_paraphrase(sentence_to_paraphrase)\n",
    "            summary = summary.replace(sentence_to_paraphrase, alternate)\n",
    "            responses[other][key] = summary\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 500/500 [52:34<00:00,  6.31s/it]   \n"
     ]
    }
   ],
   "source": [
    "modified_responses = replace_gpt_paraphraser(\"cnn\", 500)\n",
    "file_name = \"cnn_gpt4_paraphrased_responses_1.json\"\n",
    "path = os.path.join(\"summaries\",\"cnn\",file_name)\n",
    "save_to_json(modified_responses,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Court on Heather Mills: Extent of wealth before marriage \"overstated\"\\nMcCartney gave Mills several hundred thousand dollars each year\\nJudge: Mills\\' case \"boils down to ... \\'if he has it, I want it too\\' \"'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_responses['human']['9177e5ac94f038749e8d4eb526a65461e0f6df4c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'responses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m responses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m9177e5ac94f038749e8d4eb526a65461e0f6df4c\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'responses' is not defined"
     ]
    }
   ],
   "source": [
    "responses['human']['9177e5ac94f038749e8d4eb526a65461e0f6df4c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 500/500 [1:04:28<00:00,  7.74s/it]   \n"
     ]
    }
   ],
   "source": [
    "responses_modified_by_claude = replace_claude_paraphraser(\"cnn\", 500)\n",
    "file_name = \"cnn_claude3-5_paraphrased_responses.json\"\n",
    "path = os.path.join(\"summaries\",\"cnn\",file_name)\n",
    "save_to_json(responses_modified_by_claude,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The court determined that Heather Mills had overstated her financial status before marriage.\\nMcCartney gave Mills several hundred thousand dollars each year\\nJudge: Mills\\' case \"boils down to ... \\'if he has it, I want it too\\' \"'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_modified_by_claude['human']['9177e5ac94f038749e8d4eb526a65461e0f6df4c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'responses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m responses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m9177e5ac94f038749e8d4eb526a65461e0f6df4c\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'responses' is not defined"
     ]
    }
   ],
   "source": [
    "responses['human']['9177e5ac94f038749e8d4eb526a65461e0f6df4c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Self Recognition Results on modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_modified_results():\n",
    "    file_name = \"cnn_claude3-5_paraphrased_responses.json\"\n",
    "    path = os.path.join(\"summaries\",\"cnn\",file_name)\n",
    "    loaded = load_from_json(path)\n",
    "    return loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt_detect_recognition_paraphrase(\n",
    "    dataset,\n",
    "    model,\n",
    "    starting_idx=0,\n",
    "    ending_idx=1000,\n",
    "    detection_type=\"detection\",\n",
    "    paraphrase_source=False,\n",
    "    paraphrase_other=False\n",
    "):\n",
    "    # For retrieving summaries, the specific fine-tuning version isn't needed\n",
    "    exact_model = model\n",
    "    model = \"gpt35\" if model.endswith(\"gpt35\") else model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "    modified_responses = load_modified_results()\n",
    "    results = []  # load_from_json(f\"results/{model}_results.json\")\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:ending_idx], desc=\"Processing keys\"):\n",
    "        article = articles[key]\n",
    "\n",
    "        source_summary = responses[model][key]\n",
    "\n",
    "        # replace synonym\n",
    "        if paraphrase_source:\n",
    "            source_summary = modified_responses[model][key]\n",
    "\n",
    "        for other in [s for s in SOURCES if s != model]:\n",
    "            result = {\"key\": key, \"model\": other}\n",
    "            other_summary = responses[other][key]\n",
    "            if paraphrase_other:\n",
    "                other_summary = modified_responses[other][key]\n",
    "            # Detection\n",
    "            try:\n",
    "                forward_result = get_model_choice(\n",
    "                    source_summary,\n",
    "                    other_summary,\n",
    "                    article,\n",
    "                    detection_type,\n",
    "                    exact_model,\n",
    "                    return_logprobs=True,\n",
    "                )\n",
    "                backward_result = get_model_choice(\n",
    "                    other_summary,\n",
    "                    source_summary,\n",
    "                    article,\n",
    "                    detection_type,\n",
    "                    exact_model,\n",
    "                    return_logprobs=True,\n",
    "                )\n",
    "\n",
    "                forward_choice = forward_result[0].token\n",
    "                backward_choice = backward_result[0].token\n",
    "\n",
    "                result[\"forward_detection\"] = forward_choice\n",
    "                result[\"forward_detection_probability\"] = exp(forward_result[0].logprob)\n",
    "                result[\"backward_detection\"] = backward_choice\n",
    "                result[\"backward_detection_probability\"] = exp(backward_result[0].logprob)\n",
    "\n",
    "                match (forward_choice, backward_choice):\n",
    "                    case (\"1\", \"2\"):\n",
    "                        result[\"detection_score\"] = 0.5 * (\n",
    "                            exp(forward_result[0].logprob) + exp(backward_result[0].logprob)\n",
    "                        )\n",
    "                    case (\"2\", \"1\"):\n",
    "                        result[\"detection_score\"] = 0.5 * (\n",
    "                            exp(forward_result[1].logprob) + exp(backward_result[1].logprob)\n",
    "                        )\n",
    "                    case (\"1\", \"1\"):\n",
    "                        result[\"detection_score\"] = 0.5 * (\n",
    "                            exp(forward_result[0].logprob) + exp(backward_result[1].logprob)\n",
    "                        )\n",
    "                    case (\"2\", \"2\"):\n",
    "                        result[\"detection_score\"] = 0.5 * (\n",
    "                            exp(forward_result[1].logprob) + exp(backward_result[0].logprob)\n",
    "                        )\n",
    "            except ValueError:\n",
    "                print('Error:', key, other)\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gpt4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 450/450 [43:14<00:00,  5.77s/it]   \n"
     ]
    }
   ],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    #print(SOURCES)\n",
    "    print(f\"Starting {model}\")\n",
    "    results = generate_gpt_detect_recognition_paraphrase(\n",
    "        \"cnn\", model, starting_idx=500, ending_idx=950, paraphrase_source=True, paraphrase_other=True\n",
    "    )\n",
    "    #Save results\n",
    "    file_name = f\"{model}_results_claude_paraphrased_450.json\"\n",
    "    path = os.path.join(\"results\",\"cnn\",\"synonym\",file_name)\n",
    "    save_to_json(results,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gpt4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 450/450 [35:44<00:00,  4.77s/it]\n"
     ]
    }
   ],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    print(f\"Starting {model}\")\n",
    "    results = generate_gpt_detect_recognition_paraphrase(\n",
    "        \"cnn\", model, starting_idx=500, ending_idx=950, paraphrase_source=True, paraphrase_other=False\n",
    "    )\n",
    "    #Save results\n",
    "    file_name = f\"{model}_results_claude_paraphrased_source_only_450.json\"\n",
    "    path = os.path.join(\"results\",\"cnn\",\"synonym\",file_name)\n",
    "    save_to_json(results,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gpt4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 450/450 [39:52<00:00,  5.32s/it]  \n"
     ]
    }
   ],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    print(f\"Starting {model}\")\n",
    "    results = generate_gpt_detect_recognition_paraphrase(\n",
    "        \"cnn\", model,  starting_idx=500, ending_idx=950, paraphrase_source=False, paraphrase_other=True\n",
    "    )\n",
    "    #Save results\n",
    "    file_name = f\"{model}_results_claude_paraphrased_other_only_450.json\"\n",
    "    path = os.path.join(\"results\",\"cnn\",\"synonym\",file_name)\n",
    "    save_to_json(results,path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Self Preference Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_modified_results():\n",
    "    file_name = \"cnn_gpt4_paraphrased_responses.json\"\n",
    "    path = os.path.join(\"summaries\",\"cnn\",file_name)\n",
    "    loaded = load_from_json(path)\n",
    "    return loaded\n",
    "\n",
    "# Only suitable for GPT models\n",
    "def generate_gpt_preference_paraphrase(\n",
    "    dataset,\n",
    "    model,\n",
    "    starting_idx=0,\n",
    "    ending_idx=1000,\n",
    "    detection_type=\"detection\",\n",
    "    comparison_type=\"comparison\",\n",
    "    paraphrase_source=False,\n",
    "    paraphrase_other=False\n",
    "    \n",
    "):\n",
    "    # For retrieving summaries, the specific fine-tuning version isn't needed\n",
    "    exact_model = model\n",
    "    model = \"gpt35\" if model.endswith(\"gpt35\") else model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "    modified_responses = load_modified_results()\n",
    "    results = []  # load_from_json(f\"results/{model}_results.json\")\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:], desc=\"Processing keys\"):\n",
    "        article = articles[key]\n",
    "\n",
    "        # replace synonym\n",
    "        if paraphrase_source:\n",
    "            source_summary = modified_responses[model][key]\n",
    "\n",
    "        for other in [s for s in SOURCES if s != model]:\n",
    "            result = {\"key\": key, \"model\": other}\n",
    "            other_summary = responses[other][key]\n",
    "\n",
    "            # Detection\n",
    "            forward_result = get_model_choice(\n",
    "                source_summary,\n",
    "                other_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "            backward_result = get_model_choice(\n",
    "                other_summary,\n",
    "                source_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "\n",
    "            forward_choice = forward_result[0].token\n",
    "            backward_choice = backward_result[0].token\n",
    "\n",
    "            result[\"forward_detection\"] = forward_choice\n",
    "            result[\"forward_detection_probability\"] = exp(forward_result[0].logprob)\n",
    "            result[\"backward_detection\"] = backward_choice\n",
    "            result[\"backward_detection_probability\"] = exp(backward_result[0].logprob)\n",
    "\n",
    "            match (forward_choice, backward_choice):\n",
    "                case (\"1\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"1\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "\n",
    "            # Comparison\n",
    "            forward_result = get_model_choice(\n",
    "                source_summary,\n",
    "                other_summary,\n",
    "                article,\n",
    "                comparison_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "            backward_result = get_model_choice(\n",
    "                other_summary,\n",
    "                source_summary,\n",
    "                article,\n",
    "                comparison_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "\n",
    "            forward_choice = forward_result[0].token\n",
    "            backward_choice = backward_result[0].token\n",
    "\n",
    "            # If the comparison asked \"Which is worse?\" then reverse the options\n",
    "            if comparison_type == \"comparison_with_worse\":\n",
    "                forward_choice = \"1\" if forward_choice == \"2\" else \"2\"\n",
    "                backward_choice = \"1\" if backward_choice == \"2\" else \"2\"\n",
    "\n",
    "            result[\"forward_comparison\"] = forward_choice\n",
    "            result[\"forward_comparison_probability\"] = exp(forward_result[0].logprob)\n",
    "            result[\"backward_comparison\"] = backward_choice\n",
    "            result[\"backward_comparison_probability\"] = exp(backward_result[0].logprob)\n",
    "\n",
    "            match (forward_choice, backward_choice):\n",
    "                case (\"1\", \"2\"):\n",
    "                    result[\"self_preference\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"1\"):\n",
    "                    result[\"self_preference\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"1\", \"1\"):\n",
    "                    result[\"self_preference\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"2\"):\n",
    "                    result[\"self_preference\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "\n",
    "            results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gpt4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys:   2%|▏         | 8/500 [01:31<1:34:11, 11.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt4\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m#print(SOURCES)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     results \u001b[38;5;241m=\u001b[39m generate_gpt_preference_paraphrase(\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn\u001b[39m\u001b[38;5;124m\"\u001b[39m, model, starting_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, ending_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m950\u001b[39m, paraphrase_source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, paraphrase_other\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     )\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#Save results\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_preference_results_gpt_paraphrased_450.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[29], line 91\u001b[0m, in \u001b[0;36mgenerate_gpt_preference_paraphrase\u001b[1;34m(dataset, model, starting_idx, ending_idx, detection_type, comparison_type, paraphrase_source, paraphrase_other)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Comparison\u001b[39;00m\n\u001b[0;32m     83\u001b[0m forward_result \u001b[38;5;241m=\u001b[39m get_model_choice(\n\u001b[0;32m     84\u001b[0m     source_summary,\n\u001b[0;32m     85\u001b[0m     other_summary,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m     return_logprobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     90\u001b[0m )\n\u001b[1;32m---> 91\u001b[0m backward_result \u001b[38;5;241m=\u001b[39m get_model_choice(\n\u001b[0;32m     92\u001b[0m     other_summary,\n\u001b[0;32m     93\u001b[0m     source_summary,\n\u001b[0;32m     94\u001b[0m     article,\n\u001b[0;32m     95\u001b[0m     comparison_type,\n\u001b[0;32m     96\u001b[0m     exact_model,\n\u001b[0;32m     97\u001b[0m     return_logprobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m    100\u001b[0m forward_choice \u001b[38;5;241m=\u001b[39m forward_result[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtoken\n\u001b[0;32m    101\u001b[0m backward_choice \u001b[38;5;241m=\u001b[39m backward_result[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtoken\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\OneDrive - The George Washington University\\Desktop\\self_recognition\\models.py:280\u001b[0m, in \u001b[0;36mget_model_choice\u001b[1;34m(summary1, summary2, article, choice_type, model, return_logprobs)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_claude_choice(\n\u001b[0;32m    273\u001b[0m         summary1,\n\u001b[0;32m    274\u001b[0m         summary2,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    277\u001b[0m         return_logprobs\u001b[38;5;241m=\u001b[39mreturn_logprobs,\n\u001b[0;32m    278\u001b[0m     )\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt4\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_gpt_choice(\n\u001b[0;32m    281\u001b[0m         summary1,\n\u001b[0;32m    282\u001b[0m         summary2,\n\u001b[0;32m    283\u001b[0m         article,\n\u001b[0;32m    284\u001b[0m         choice_type,\n\u001b[0;32m    285\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4-1106-preview\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    286\u001b[0m         return_logprobs\u001b[38;5;241m=\u001b[39mreturn_logprobs,\n\u001b[0;32m    287\u001b[0m     )\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt35\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_gpt_choice(\n\u001b[0;32m    290\u001b[0m         summary1,\n\u001b[0;32m    291\u001b[0m         summary2,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    295\u001b[0m         return_logprobs\u001b[38;5;241m=\u001b[39mreturn_logprobs,\n\u001b[0;32m    296\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\OneDrive - The George Washington University\\Desktop\\self_recognition\\models.py:169\u001b[0m, in \u001b[0;36mget_gpt_choice\u001b[1;34m(summary1, summary2, article, choice_type, model, return_logprobs)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mmatch\u001b[39;00m choice_type:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomparison\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    139\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m COMPARISON_PROMPT_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m             summary1\u001b[38;5;241m=\u001b[39msummary1, summary2\u001b[38;5;241m=\u001b[39msummary2, article\u001b[38;5;241m=\u001b[39marticle\n\u001b[0;32m    162\u001b[0m         )\n\u001b[0;32m    164\u001b[0m history \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    165\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt},\n\u001b[0;32m    166\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt},\n\u001b[0;32m    167\u001b[0m ]\n\u001b[1;32m--> 169\u001b[0m response \u001b[38;5;241m=\u001b[39m openai_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    170\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    171\u001b[0m     messages\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[0;32m    172\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m    173\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    174\u001b[0m     logprobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    175\u001b[0m     top_logprobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    176\u001b[0m )\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_logprobs:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlogprobs\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtop_logprobs\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\openai\\_utils\\_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:742\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    739\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    740\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    741\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    744\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    745\u001b[0m             {\n\u001b[0;32m    746\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    747\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    748\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    749\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    750\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    751\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    752\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    753\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    754\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    755\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    756\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    757\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    758\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    759\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    760\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    761\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    762\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    763\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    764\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    765\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    766\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    767\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    768\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    769\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    770\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    771\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    772\u001b[0m             },\n\u001b[0;32m    773\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    774\u001b[0m         ),\n\u001b[0;32m    775\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    776\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    777\u001b[0m         ),\n\u001b[0;32m    778\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    779\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    780\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    781\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\openai\\_base_client.py:1270\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1258\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1265\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1267\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1268\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1269\u001b[0m     )\n\u001b[1;32m-> 1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\openai\\_base_client.py:947\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    945\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    948\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    949\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    950\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    951\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    952\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m    953\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\openai\\_base_client.py:983\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m    980\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 983\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    984\u001b[0m         request,\n\u001b[0;32m    985\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    987\u001b[0m     )\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    989\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[0;32m    910\u001b[0m )\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[0;32m    915\u001b[0m     request,\n\u001b[0;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[0;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m    919\u001b[0m )\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[0;32m    943\u001b[0m         request,\n\u001b[0;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\httpx\\_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1012\u001b[0m     )\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\httpx\\_transports\\default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    231\u001b[0m )\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    242\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_closed(status)\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool_lock:\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;66;03m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;66;03m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\httpcore\\_sync\\http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\httpcore\\_sync\\http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m    105\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    106\u001b[0m     (\n\u001b[0;32m    107\u001b[0m         http_version,\n\u001b[0;32m    108\u001b[0m         status,\n\u001b[0;32m    109\u001b[0m         reason_phrase,\n\u001b[0;32m    110\u001b[0m         headers,\n\u001b[1;32m--> 111\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    112\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    113\u001b[0m         http_version,\n\u001b[0;32m    114\u001b[0m         status,\n\u001b[0;32m    115\u001b[0m         reason_phrase,\n\u001b[0;32m    116\u001b[0m         headers,\n\u001b[0;32m    117\u001b[0m     )\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    120\u001b[0m     status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[0;32m    121\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m     },\n\u001b[0;32m    128\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\httpcore\\_sync\\http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    173\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\httpcore\\_sync\\http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 212\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    214\u001b[0m     )\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv(max_bytes)\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\ssl.py:1233\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1230\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1231\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1232\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(buflen)\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\ssl.py:1106\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    #print(SOURCES)\n",
    "    print(f\"Starting {model}\")\n",
    "    results = generate_gpt_preference_paraphrase(\n",
    "        \"cnn\", model, starting_idx=500, ending_idx=950, paraphrase_source=True, paraphrase_other=True\n",
    "    )\n",
    "    #Save results\n",
    "    file_name = f\"{model}_preference_results_gpt_paraphrased_450.json\"\n",
    "    path = os.path.join(\"results\",\"cnn\",\"synonym\",file_name)\n",
    "    save_to_json(results,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    print(f\"Starting {model}\")\n",
    "    results = generate_gpt_detect_recognition_paraphrase(\n",
    "        \"cnn\", model, starting_idx=500, ending_idx=950, paraphrase_source=True, paraphrase_other=False\n",
    "    )\n",
    "    #Save results\n",
    "    file_name = f\"{model}_preference_results_gpt_paraphrased_source_only_450.json\"\n",
    "    path = os.path.join(\"results\",\"cnn\",\"synonym\",file_name)\n",
    "    save_to_json(results,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    print(f\"Starting {model}\")\n",
    "    results = generate_gpt_detect_recognition_paraphrase(\n",
    "        \"cnn\", model,  starting_idx=500, ending_idx=950, paraphrase_source=False, paraphrase_other=True\n",
    "    )\n",
    "    #Save results\n",
    "    file_name = f\"{model}_preference_results_gpt_paraphrased_other_only_450.json\"\n",
    "    path = os.path.join(\"results\",\"cnn\",\"synonym\",file_name)\n",
    "    save_to_json(results,path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Percentage of Sentence Modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_modified_results():\n",
    "    file_name = \"cnn_gpt4_paraphrased_responses_1.json\"\n",
    "    path = os.path.join(\"summaries\",\"cnn\",file_name)\n",
    "    loaded = load_from_json(path)\n",
    "    return loaded\n",
    "\n",
    "###\n",
    "def generate_percentage_modifed(\n",
    "    dataset,\n",
    "    starting_idx=0,\n",
    "    ending_idx=1000\n",
    "):\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "    modified_responses = load_modified_results()\n",
    "    results = []  # load_from_json(f\"results/{model}_results.json\")\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:ending_idx], desc=\"Processing keys\"):\n",
    "        for idx, other in enumerate([s for s in SOURCES]):\n",
    "            source_summary = responses[other][key]\n",
    "            sentence_to_paraphrase = similar_sentences_cnn[key][other]\n",
    "            \n",
    "            main_length = len(source_summary)\n",
    "            substring_length = len(sentence_to_paraphrase)\n",
    "            # Calculate percentage\n",
    "            percentage = (substring_length / main_length) * 100\n",
    "            result = {\"key\": key, \"model\": other, \"paraphrase\":percentage}\n",
    "            results.append(result)\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'similar_sentences_cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m modified_percentages \u001b[38;5;241m=\u001b[39m generate_percentage_modifed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn\u001b[39m\u001b[38;5;124m\"\u001b[39m, starting_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 20\u001b[0m, in \u001b[0;36mgenerate_percentage_modifed\u001b[1;34m(dataset, starting_idx, ending_idx)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, other \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m([s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m SOURCES]):\n\u001b[0;32m     19\u001b[0m     source_summary \u001b[38;5;241m=\u001b[39m responses[other][key]\n\u001b[1;32m---> 20\u001b[0m     sentence_to_paraphrase \u001b[38;5;241m=\u001b[39m similar_sentences_cnn[key][other]\n\u001b[0;32m     22\u001b[0m     main_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(source_summary)\n\u001b[0;32m     23\u001b[0m     substring_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentence_to_paraphrase)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'similar_sentences_cnn' is not defined"
     ]
    }
   ],
   "source": [
    "modified_percentages = generate_percentage_modifed(\"cnn\", starting_idx=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"cnn_gpt4_modified_percentage_1.json\"\n",
    "path = os.path.join(\"summaries\",\"cnn\",file_name)\n",
    "save_to_json(modified_percentages,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "def generate_length_modifed(\n",
    "    dataset,\n",
    "    starting_idx=0,\n",
    "    ending_idx=1000\n",
    "):\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "    modified_responses = load_modified_results()\n",
    "    results = []  # load_from_json(f\"results/{model}_results.json\")\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:ending_idx], desc=\"Processing keys\"):\n",
    "        for idx, other in enumerate([s for s in SOURCES]):\n",
    "            sentence_to_paraphrase = similar_sentences_cnn[key][other]\n",
    "            substring_length = len(sentence_to_paraphrase)\n",
    "            result = {\"key\": key, \"model\": other, \"paraphrase_length\":substring_length}\n",
    "            results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 500/500 [00:00<00:00, 500274.81it/s]\n"
     ]
    }
   ],
   "source": [
    "length_modified = generate_length_modifed(\"cnn\", starting_idx=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"cnn_gpt4_modified_length_1.json\"\n",
    "path = os.path.join(\"summaries\",\"cnn\",file_name)\n",
    "save_to_json(length_modified,path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self_recog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
