{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\G34371231\\\\OneDrive - The George Washington University\\\\Desktop\\\\self_recognition'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data import load_data, SOURCES, save_to_json, load_from_json\n",
    "from models import (\n",
    "    get_gpt_recognition_logprobs,\n",
    "    get_model_choice,\n",
    "    get_logprobs_choice_with_sources,\n",
    "    get_gpt_score,\n",
    "    #get_gpt_summary_similarity_index,\n",
    ")\n",
    "\n",
    "from math import exp\n",
    "from pprint import pprint\n",
    "from random import shuffle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import openai\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonym Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "random.seed(123)\n",
    "all_words = [] #need to have different lists depending on method\n",
    "all_syn = []\n",
    "\n",
    "def ind_syn_ChatGPT(replacement_word):\n",
    "    #just returns the synonym provided by ChatGPT \n",
    "    #may wish to try system message where i describe desire behavior rather than in the prompt.\n",
    "    \n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f'Provide a synonym for the word \"{replacement_word}\". Your response should strictly be the synonym, so that it may immediately replace the original word within some sentence. If you don not find a synonym just return the original word.'\n",
    "            }\n",
    "        ]\n",
    ")\n",
    "\n",
    "    #synonym = completion['choices'][0]['message']['content'].strip()\n",
    "    #synonym = completion.choices[0].message['content'].strip()\n",
    "    synonym = completion.choices[0].message.content.lower()\n",
    "    return synonym\n",
    "\n",
    "def syn_from_contxt_ChatGPT(replacement_phrase):\n",
    "    #performs synonym replacement with context. The replacement_phrase can be the entire sentence of the word to be replaced, or it can be a smaller phrase.\n",
    "    \n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\") # nosec\n",
    "        \n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that rewrites phrases by replacing words surrounded by square brackets with synonyms while preserving context and meaning.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f' \"There are word(s) in this phrase surrounded by square brackets []. Replace the words with their synonyms and get rid of the brackets. Your response is strictly the new phrase containing the synonyms. The phrase is: {replacement_phrase}'\n",
    "            }\n",
    "        ]\n",
    ")\n",
    "    synonym = completion.choices[0].message.content\n",
    "    return synonym\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"\n",
    "    Finds and returns synonyms for a given word using WordNet.\n",
    "    This function takes a word as input, searches for its synonyms \n",
    "    using the WordNet synsets, and returns a list of synonyms.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word for which to find synonyms.\n",
    "    Returns:\n",
    "        list: A list of synonyms for the input word. Returns an \n",
    "              empty list if no synonyms are found.\n",
    "    \"\"\"\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for name in syn.lemma_names():\n",
    "            # Exclude the original word to avoid replacing it with itself\n",
    "            name = name.replace('_',' ')\n",
    "            if name.lower() != word.lower():\n",
    "                synonyms.append(name)\n",
    "    return synonyms\n",
    "\n",
    "def sample_words(words_alpha, num_words_to_replace):\n",
    "    # Randomly sample words to replace - i use 2x words just to account for words without synonym\n",
    "    idx_words = random.sample(list(enumerate(words_alpha)), min(2*num_words_to_replace, len(words_alpha)))\n",
    "    chosen_indices = []\n",
    "    words_to_replace = []\n",
    "    for pair in idx_words:\n",
    "        chosen_indices.append(pair[0])\n",
    "        words_to_replace.append(pair[1])  \n",
    "      \n",
    "    #idx_words is the list of words selected paired with their index in original sentence/phrase\n",
    "    #example: \"This is a test sentence to see which words get sampled, and what is returned by the replacement function.\"\n",
    "    #(1, 'is'), (8, 'words'), (2, 'a'), (13, 'is'), (4, 'sentence'), (18, 'function'), (0, 'This'), (6, 'see'), (17, 'replacement'), (10, 'sampled')\n",
    "    return chosen_indices, words_to_replace\n",
    "\n",
    "def replace_words_WordNet(sentence, num_words_to_replace):\n",
    "    \"\"\"\n",
    "    Replaces a specified number of words in a sentence with their synonyms using\n",
    "    the WordNet library, and then using ChatGPT. \n",
    "\n",
    "    This function takes a sentence and an integer specifying the number of words \n",
    "    to replace with synonyms. It randomly samples 2x the required number of words \n",
    "    to ensure replacements are possible even if some words do not have synonyms.\n",
    "    It uses the `get_synonyms` function to find synonyms for each sampled word,\n",
    "    and replaces words in the sentence until the specified number is reached.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence from which words will be replaced.\n",
    "        num_words_to_replace (int): The number of words in the sentence to be replaced by synonyms.\n",
    "\n",
    "    Returns:\n",
    "        str: The modified sentence with the specified number of words replaced by synonyms.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "    # Filter out non-alphabetic tokens (like punctuation)\n",
    "    words_alpha = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    # Randomly sample words to replace - i use 2x words just to account for words without synonym\n",
    "    indices, words_to_replace = sample_words(words_alpha, num_words_to_replace)\n",
    "    #print(words_to_replace)\n",
    "    #print(indices)\n",
    "    # Create a new sentence with synonyms replaced\n",
    "    words_replaced = 0\n",
    "    new_sentence = []\n",
    "    for word in words:\n",
    "        if word in words_to_replace and words_alpha.index(word) in indices:\n",
    "            synonyms = get_synonyms(word)\n",
    "            if synonyms and words_replaced < num_words_to_replace:\n",
    "                # Replace with a random synonym\n",
    "                new_word = random.choice(synonyms)\n",
    "                new_sentence.append(new_word)\n",
    "                #operational\n",
    "                all_words.append(word)\n",
    "                all_syn.append(synonyms)\n",
    "                words_replaced +=1\n",
    "            else:\n",
    "                # If no synonym is found, keep the original word\n",
    "                new_sentence.append(word)\n",
    "        else:\n",
    "            new_sentence.append(word)\n",
    "    \n",
    "    return ' '.join(new_sentence)\n",
    "    \n",
    "def replace_words_ChatGPT(sentence,num_words_to_replace):\n",
    "    #function to perform synonym replacement for individual words, without context provided\n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "    # Filter out non-alphabetic tokens (like punctuation)\n",
    "    words_alpha = [word for word in words if word.isalpha()]\n",
    "\n",
    "    # Randomly sample words to replace - i use 2x words just to account for words without synonym\n",
    "    replacement_indices, words_to_replace = sample_words(words_alpha, num_words_to_replace)\n",
    "    #print(replacement_indices)\n",
    "    #print(words_to_replace)\n",
    "    # Create a new sentence with synonyms replaced\n",
    "    words_replaced = 0\n",
    "    new_sentence = []\n",
    "    for word in words:\n",
    "        if word in words_to_replace and words_alpha.index(word) in replacement_indices and words_replaced < num_words_to_replace:\n",
    "            #ask chatGPT for synonym\n",
    "            new_word = ind_syn_ChatGPT(word) \n",
    "            new_sentence.append(new_word)\n",
    "            #operational\n",
    "            all_words.append(word)\n",
    "            words_replaced +=1\n",
    "        else:\n",
    "            new_sentence.append(word)\n",
    "    \n",
    "    return ' '.join(new_sentence)\n",
    "\n",
    "def insert_brackets(phrase, indices):\n",
    "    for i in indices:\n",
    "        if 0 <= i < len(phrase):\n",
    "            phrase[i] = f\"[{phrase[i]}]\"\n",
    "    new_phrase = ' '.join(phrase)\n",
    "    return new_phrase\n",
    "\n",
    "def replace_words_ChatGPT_context(sentence, num_words_to_replace):\n",
    "#get indices of words randomly sampled from sentence/phrase\n",
    "#indices are passed into llm prompting function to replace words given some context. \n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "    # Filter out non-alphabetic tokens (like punctuation)\n",
    "    words_alpha = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    # Randomly sample words to replace - i use 2x words just to account for words without synonym\n",
    "    replacement_indices, words_to_replace = sample_words(words_alpha, num_words_to_replace)\n",
    "    print(replacement_indices)\n",
    "    print(words_to_replace)\n",
    "\n",
    "    phrase_to_replace = insert_brackets(words_alpha, replacement_indices)\n",
    "    new_phrase = syn_from_contxt_ChatGPT(phrase_to_replace)\n",
    "    return new_phrase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import load_data, SOURCES, save_to_json, load_from_json\n",
    "responses, articles, keys = load_data(\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Former government contractor indicted for stealing nuclear materials\n",
      "Roy Lynn Oakley accused of attempting to sell restricted uranium enrichment components\n",
      "Oakley faces up to 10 years in prison and a $250,000 fine per count\n",
      "FBI sting operation prevented the materials from reaching foreign entities\n"
     ]
    }
   ],
   "source": [
    "key = keys[22]\n",
    "random_summary = responses[\"gpt4\"][key]\n",
    "print(random_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------wordNET summary----------\n",
      "Former government declarer indict for thievery atomic materials Roy Lynn Oakley accused of attempting to sell restricted atomic number 92 enrichment components Oakley faces up to 10 years in prison and a $ 250,000 fine per count FBI sting operation prevented the materials from reaching foreign entities\n",
      "-----chatGPT no context summary---\n",
      "previous government contractor indicted because theft nuclear materials Roy Lynn oakley accused of attempting to market restricted uranium enrichment components Oakley faces up to 10 years in prison and a $ 250,000 fine per count FBI sting operation prevented the materials from reaching foreign entities\n",
      "-----GPT with context summary-----\n",
      "Former state employee indicted for pilfering atomic materials Roy Lynn Oakley accused of attempting to sell confidential uranium refinement components Oakley faces up to years in prison and a fine for each count FBI sting operation prevented the materials from reaching foreign entities\n"
     ]
    }
   ],
   "source": [
    "new_summary_wordNET = replace_words_WordNet(random_summary, 5)  # Replace 5of the words\n",
    "new_summary_chatGPT = replace_words_ChatGPT(random_summary, 5)\n",
    "new_summary_chatGPT_context = replace_words_ChatGPT_context(random_summary, 5)\n",
    "print(\"---------wordNET summary----------\")\n",
    "print(new_summary_wordNET)\n",
    "print(\"-----chatGPT no context summary---\")\n",
    "print(new_summary_chatGPT)\n",
    "print(\"-----GPT with context summary-----\")\n",
    "print(new_summary_chatGPT_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: contractor\n",
      "['declarer', 'contractile organ']\n",
      "word: indicted\n",
      "['indict']\n",
      "word: stealing\n",
      "['larceny', 'theft', 'thievery', 'thieving', 'stealth', 'steal', 'steal', 'slip', 'steal']\n",
      "word: nuclear\n",
      "['atomic']\n",
      "word: uranium\n",
      "['U', 'atomic number 92']\n",
      "word: Former\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(all_words)):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword:\u001b[39m\u001b[38;5;124m'\u001b[39m, all_words[i])\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(all_syn[i])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_words)):\n",
    "    print('word:', all_words[i])\n",
    "    print(all_syn[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply to Dataset and Check Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only suitable for GPT models\n",
    "def generate_gpt_detect_recognition_synonym(\n",
    "    dataset,\n",
    "    model,\n",
    "    starting_idx=0,\n",
    "    detection_type=\"detection\",\n",
    "    replace_synonym = False,\n",
    "    num_words_to_replace = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates detection scores for GPT model outputs compared to other summaries.\n",
    "\n",
    "    This function takes a dataset name, a base model for inference, a starting index \n",
    "    from which to begin enumeration of the dataset, and various options for detection \n",
    "    and synonym replacement. It makes API calls to GPT models using the OpenAI API key \n",
    "    to evaluate the similarity of each summary against all other summaries. If synonym \n",
    "    replacement is enabled, a specified number of words are replaced before comparison.\n",
    "    \n",
    "    The function performs inference using the base model, compares generated summaries \n",
    "    in forward and backward order, and returns a JSON object containing detection results.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): The name of the dataset (e.g., \"cnn\") containing the articles.\n",
    "        model (str): The base model on which inference will be performed.\n",
    "        starting_idx (int, optional): The index to start processing articles from. Defaults to 0.\n",
    "        detection_type (str, optional): The type of detection to perform. Defaults to \"detection\".\n",
    "        replace_synonym (bool, optional): Whether to replace words in the summaries with synonyms. Defaults to False.\n",
    "        num_words_to_replace (int, optional): The number of words to replace with synonyms if `replace_synonym` is True. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        dict: A JSON object containing information about:\n",
    "            - Model compared against\n",
    "            - Key of the article\n",
    "            - Forward detection + probability\n",
    "            - Backward detection + probability\n",
    "            - Overall detection score\n",
    "\n",
    "    \"\"\"\n",
    "    # For retrieving summaries, the specific fine-tuning version isn't needed\n",
    "    exact_model = model\n",
    "    model = \"gpt35\" if model.endswith(\"gpt35\") else model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "    results = []  # load_from_json(f\"results/{model}_results.json\")\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:], desc=\"Processing keys\"):\n",
    "        article = articles[key]\n",
    "\n",
    "        source_summary = responses[model][key]\n",
    "\n",
    "        # replace synonym\n",
    "        if replace_synonym:\n",
    "            source_summary = replace_words_ChatGPT_context(source_summary, num_words_to_replace)\n",
    "\n",
    "        for other in [s for s in SOURCES if s != model]:\n",
    "            result = {\"key\": key, \"model\": other}\n",
    "            other_summary = responses[other][key]\n",
    "\n",
    "            # Detection\n",
    "            forward_result = get_model_choice(\n",
    "                source_summary,\n",
    "                other_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "            backward_result = get_model_choice(\n",
    "                other_summary,\n",
    "                source_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "\n",
    "            forward_choice = forward_result[0].token\n",
    "            backward_choice = backward_result[0].token\n",
    "\n",
    "            result[\"forward_detection\"] = forward_choice\n",
    "            result[\"forward_detection_probability\"] = exp(forward_result[0].logprob)\n",
    "            result[\"backward_detection\"] = backward_choice\n",
    "            result[\"backward_detection_probability\"] = exp(backward_result[0].logprob)\n",
    "\n",
    "            match (forward_choice, backward_choice):\n",
    "                case (\"1\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"1\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "\n",
    "            results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'claude', 'gpt35', 'gpt4', 'llama']\n",
      "Starting gpt4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 50/50 [14:40<00:00, 17.61s/it] \n"
     ]
    }
   ],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    print(SOURCES)\n",
    "    print(f\"Starting {model}\")\n",
    "    num_synonym = 5\n",
    "    results = generate_gpt_detect_recognition_synonym(\n",
    "        \"cnn\", model,replace_synonym=True, num_words_to_replace=num_synonym, starting_idx=950\n",
    "    )\n",
    "    #Save results\n",
    "    file_name = f\"{model}_results_{num_synonym}_ChatGPT_with_context_50_sentence.json\"\n",
    "    path = os.path.join(\"results\",\"cnn\",\"synonym\",file_name)\n",
    "    save_to_json(results,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR THE OTHER 450 data points\n",
    "def generate_gpt_detect_recognition_synonym(\n",
    "    dataset,\n",
    "    model,\n",
    "    starting_idx=0,\n",
    "    ending_idx=1000,\n",
    "    detection_type=\"detection\",\n",
    "    replace_synonym = False,\n",
    "    num_words_to_replace = 0\n",
    "):\n",
    "\n",
    "    # For retrieving summaries, the specific fine-tuning version isn't needed\n",
    "    exact_model = model\n",
    "    model = \"gpt35\" if model.endswith(\"gpt35\") else model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "    results = []  # load_from_json(f\"results/{model}_results.json\")\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:ending_idx], desc=\"Processing keys\"):\n",
    "        article = articles[key]\n",
    "\n",
    "        source_summary = responses[model][key]\n",
    "\n",
    "        # replace synonym\n",
    "        if replace_synonym:\n",
    "            source_summary = replace_words_ChatGPT_context(source_summary, num_words_to_replace)\n",
    "\n",
    "        for other in [s for s in SOURCES if s != model]:\n",
    "            result = {\"key\": key, \"model\": other}\n",
    "            other_summary = responses[other][key]\n",
    "\n",
    "            # Detection\n",
    "            forward_result = get_model_choice(\n",
    "                source_summary,\n",
    "                other_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "            backward_result = get_model_choice(\n",
    "                other_summary,\n",
    "                source_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "\n",
    "            forward_choice = forward_result[0].token\n",
    "            backward_choice = backward_result[0].token\n",
    "\n",
    "            result[\"forward_detection\"] = forward_choice\n",
    "            result[\"forward_detection_probability\"] = exp(forward_result[0].logprob)\n",
    "            result[\"backward_detection\"] = backward_choice\n",
    "            result[\"backward_detection_probability\"] = exp(backward_result[0].logprob)\n",
    "\n",
    "            match (forward_choice, backward_choice):\n",
    "                case (\"1\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"1\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "\n",
    "            results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'claude', 'gpt35', 'gpt4', 'llama']\n",
      "Starting gpt4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys:   0%|          | 0/450 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 450/450 [2:00:53<00:00, 16.12s/it]    \n"
     ]
    }
   ],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    print(SOURCES)\n",
    "    print(f\"Starting {model}\")\n",
    "    num_synonym = 5\n",
    "    results = generate_gpt_detect_recognition_synonym(\n",
    "        \"cnn\", model,replace_synonym=True, num_words_to_replace=num_synonym, starting_idx=500, ending_idx=950\n",
    "    )\n",
    "    #Save results\n",
    "    file_name = f\"{model}_results_{num_synonym}_ChatGPT_with_context_450_sentence.json\"\n",
    "    path = os.path.join(\"results\",\"cnn\",\"synonym\",file_name)\n",
    "    save_to_json(results,path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply synonym to 'other' example as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only suitable for GPT models\n",
    "def generate_gpt_detect_recognition_dual_synonym(\n",
    "    dataset,\n",
    "    model,\n",
    "    starting_idx=0,\n",
    "    detection_type=\"detection\",\n",
    "    replace_synonym = False,\n",
    "    num_words_to_replace = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates detection scores for GPT model outputs compared to other summaries using a dual synyonym replacement strategy.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): The name of the dataset (e.g., \"cnn\") containing the articles.\n",
    "        model (str): The base model on which inference will be performed.\n",
    "        starting_idx (int, optional): The index to start processing articles from. Defaults to 0.\n",
    "        detection_type (str, optional): The type of detection to perform. Defaults to \"detection\".\n",
    "        replace_synonym (bool, optional): Whether to replace words in the summaries with synonyms. Defaults to False.\n",
    "        num_words_to_replace (int, optional): The number of words to replace with synonyms if `replace_synonym` is True. Defaults to 0. Replaces the same number in both sentences being compared.\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # For retrieving summaries, the specific fine-tuning version isn't needed\n",
    "    exact_model = model\n",
    "    model = \"gpt35\" if model.endswith(\"gpt35\") else model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "    results = []  # load_from_json(f\"results/{model}_results.json\")\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:], desc=\"Processing keys\"):\n",
    "        article = articles[key]\n",
    "\n",
    "        source_summary = responses[model][key]\n",
    "\n",
    "        # replace synonym\n",
    "        if replace_synonym:\n",
    "            source_summary = replace_words_ChatGPT_context(source_summary, num_words_to_replace)\n",
    "\n",
    "        for other in [s for s in SOURCES if s != model]:\n",
    "            result = {\"key\": key, \"model\": other}\n",
    "            other_summary = responses[other][key]\n",
    "\n",
    "            # replace synonym\n",
    "            if replace_synonym:\n",
    "                other_summary = replace_words_ChatGPT_context(other_summary, num_words_to_replace)\n",
    "\n",
    "            # Detection\n",
    "            forward_result = get_model_choice(\n",
    "                source_summary,\n",
    "                other_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "            backward_result = get_model_choice(\n",
    "                other_summary,\n",
    "                source_summary,\n",
    "                article,\n",
    "                detection_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "\n",
    "            forward_choice = forward_result[0].token\n",
    "            backward_choice = backward_result[0].token\n",
    "\n",
    "            result[\"forward_detection\"] = forward_choice\n",
    "            result[\"forward_detection_probability\"] = exp(forward_result[0].logprob)\n",
    "            result[\"backward_detection\"] = backward_choice\n",
    "            result[\"backward_detection_probability\"] = exp(backward_result[0].logprob)\n",
    "\n",
    "            match (forward_choice, backward_choice):\n",
    "                case (\"1\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"1\", \"1\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"2\"):\n",
    "                    result[\"detection_score\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "\n",
    "            results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'claude', 'gpt35', 'gpt4', 'llama']\n",
      "Starting gpt4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 50/50 [13:25<00:00, 16.10s/it]\n"
     ]
    }
   ],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    print(SOURCES)\n",
    "    print(f\"Starting {model}\")\n",
    "    num_synonym = 5\n",
    "    results = generate_gpt_detect_recognition_dual_synonym(\n",
    "        \"cnn\", model,replace_synonym=True, num_words_to_replace=num_synonym, starting_idx=950\n",
    "    )\n",
    "    #Save results\n",
    "    file_name = f\"{model}_results_{num_synonym}_ChatGPT_with_context_bothsentences_50_sentence.json\"\n",
    "    path = os.path.join(\"results\",\"cnn\",\"synonym\",file_name)\n",
    "    save_to_json(results,path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Self Preference Scores for GPT4 w/ context (2 word synonym replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only suitable for GPT models\n",
    "import json \n",
    "\n",
    "def generate_gpt_detect_preference_synonym(\n",
    "    dataset,\n",
    "    model,\n",
    "    starting_idx=0,\n",
    "    detection_type=\"detection\",\n",
    "    comparison_type=\"comparison\",\n",
    "    replace_synonym = False,\n",
    "    num_words_to_replace = 0, \n",
    "    output_file=\"synonym_replacement_GPT4_context_summaries.json\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates detection scores for GPT model outputs compared to other summaries.\n",
    "\n",
    "    This function takes a dataset name, a base model for inference, a starting index \n",
    "    from which to begin enumeration of the dataset, and various options for detection \n",
    "    and synonym replacement. It makes API calls to GPT models using the OpenAI API key \n",
    "    to evaluate the similarity of each summary against all other summaries. If synonym \n",
    "    replacement is enabled, a specified number of words are replaced before comparison.\n",
    "    \n",
    "    The function performs inference using the base model, compares generated summaries \n",
    "    in forward and backward order, and returns a JSON object containing detection results.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): The name of the dataset (e.g., \"cnn\") containing the articles.\n",
    "        model (str): The base model on which inference will be performed.\n",
    "        starting_idx (int, optional): The index to start processing articles from. Defaults to 0.\n",
    "        detection_type (str, optional): The type of detection to perform. Defaults to \"detection\".\n",
    "        replace_synonym (bool, optional): Whether to replace words in the summaries with synonyms. Defaults to False.\n",
    "        num_words_to_replace (int, optional): The number of words to replace with synonyms if `replace_synonym` is True. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        dict: A JSON object containing information about:\n",
    "            - Model compared against\n",
    "            - Key of the article\n",
    "            - Forward detection + probability\n",
    "            - Backward detection + probability\n",
    "            - Overall detection score\n",
    "\n",
    "    \"\"\"\n",
    "    # For retrieving summaries, the specific fine-tuning version isn't needed\n",
    "    exact_model = model\n",
    "    model = \"gpt35\" if model.endswith(\"gpt35\") else model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "    results = []  # load_from_json(f\"results/{model}_results.json\")\n",
    "    updated_summaries = {}  #to store updated summaries with synonym replacement\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:], desc=\"Processing keys\"):\n",
    "        article = articles[key]\n",
    "\n",
    "        source_summary = responses[model][key]\n",
    "\n",
    "        # replace synonym\n",
    "        if replace_synonym:\n",
    "            source_summary = replace_words_ChatGPT_context(source_summary, num_words_to_replace)\n",
    "            updated_summaries[key] = source_summary  #store new summary\n",
    "\n",
    "        for other in [s for s in SOURCES if s != model]:\n",
    "            result = {\"key\": key, \"model\": other}\n",
    "            other_summary = responses[other][key]\n",
    "\n",
    "            # Comparison\n",
    "            forward_result = get_model_choice(\n",
    "                source_summary,\n",
    "                other_summary,\n",
    "                article,\n",
    "                comparison_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "            backward_result = get_model_choice(\n",
    "                other_summary,\n",
    "                source_summary,\n",
    "                article,\n",
    "                comparison_type,\n",
    "                exact_model,\n",
    "                return_logprobs=True,\n",
    "            )\n",
    "\n",
    "            forward_choice = forward_result[0].token\n",
    "            backward_choice = backward_result[0].token\n",
    "\n",
    "            # If the comparison asked \"Which is worse?\" then reverse the options\n",
    "            if comparison_type == \"comparison_with_worse\":\n",
    "                forward_choice = \"1\" if forward_choice == \"2\" else \"2\"\n",
    "                backward_choice = \"1\" if backward_choice == \"2\" else \"2\"\n",
    "\n",
    "            result[\"forward_comparison\"] = forward_choice\n",
    "            result[\"forward_comparison_probability\"] = exp(forward_result[0].logprob)\n",
    "            result[\"backward_comparison\"] = backward_choice\n",
    "            result[\"backward_comparison_probability\"] = exp(backward_result[0].logprob)\n",
    "\n",
    "            match (forward_choice, backward_choice):\n",
    "                case (\"1\", \"2\"):\n",
    "                    result[\"self_preference\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"1\"):\n",
    "                    result[\"self_preference\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"1\", \"1\"):\n",
    "                    result[\"self_preference\"] = 0.5 * (\n",
    "                        exp(forward_result[0].logprob) + exp(backward_result[1].logprob)\n",
    "                    )\n",
    "                case (\"2\", \"2\"):\n",
    "                    result[\"self_preference\"] = 0.5 * (\n",
    "                        exp(forward_result[1].logprob) + exp(backward_result[0].logprob)\n",
    "                    )\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "    if replace_synonym:\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(updated_summaries, f, indent=4)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'claude', 'gpt35', 'gpt4', 'llama']\n",
      "Starting gpt4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys:   0%|          | 0/1 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m num_synonym \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m generate_gpt_detect_preference_synonym(\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn\u001b[39m\u001b[38;5;124m\"\u001b[39m, model,replace_synonym\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_words_to_replace\u001b[38;5;241m=\u001b[39mnum_synonym, starting_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m999\u001b[39m, output_file \u001b[38;5;241m=\u001b[39m responses_file\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#Save results\u001b[39;00m\n\u001b[0;32m     11\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_preference_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_synonym\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ChatGPT_with_context_bothsentences_500_sentence.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[11], line 58\u001b[0m, in \u001b[0;36mgenerate_gpt_detect_preference_synonym\u001b[1;34m(dataset, model, starting_idx, detection_type, comparison_type, replace_synonym, num_words_to_replace, output_file)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# replace synonym\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m replace_synonym:\n\u001b[1;32m---> 58\u001b[0m     source_summary \u001b[38;5;241m=\u001b[39m replace_words_ChatGPT_context(source_summary, num_words_to_replace)\n\u001b[0;32m     59\u001b[0m     updated_summaries[key] \u001b[38;5;241m=\u001b[39m source_summary  \u001b[38;5;66;03m#store new summary\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m [s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m SOURCES \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;241m!=\u001b[39m model]:\n",
      "Cell \u001b[1;32mIn[7], line 179\u001b[0m, in \u001b[0;36mreplace_words_ChatGPT_context\u001b[1;34m(sentence, num_words_to_replace)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m#print(replacement_indices)\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m#print(words_to_replace)\u001b[39;00m\n\u001b[0;32m    178\u001b[0m phrase_to_replace \u001b[38;5;241m=\u001b[39m insert_brackets(words_alpha, replacement_indices)\n\u001b[1;32m--> 179\u001b[0m new_phrase \u001b[38;5;241m=\u001b[39m syn_from_contxt_ChatGPT(phrase_to_replace)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_phrase\n",
      "Cell \u001b[1;32mIn[7], line 33\u001b[0m, in \u001b[0;36msyn_from_contxt_ChatGPT\u001b[1;34m(replacement_phrase)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msyn_from_contxt_ChatGPT\u001b[39m(replacement_phrase):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m#performs synonym replacement with context. The replacement_phrase can be the entire sentence of the word to be replaced, or it can be a smaller phrase.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     completion \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     34\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     35\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     36\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant that rewrites phrases by replacing words surrounded by square brackets with synonyms while preserving context and meaning.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     37\u001b[0m             {\n\u001b[0;32m     38\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     39\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are word(s) in this phrase surrounded by square brackets []. Replace the words with their synonyms and get rid of the brackets. Your response is strictly the new phrase containing the synonyms. The phrase is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreplacement_phrase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     40\u001b[0m             }\n\u001b[0;32m     41\u001b[0m         ]\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     43\u001b[0m     synonym \u001b[38;5;241m=\u001b[39m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m synonym\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\openai\\_utils\\_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:742\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    739\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    740\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    741\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    744\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    745\u001b[0m             {\n\u001b[0;32m    746\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    747\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    748\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    749\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    750\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    751\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    752\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    753\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    754\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    755\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    756\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    757\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    758\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    759\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    760\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    761\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    762\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    763\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    764\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    765\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    766\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    767\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    768\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    769\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    770\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    771\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    772\u001b[0m             },\n\u001b[0;32m    773\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    774\u001b[0m         ),\n\u001b[0;32m    775\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    776\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    777\u001b[0m         ),\n\u001b[0;32m    778\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    779\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    780\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    781\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\openai\\_base_client.py:1270\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1258\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1265\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1267\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1268\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1269\u001b[0m     )\n\u001b[1;32m-> 1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\openai\\_base_client.py:947\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    945\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    948\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    949\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    950\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    951\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    952\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m    953\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\openai\\_base_client.py:1036\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1035\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1037\u001b[0m         input_options,\n\u001b[0;32m   1038\u001b[0m         cast_to,\n\u001b[0;32m   1039\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1040\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1041\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1042\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1043\u001b[0m     )\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\openai\\_base_client.py:1085\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1086\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1087\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1088\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1089\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1090\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1091\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\openai\\_base_client.py:1036\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1035\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1037\u001b[0m         input_options,\n\u001b[0;32m   1038\u001b[0m         cast_to,\n\u001b[0;32m   1039\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1040\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1041\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1042\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1043\u001b[0m     )\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\openai\\_base_client.py:1085\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1086\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1087\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1088\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1089\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1090\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1091\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\G34371231\\AppData\\Local\\anaconda3\\envs\\self_recog\\Lib\\site-packages\\openai\\_base_client.py:1051\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1048\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1050\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1054\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1055\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1059\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1060\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "responses_file = \"cnn_gpt4_context_synonym_replacement_responses.json\"\n",
    "path = os.path.join(\"summaries\",\"cnn\",responses_file)\n",
    "for model in [\"gpt4\"]:\n",
    "    print(SOURCES)\n",
    "    print(f\"Starting {model}\")\n",
    "    num_synonym = 5\n",
    "    results = generate_gpt_detect_preference_synonym(\n",
    "        \"cnn\", model,replace_synonym=True, num_words_to_replace=num_synonym, starting_idx=999, output_file = responses_file\n",
    "    )\n",
    "    #Save results\n",
    "    file_name = f\"{model}_preference_{num_synonym}_ChatGPT_with_context_bothsentences_500_sentence.json\"\n",
    "    path = os.path.join(\"results\",\"cnn\",\"synonym\",file_name)\n",
    "    save_to_json(results,path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding semantically similar indexes in the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only suitable for GPT models\n",
    "def get_similar_sentence(\n",
    "    dataset,\n",
    "    model,\n",
    "    starting_idx=0,\n",
    "):\n",
    "    exact_model = model\n",
    "    model = \"gpt35\" if model.endswith(\"gpt35\") else model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "    results = []  # load_from_json(f\"results/{model}_results.json\")\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:], desc=\"Processing keys\"):\n",
    "        summaries = []\n",
    "        result = {\"key\": key}\n",
    "        for other in [s for s in SOURCES]:\n",
    "            summaries.append(responses[other][key])\n",
    "            \n",
    "        result_json = get_gpt_summary_similarity_index(summaries[0],summaries[1],summaries[2],summaries[3],summaries[4])\n",
    "        result[\"indexes\"]= result_json \n",
    "        results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'claude', 'gpt35', 'gpt4', 'llama']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'key': '9177e5ac94f038749e8d4eb526a65461e0f6df4c', 'indexes': '[\"Summary1:0\", \"Summary2:0\", \"Summary3:0\", \"Summary4:0\", \"Summary5:1\"]'}, {'key': 'f12e4bbb07211de7d43b4e331dc73404aa804562', 'indexes': '[\"Summary1:2\", \"Summary2:1\", \"Summary3:2\", \"Summary4:1\", \"Summary5:0\"]'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    print(SOURCES)\n",
    "    num_synonym = 2\n",
    "    results = get_similar_sentence(\"cnn\", model, starting_idx=998)\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'key': '9177e5ac94f038749e8d4eb526a65461e0f6df4c',\n",
       "  'indexes': '[\"Summary1:0\", \"Summary2:0\", \"Summary3:0\", \"Summary4:0\", \"Summary5:1\"]'},\n",
       " {'key': 'f12e4bbb07211de7d43b4e331dc73404aa804562',\n",
       "  'indexes': '[\"Summary1:2\", \"Summary2:1\", \"Summary3:2\", \"Summary4:1\", \"Summary5:0\"]'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize what is similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract a record by key\n",
    "def get_record_by_key(results, search_key):\n",
    "    for record in results:\n",
    "        if record['key'] == search_key:\n",
    "            return record\n",
    "    return None  # Return None if key is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract indexes from the record\n",
    "def extract_indexes(record):\n",
    "    if record and 'indexes' in record:\n",
    "        indexes_str = record['indexes']\n",
    "        # Convert the indexes string to a list using json.loads\n",
    "        indexes_list = json.loads(indexes_str)\n",
    "        return indexes_list\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only suitable for GPT models\n",
    "def visualize_similar(\n",
    "    dataset,\n",
    "    model,\n",
    "    starting_idx=0,\n",
    "):\n",
    "    exact_model = model\n",
    "    model = \"gpt35\" if model.endswith(\"gpt35\") else model\n",
    "\n",
    "    responses, articles, keys = load_data(dataset)\n",
    "\n",
    "    for key in tqdm(keys[starting_idx:], desc=\"Processing keys\"):\n",
    "        summaries = []\n",
    "        result = get_record_by_key(results,key)\n",
    "        count = 1\n",
    "        for idx, other in enumerate([s for s in SOURCES]):\n",
    "            print(other)\n",
    "            summary = responses[other][key]\n",
    "            summary = summary.split('\\n')\n",
    "            indexes = extract_indexes(result)\n",
    "            item = indexes[idx]\n",
    "            summary_value = int(item.split(\":\")[1])\n",
    "            summary = summary[summary_value]\n",
    "            print(summary)\n",
    "            \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'claude', 'gpt35', 'gpt4', 'llama']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys: 100%|██████████| 2/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human\n",
      "Judge on Heather Mills: Level of premarital wealth \"exaggerated\"\n",
      "claude\n",
      "Judge rejects Mills' claim that she was wealthy before meeting McCartney in 1999\n",
      "gpt35\n",
      "Judge rejects Heather Mills' claim of wealth before marriage to Paul McCartney\n",
      "gpt4\n",
      "Judge finds Heather Mills' claims of wealth in 1999 exaggerated and rejects her portrayal as Paul McCartney's business partner\n",
      "llama\n",
      "Judge finds Mills' wealth exaggerated and her living style unrealistic\n",
      "human\n",
      "President Taylor's daughter married future president of an enemy power\n",
      "claude\n",
      "Elizabeth Harrison Walker, daughter of President Benjamin Harrison, was an economic expert who appeared on radio and TV shows\n",
      "gpt35\n",
      "Elizabeth Harrison Walker: Accomplished woman, lawyer, economist, and media personality\n",
      "gpt4\n",
      "Elizabeth Harrison Walker, daughter of President Benjamin Harrison, became a lawyer and economic expert, appearing on radio and TV\n",
      "llama\n",
      "Sarah Knox Taylor Davis - Died at 21 after falling ill with malaria while visiting her husband's relatives in Louisiana\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [\"gpt4\"]:\n",
    "    print(SOURCES)\n",
    "    visualize_similar(\"cnn\", model, starting_idx=998)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self_recog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
