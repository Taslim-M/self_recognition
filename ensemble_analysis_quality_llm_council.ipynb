{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\self_pref_quality.json', 'r') as file:\n",
    "    preference_results = json.load(file)\n",
    "\n",
    "with open('.\\quality\\pref_other_wrong_quality.json', 'r') as file:\n",
    "    preference_results_other_wrong = json.load(file)\n",
    "\n",
    "with open('.\\quality\\pref_results_third_party_eval_original_harmful.json', 'r') as file:\n",
    "    pref_results_third_party_eval_original_harmful = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lookup dictionaries\n",
    "other_wrong_lookup = {(r['evaluator'], r['evaluatee'], r['pid']): r for r in preference_results_other_wrong}\n",
    "third_party_lookup = {(r['correct_answer_model'], r['wrong_answer_model'], r['pid']): r for r in pref_results_third_party_eval_original_harmful}\n",
    "\n",
    "# Assemble results\n",
    "ensemble_results = []\n",
    "\n",
    "for r in preference_results:\n",
    "    pid = r['pid']\n",
    "    evaluator = r['evaluator']\n",
    "    evaluatee = r['evaluatee']\n",
    "\n",
    "    result = {\n",
    "        'pid': pid,\n",
    "        'judge_1': evaluator,\n",
    "        'judge_1_correct': (r['forward_comparison'] == '2' and r['backward_comparison'] == '1')\n",
    "    }\n",
    "\n",
    "    # Judge 2\n",
    "    key2 = (evaluatee, evaluator, pid)\n",
    "    if key2 in other_wrong_lookup:\n",
    "        r2 = other_wrong_lookup[key2]\n",
    "        result['judge_2'] = r2['evaluator']\n",
    "        result['judge_2_correct'] = (r2['forward_comparison'] == '1' and r2['backward_comparison'] == '2')\n",
    "    else:\n",
    "        result['judge_2'] = None\n",
    "        result['judge_2_correct'] = None\n",
    "\n",
    "    # Judge 3\n",
    "    key3 = (evaluatee, evaluator, pid)\n",
    "    if key3 in third_party_lookup:\n",
    "        r3 = third_party_lookup[key3]\n",
    "        result['judge_3'] = r3['judge_model']\n",
    "        result['judge_3_correct'] = (r3['forward_comparison'] == '2' and r3['backward_comparison'] == '1')\n",
    "    else:\n",
    "        result['judge_3'] = None\n",
    "        result['judge_3_correct'] = None\n",
    "\n",
    "    ensemble_results.append(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1439, 2353, '61.16%')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize counter\n",
    "correct_ensemble = 0\n",
    "\n",
    "# Count how many records have all three judges correct\n",
    "for record in ensemble_results:\n",
    "    correct_votes = sum([\n",
    "        record.get(\"judge_1_correct\") is True,\n",
    "        record.get(\"judge_2_correct\") is True,\n",
    "        record.get(\"judge_3_correct\") is True\n",
    "    ])\n",
    "    if correct_votes >= 2:\n",
    "        correct_ensemble += 1\n",
    "\n",
    "# Calculate accuracy\n",
    "total_records = len(ensemble_results)\n",
    "ensemble_accuracy = (correct_ensemble / total_records * 100) if total_records > 0 else 0\n",
    "\n",
    "correct_ensemble, total_records, f\"{ensemble_accuracy:.2f}%\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\perturb2_meta_self_pref_quality.json', 'r') as file:\n",
    "    perturb2_meta_preference_results = json.load(file)\n",
    "\n",
    "with open('.\\quality\\perturb2_meta_self_pref_quality_other_wrong.json', 'r') as file:\n",
    "    perturb2_meta_preference_results_other_wrong = json.load(file)\n",
    "\n",
    "with open('.\\quality\\pref_results_third_party_eval_perturb2.json', 'r') as file:\n",
    "    pref_results_third_party_eval_perturb2 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lookup dictionaries\n",
    "other_wrong_lookup = {(r['evaluator'], r['evaluatee'], r['pid']): r for r in perturb2_meta_preference_results_other_wrong}\n",
    "third_party_lookup = {(r['correct_answer_model'], r['wrong_answer_model'], r['pid']): r for r in pref_results_third_party_eval_perturb2}\n",
    "\n",
    "# Assemble results\n",
    "ensemble_results = []\n",
    "\n",
    "for r in perturb2_meta_preference_results:\n",
    "    pid = r['pid']\n",
    "    evaluator = r['evaluator']\n",
    "    evaluatee = r['evaluatee']\n",
    "\n",
    "    result = {\n",
    "        'pid': pid,\n",
    "        'judge_1': evaluator,\n",
    "        'judge_1_correct': (r['forward_comparison'] == '2' and r['backward_comparison'] == '1')\n",
    "    }\n",
    "\n",
    "    # Judge 2\n",
    "    key2 = (evaluatee, evaluator, pid)\n",
    "    if key2 in other_wrong_lookup:\n",
    "        r2 = other_wrong_lookup[key2]\n",
    "        result['judge_2'] = r2['evaluator']\n",
    "        result['judge_2_correct'] = (r2['forward_comparison'] == '1' and r2['backward_comparison'] == '2')\n",
    "    else:\n",
    "        result['judge_2'] = None\n",
    "        result['judge_2_correct'] = None\n",
    "\n",
    "    # Judge 3\n",
    "    key3 = (evaluatee, evaluator, pid)\n",
    "    if key3 in third_party_lookup:\n",
    "        r3 = third_party_lookup[key3]\n",
    "        result['judge_3'] = r3['judge_model']\n",
    "        result['judge_3_correct'] = (r3['forward_comparison'] == '2' and r3['backward_comparison'] == '1')\n",
    "    else:\n",
    "        result['judge_3'] = None\n",
    "        result['judge_3_correct'] = None\n",
    "\n",
    "    ensemble_results.append(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1596, 2353, '67.83%')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize counter\n",
    "correct_ensemble = 0\n",
    "\n",
    "for record in ensemble_results:\n",
    "    correct_votes = sum([\n",
    "        record.get(\"judge_1_correct\") is True,\n",
    "        record.get(\"judge_2_correct\") is True,\n",
    "        record.get(\"judge_3_correct\") is True\n",
    "    ])\n",
    "    if correct_votes >= 2:\n",
    "        correct_ensemble += 1\n",
    "\n",
    "# Calculate accuracy\n",
    "total_records = len(ensemble_results)\n",
    "ensemble_accuracy = (correct_ensemble / total_records * 100) if total_records > 0 else 0\n",
    "\n",
    "correct_ensemble, total_records, f\"{ensemble_accuracy:.2f}%\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\quality\\paraphrase_other_by_eval_preference_results.json', 'r') as file:\n",
    "    paraphrase_other_by_eval_preference_results = json.load(file)\n",
    "\n",
    "with open('.\\quality\\paraphrase_other_by_eval_preference_results_other_wrong.json', 'r') as file:\n",
    "    paraphrase_other_by_eval_preference_results_other_wrong = json.load(file)\n",
    "\n",
    "with open('.\\quality\\pref_results_third_party_eval_paraphrase.json', 'r') as file:\n",
    "    pref_results_third_party_eval_paraphrase = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lookup dictionaries\n",
    "other_wrong_lookup = {(r['evaluator'], r['evaluatee'], r['pid']): r for r in paraphrase_other_by_eval_preference_results_other_wrong}\n",
    "third_party_lookup = {(r['correct_answer_model'], r['wrong_answer_model'], r['pid']): r for r in pref_results_third_party_eval_paraphrase}\n",
    "\n",
    "# Assemble results\n",
    "ensemble_results = []\n",
    "\n",
    "for r in paraphrase_other_by_eval_preference_results:\n",
    "    pid = r['pid']\n",
    "    evaluator = r['evaluator']\n",
    "    evaluatee = r['evaluatee']\n",
    "\n",
    "    result = {\n",
    "        'pid': pid,\n",
    "        'judge_1': evaluator,\n",
    "        'judge_1_correct': (r['forward_comparison'] == '2' and r['backward_comparison'] == '1')\n",
    "    }\n",
    "\n",
    "    # Judge 2\n",
    "    key2 = (evaluatee, evaluator, pid)\n",
    "    if key2 in other_wrong_lookup:\n",
    "        r2 = other_wrong_lookup[key2]\n",
    "        result['judge_2'] = r2['evaluator']\n",
    "        result['judge_2_correct'] = (r2['forward_comparison'] == '1' and r2['backward_comparison'] == '2')\n",
    "    else:\n",
    "        result['judge_2'] = None\n",
    "        result['judge_2_correct'] = None\n",
    "\n",
    "    # Judge 3\n",
    "    key3 = (evaluatee, evaluator, pid)\n",
    "    if key3 in third_party_lookup:\n",
    "        r3 = third_party_lookup[key3]\n",
    "        result['judge_3'] = r3['judge_model']\n",
    "        result['judge_3_correct'] = (r3['forward_comparison'] == '2' and r3['backward_comparison'] == '1')\n",
    "    else:\n",
    "        result['judge_3'] = None\n",
    "        result['judge_3_correct'] = None\n",
    "\n",
    "    ensemble_results.append(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1243, 2353, '52.83%')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize counter\n",
    "correct_ensemble = 0\n",
    "\n",
    "for record in ensemble_results:\n",
    "    correct_votes = sum([\n",
    "        record.get(\"judge_1_correct\") is True,\n",
    "        record.get(\"judge_2_correct\") is True,\n",
    "        record.get(\"judge_3_correct\") is True\n",
    "    ])\n",
    "    if correct_votes >= 2:\n",
    "        correct_ensemble += 1\n",
    "\n",
    "# Calculate accuracy\n",
    "total_records = len(ensemble_results)\n",
    "ensemble_accuracy = (correct_ensemble / total_records * 100) if total_records > 0 else 0\n",
    "\n",
    "correct_ensemble, total_records, f\"{ensemble_accuracy:.2f}%\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
