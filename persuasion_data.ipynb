{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import (get_gpt_argument)\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_df(df):\n",
    "    \n",
    "    # Ensure 'claim' and 'source' columns exist\n",
    "    if 'claim' not in df.columns or 'source' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'claim' and 'source' columns\")\n",
    "    \n",
    "    # Count unique claims and their occurrences\n",
    "    claim_counts = df['claim'].value_counts()\n",
    "    \n",
    "    # Grouping claims by their row count\n",
    "    claim_frequency_distribution = Counter(claim_counts.values)\n",
    "    \n",
    "    # Count occurrences of each unique source\n",
    "    source_counts = df['source'].value_counts()\n",
    "\n",
    "    # #     # Find claims with exactly 78 rows\n",
    "    # claims_with_78_rows = claim_counts[claim_counts == 26].index.tolist()\n",
    "    # print(\"Claims with 78 rows:\", claims_with_78_rows)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Total unique claims: {len(claim_counts)}\")\n",
    "    print(\"Claim frequency distribution:\")\n",
    "    for num_rows, num_claims in claim_frequency_distribution.items():\n",
    "        print(f\"{num_claims} claims have {num_rows} rows\")\n",
    "    \n",
    "    print(\"\\nSource counts:\")\n",
    "    print(source_counts)\n",
    "    \n",
    "    return {\n",
    "        'total_unique_claims': len(claim_counts),\n",
    "        'claim_frequency_distribution': dict(claim_frequency_distribution),\n",
    "        'source_counts': source_counts.to_dict()\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique claims: 56\n",
      "Claim frequency distribution:\n",
      "2 claims have 26 rows\n",
      "54 claims have 23 rows\n",
      "\n",
      "Source counts:\n",
      "source\n",
      "Claude 2              224\n",
      "Claude 3 Haiku        224\n",
      "Claude 3 Opus         224\n",
      "Claude Instant 1.2    224\n",
      "Claude 1.3            224\n",
      "Human                 174\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('anthropic_persuasion\\persuasion_data.csv')\n",
    "df = df.drop(columns=['worker_id'], errors='ignore')\n",
    "df = df[df['source'] != 'Control']\n",
    "df = df.drop_duplicates(subset=['argument']).dropna(subset=['argument'])\n",
    "result = analyze_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_claims = df['claim'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting GPT arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_arg_compelling = get_gpt_argument(unique_claims[0], 'compelling', \"gpt-4-1106-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Items: 100%|██████████| 56/56 [10:00<00:00, 10.73s/it]\n"
     ]
    }
   ],
   "source": [
    "# Compelling\n",
    "claim_type = 'compelling'\n",
    "gpt4_compelling = []\n",
    "for i in tqdm(range(len(unique_claims)), desc=\"Processing Items\"):\n",
    "    claim = unique_claims[i]\n",
    "    # Perform some operation\n",
    "    gpt_arg = get_gpt_argument(claim, claim_type, \"gpt-4-1106-preview\")\n",
    "    gpt4_compelling.append(gpt_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, claim in enumerate(unique_claims):\n",
    "    new_row = {\n",
    "        'claim': claim,\n",
    "        'argument': gpt4_compelling[index],\n",
    "        'source': 'GPT4',\n",
    "        'prompt_type': 'Compelling Case'\n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Items: 100%|██████████| 56/56 [09:54<00:00, 10.62s/it]\n"
     ]
    }
   ],
   "source": [
    "# role-play\n",
    "claim_type = 'role-play'\n",
    "gpt4_role = []\n",
    "for i in tqdm(range(len(unique_claims)), desc=\"Processing Items\"):\n",
    "    claim = unique_claims[i]\n",
    "    gpt_arg = get_gpt_argument(claim, claim_type, \"gpt-4-1106-preview\")\n",
    "    gpt4_role.append(gpt_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, claim in enumerate(unique_claims):\n",
    "    new_row = {\n",
    "        'claim': claim,\n",
    "        'argument': gpt4_role[index],\n",
    "        'source': 'GPT4',\n",
    "        'prompt_type': 'Expert Writer Rhetorics'\n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Items: 100%|██████████| 56/56 [09:11<00:00,  9.86s/it]\n"
     ]
    }
   ],
   "source": [
    "# logical\n",
    "claim_type = 'logical'\n",
    "gpt4_logical = []\n",
    "for i in tqdm(range(len(unique_claims)), desc=\"Processing Items\"):\n",
    "    claim = unique_claims[i]\n",
    "    gpt_arg = get_gpt_argument(claim, claim_type, \"gpt-4-1106-preview\")\n",
    "    gpt4_logical.append(gpt_arg)\n",
    "\n",
    "for index, claim in enumerate(unique_claims):\n",
    "    new_row = {\n",
    "        'claim': claim,\n",
    "        'argument': gpt4_logical[index],\n",
    "        'source': 'GPT4',\n",
    "        'prompt_type': 'Logical Reasoning'\n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df as a JSON object\n",
    "df.to_json('anthropic_persuasion\\gpt4_data_clean.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Items: 100%|██████████| 56/56 [02:35<00:00,  2.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# Compelling\n",
    "claim_type = 'compelling'\n",
    "gpt35_compelling = []\n",
    "for i in tqdm(range(len(unique_claims)), desc=\"Processing Items\"):\n",
    "    claim = unique_claims[i]\n",
    "    # Perform some operation\n",
    "    gpt_arg = get_gpt_argument(claim, claim_type, \"gpt-3.5-turbo-1106\")\n",
    "    gpt35_compelling.append(gpt_arg)\n",
    "\n",
    "for index, claim in enumerate(unique_claims):\n",
    "    new_row = {\n",
    "        'claim': claim,\n",
    "        'argument': gpt35_compelling[index],\n",
    "        'source': 'GPT3.5',\n",
    "        'prompt_type': 'Compelling Case'\n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Items: 100%|██████████| 56/56 [02:57<00:00,  3.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# Role\n",
    "claim_type = 'role-play'\n",
    "gpt35_role = []\n",
    "for i in tqdm(range(len(unique_claims)), desc=\"Processing Items\"):\n",
    "    claim = unique_claims[i]\n",
    "    # Perform some operation\n",
    "    gpt_arg = get_gpt_argument(claim, claim_type, \"gpt-3.5-turbo-1106\")\n",
    "    gpt35_role.append(gpt_arg)\n",
    "\n",
    "for index, claim in enumerate(unique_claims):\n",
    "    new_row = {\n",
    "        'claim': claim,\n",
    "        'argument': gpt35_role[index],\n",
    "        'source': 'GPT3.5',\n",
    "        'prompt_type': 'Expert Writer Rhetorics'\n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Items: 100%|██████████| 56/56 [02:28<00:00,  2.66s/it]\n"
     ]
    }
   ],
   "source": [
    "# logical\n",
    "claim_type = 'logical'\n",
    "gpt35_logical = []\n",
    "for i in tqdm(range(len(unique_claims)), desc=\"Processing Items\"):\n",
    "    claim = unique_claims[i]\n",
    "    gpt_arg = get_gpt_argument(claim, claim_type, \"gpt-3.5-turbo-1106\")\n",
    "    gpt35_logical.append(gpt_arg)\n",
    "\n",
    "for index, claim in enumerate(unique_claims):\n",
    "    new_row = {\n",
    "        'claim': claim,\n",
    "        'argument': gpt35_logical[index],\n",
    "        'source': 'GPT3.5',\n",
    "        'prompt_type': 'Logical Reasoning'\n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df as a JSON object\n",
    "df.to_json('anthropic_persuasion\\gpt_data_clean.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique claims: 56\n",
      "Claim frequency distribution:\n",
      "2 claims have 32 rows\n",
      "54 claims have 29 rows\n",
      "\n",
      "Source counts:\n",
      "source\n",
      "Claude 2              224\n",
      "Claude 3 Haiku        224\n",
      "Claude 3 Opus         224\n",
      "Claude Instant 1.2    224\n",
      "Claude 1.3            224\n",
      "Human                 174\n",
      "GPT4                  168\n",
      "GPT3.5                168\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "result = analyze_df(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
